2025-05-27 13:47:57,667 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:15:08,261 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:16:02,059 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:17:03,473 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:18:39,668 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:20:04,010 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:22:45,685 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:24:37,176 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:24:44,100 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:27:39,300 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:30:40,105 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:32:52,039 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:38:50,277 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:40:12,827 | INFO | aether2 | Epoch 1: Loss = 0.9148
2025-05-27 14:40:13,068 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:41:42,825 | INFO | aether2 | Epoch 2: Loss = 0.5335
2025-05-27 14:41:43,082 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:43:05,571 | INFO | aether2 | Epoch 3: Loss = 0.4889
2025-05-27 14:43:05,813 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:44:25,058 | INFO | aether2 | Epoch 4: Loss = 0.4695
2025-05-27 14:44:25,304 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:45:57,629 | INFO | aether2 | Epoch 5: Loss = 0.4582
2025-05-27 14:45:57,886 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:47:22,868 | INFO | aether2 | Epoch 6: Loss = 0.4478
2025-05-27 14:47:23,124 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:48:52,295 | INFO | aether2 | Epoch 7: Loss = 0.4546
2025-05-27 14:48:52,564 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:50:16,099 | INFO | aether2 | Epoch 8: Loss = 0.4461
2025-05-27 14:50:16,360 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:51:37,198 | INFO | aether2 | Epoch 9: Loss = 0.4467
2025-05-27 14:51:37,451 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:52:55,727 | INFO | aether2 | Epoch 10: Loss = 0.4355
2025-05-27 14:52:55,960 | INFO | aether2 | Checkpoint sparad: checkpoint_latest.pth
2025-05-27 14:53:02,135 | INFO | aether2 | GOAL: wahts your name
2025-05-27 14:53:03,313 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:53:03,315 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:53:03,363 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:53:03,365 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:53:03,407 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:53:03,410 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:53:14,095 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 14:53:15,324 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:53:15,326 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:53:15,372 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:53:15,376 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:53:15,416 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:53:15,422 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:55:49,966 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:55:53,565 | INFO | aether2 | Checkpoint laddad fr銅 checkpoint_latest.pth, startar fr銅 epoch 11
2025-05-27 14:56:05,248 | INFO | aether2 | GOAL: wahts your name
2025-05-27 14:56:06,263 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:56:06,266 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:56:06,315 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:56:06,318 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:56:06,360 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:56:06,366 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:56:58,382 | INFO | aether2 | GOAL: what's your name ?
2025-05-27 14:56:58,440 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:56:58,442 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:57:08,325 | INFO | aether2 | GOAL: what is your name
2025-05-27 14:57:08,375 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:57:08,378 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:57:19,455 | INFO | aether2 | GOAL: wikipedia England
2025-05-27 14:57:20,661 | INFO | DatabaseConnector | Database connection established.
2025-05-27 14:57:20,663 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 14:57:50,317 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 14:57:53,919 | INFO | aether2 | Checkpoint laddad fr銅 checkpoint_latest.pth, startar fr銅 epoch 11
2025-05-27 15:09:03,074 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:09:06,717 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 11
2025-05-27 15:09:14,116 | INFO | aether2 | GOAL: wahts your name
2025-05-27 15:09:15,049 | INFO | aether2 | Memory added: 'wahts your name...'
2025-05-27 15:09:15,101 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:15,103 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:15,157 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:15,160 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:15,209 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:15,211 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:18,453 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 15:09:19,456 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 15:09:19,505 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:19,505 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:19,562 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:19,563 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:19,609 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:19,609 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:36,216 | INFO | aether2 | GOAL: what is your name
2025-05-27 15:09:36,256 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:36,256 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:49,095 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 15:09:50,084 | INFO | aether2 | Memory added: 'what압 is the capital of sweden ?...'
2025-05-27 15:09:50,136 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:50,138 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:50,189 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:50,191 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:09:50,239 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:09:50,241 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:15:04,834 | INFO | aether2 | GOAL: whats is the capital of sweden ?
2025-05-27 15:15:06,017 | INFO | aether2 | Memory added: 'whats is the capital of sweden ?...'
2025-05-27 15:15:06,077 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:15:06,086 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:15:06,139 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:15:06,143 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:15:06,196 | INFO | DatabaseConnector | Database connection established.
2025-05-27 15:15:06,199 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 15:17:11,268 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:21:45,767 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:21:49,350 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 15:23:38,223 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:24:12,203 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:24:15,813 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 15:27:04,850 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:27:08,558 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 15:28:03,139 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:28:06,739 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 15:40:29,800 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:40:33,346 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 15:57:19,568 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 15:57:23,138 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 15:57:23,252 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 11
2025-05-27 16:44:11,026 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 16:44:14,736 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 16:44:14,893 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 11
2025-05-27 16:44:35,177 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 16:44:36,307 | INFO | aether2 | Memory added: 'what압 is the capital of sweden ?...'
2025-05-27 16:44:36,370 | INFO | DatabaseConnector | Database connection established.
2025-05-27 16:44:36,383 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 16:44:36,437 | INFO | DatabaseConnector | Database connection established.
2025-05-27 16:44:36,440 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 16:44:36,487 | INFO | DatabaseConnector | Database connection established.
2025-05-27 16:44:36,489 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 16:49:43,748 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 16:49:47,453 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 16:49:47,569 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 11
2025-05-27 16:51:05,370 | INFO | aether2 | Epoch 12: Loss = 1.3473
2025-05-27 16:51:05,616 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:52:22,310 | INFO | aether2 | Epoch 13: Loss = 1.3480
2025-05-27 16:52:22,588 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:53:35,157 | INFO | aether2 | Epoch 14: Loss = 1.3479
2025-05-27 16:53:35,418 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:54:45,133 | INFO | aether2 | Epoch 15: Loss = 1.3470
2025-05-27 16:54:45,369 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:55:54,908 | INFO | aether2 | Epoch 16: Loss = 1.3467
2025-05-27 16:55:55,188 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:57:09,453 | INFO | aether2 | Epoch 17: Loss = 1.3465
2025-05-27 16:57:09,715 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:58:24,462 | INFO | aether2 | Epoch 18: Loss = 1.3485
2025-05-27 16:58:24,759 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 16:59:35,708 | INFO | aether2 | Epoch 19: Loss = 1.3471
2025-05-27 16:59:35,973 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 17:00:49,068 | INFO | aether2 | Epoch 20: Loss = 1.3461
2025-05-27 17:00:49,316 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 17:01:05,409 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-27 17:01:06,485 | INFO | aether2 | Memory added: 'how do i loop code in C++...'
2025-05-27 17:01:06,636 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:01:06,646 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:01:06,696 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:01:06,696 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:01:06,742 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:01:06,742 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:01:37,247 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:01:41,259 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:01:41,380 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:13:30,075 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:13:33,672 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:13:33,796 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:14:08,018 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-27 17:14:09,111 | INFO | aether2 | Memory added: 'how do i loop code in C++...'
2025-05-27 17:14:09,163 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:14:09,164 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:14:09,220 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:14:09,224 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:14:09,275 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:14:09,275 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:16:44,216 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:16:47,950 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:16:48,084 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:21:10,136 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:21:13,908 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:21:14,065 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:23:03,022 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:23:07,227 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:23:07,340 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:25:46,627 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:25:50,324 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:28:00,060 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:28:03,934 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:28:04,051 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:28:25,271 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 17:28:26,329 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 17:28:26,491 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:28:26,491 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:28:26,637 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:28:26,637 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:28:26,685 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:28:26,687 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:29:29,403 | INFO | aether2 | GOAL: what's your name ?
2025-05-27 17:29:29,442 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:29:29,442 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:30:08,558 | INFO | aether2 | GOAL: how do you feel ?
2025-05-27 17:30:09,529 | INFO | aether2 | Memory added: 'how do you feel ?...'
2025-05-27 17:30:09,585 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:30:09,594 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:30:09,650 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:30:09,653 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:30:09,707 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:30:09,710 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:31:09,624 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:31:13,448 | INFO | aether2 | Tokenizer loaded from checkpoint with vocab size 4
2025-05-27 17:31:13,568 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:36:08,759 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:36:08,790 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:36:09,231 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:36:13,336 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:36:13,336 | INFO | aether2 |  Keeping latest tokenizer vocabulary.
2025-05-27 17:36:13,487 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:36:38,056 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:36:38,093 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:36:38,521 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:36:41,871 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:36:41,871 | INFO | aether2 |  Keeping latest tokenizer vocabulary.
2025-05-27 17:36:42,019 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:41:50,377 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:41:50,407 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:41:50,891 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:41:54,481 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:42:20,518 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:42:20,557 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:42:21,007 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:42:24,755 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:42:48,718 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:42:48,761 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:42:49,275 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:42:52,769 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:43:59,530 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:43:59,588 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:44:00,426 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:44:07,034 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:44:07,036 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:44:07,410 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:44:07,416 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:45:00,063 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:45:00,087 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:45:00,541 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:45:03,777 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:45:03,777 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:45:03,777 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:45:03,899 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:45:03,903 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:48:24,277 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:48:24,312 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:48:24,760 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:48:28,031 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 17:48:28,455 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:48:28,456 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:48:28,456 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:48:28,600 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:48:28,603 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:49:21,636 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:49:21,668 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:49:22,098 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:49:25,147 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 17:49:25,477 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:49:25,477 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:49:25,477 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:49:25,599 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:49:25,604 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:50:06,700 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:50:06,725 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:50:07,167 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:50:10,584 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 17:50:10,914 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:50:10,914 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:50:10,914 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:50:11,037 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:50:11,039 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:51:55,311 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 17:53:19,898 | INFO | aether2 | GOAL: how do you feel ?
2025-05-27 17:53:21,039 | INFO | aether2 | Memory added: 'how do you feel ?...'
2025-05-27 17:53:21,197 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:53:21,209 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:53:21,261 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:53:21,261 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:53:21,311 | INFO | DatabaseConnector | Database connection established.
2025-05-27 17:53:21,311 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 17:53:40,367 | INFO | aether2 | GOAL: hur m枓 du ?
2025-05-27 17:55:42,153 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:55:42,182 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:55:42,624 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:55:45,544 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 17:55:45,849 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:55:45,849 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:55:45,849 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:55:45,974 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:55:45,979 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:55:59,348 | INFO | aether2 | GOAL: corect my spelling 
2025-05-27 17:58:06,387 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 17:58:06,420 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:58:06,885 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:58:10,065 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 17:58:10,387 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 17:58:10,387 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 17:58:10,387 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 17:58:10,512 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 17:58:10,519 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 17:58:19,432 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 18:01:49,165 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:01:49,193 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:01:49,582 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:01:52,671 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 18:01:52,983 | INFO | aether2 | Tokenizer vocab loaded from file with size 93
2025-05-27 18:01:52,983 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 18:01:52,983 | INFO | aether2 | Setting vocab_size to 93 before loading checkpoint.
2025-05-27 18:01:53,093 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 18:01:53,098 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:01:54,420 | INFO | aether2 | GOAL: what is your name
2025-05-27 18:01:54,474 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:01:54,478 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:01:59,397 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 18:06:05,766 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 18:06:21,350 | INFO | aether2 | GOAL: whats your name 
2025-05-27 18:08:34,964 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 18:11:09,494 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:11:09,524 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:11:29,975 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 18:13:53,022 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:13:53,514 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:13:56,690 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 18:13:57,037 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 18:13:57,037 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 18:13:57,037 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 18:13:57,167 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 18:13:57,167 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:14:46,677 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:14:46,931 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:15:54,464 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:15:54,729 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:16:16,626 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:16:16,859 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:16:24,697 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 18:29:25,603 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:29:25,844 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:29:33,731 | INFO | aether2 | GOAL: corect my spelling 
2025-05-27 18:33:04,820 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:33:05,080 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:33:55,461 | INFO | aether2 | GOAL: what's your name ?
2025-05-27 18:33:55,505 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:33:55,508 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:34:15,349 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 18:34:15,351 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:34:15,351 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:34:15,351 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:34:16,394 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:34:16,394 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:34:16,394 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:34:16,402 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 18:34:16,451 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:34:16,456 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:34:16,508 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:34:16,510 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:34:16,557 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:34:16,559 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:43:55,894 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:43:56,165 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:44:12,521 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 18:44:12,525 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:44:12,525 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:44:12,526 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:44:13,473 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:44:13,474 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:44:13,474 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:44:13,475 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 18:44:13,532 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:44:13,535 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:44:13,693 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:44:13,694 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:44:13,734 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:44:13,739 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:47:28,808 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:47:29,048 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:47:46,513 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 18:47:46,517 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:47:46,517 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:47:46,517 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:47:47,503 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:47:47,503 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:47:47,503 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:47:47,510 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 18:47:47,565 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:47:47,565 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:47:47,617 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:47:47,619 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:47:47,668 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:47:47,670 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:48:29,549 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:48:29,771 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:48:42,010 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 18:48:42,015 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:48:42,015 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:48:42,015 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:48:42,975 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:48:42,975 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:48:42,975 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:48:42,979 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 18:48:43,035 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:48:43,038 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:48:43,181 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:48:43,185 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:48:43,327 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:48:43,329 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:53:38,956 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:53:39,208 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:53:39,636 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:53:42,873 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 18:53:43,216 | INFO | aether2 | Tokenizer vocab loaded from file with size 90
2025-05-27 18:53:43,216 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 18:53:43,216 | INFO | aether2 | Setting vocab_size to 90 before loading checkpoint.
2025-05-27 18:53:43,349 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 18:53:43,354 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:54:08,206 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:54:08,456 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:54:08,936 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:54:12,156 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 18:54:12,486 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 18:54:12,486 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 18:54:12,486 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 18:54:12,615 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 18:54:12,619 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:55:35,568 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 18:55:35,828 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 18:55:50,334 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 18:55:50,341 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:55:50,341 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:55:50,341 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:55:51,437 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 18:55:51,438 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 18:55:51,438 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 18:55:51,443 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 18:55:51,502 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:55:51,509 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:55:51,565 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:55:51,568 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 18:55:51,621 | INFO | DatabaseConnector | Database connection established.
2025-05-27 18:55:51,624 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:12:55,851 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:12:56,104 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:13:17,653 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 19:13:17,656 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 19:13:17,657 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 19:13:17,657 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 19:13:18,698 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 19:13:18,698 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 19:13:18,699 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 19:13:18,703 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 19:13:18,760 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:13:18,770 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:13:18,825 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:13:18,828 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:13:18,877 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:13:18,880 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:15:10,961 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:15:11,196 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:15:21,416 | INFO | aether2 | GOAL: hello 
2025-05-27 19:15:21,419 | WARNING | aether2 | Unrecognized word: 'hello', replacing with <UNK>
2025-05-27 19:15:22,457 | WARNING | aether2 | Unrecognized word: 'hello', replacing with <UNK>
2025-05-27 19:15:22,463 | INFO | aether2 | Memory added: 'hello ...'
2025-05-27 19:15:22,518 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:15:22,523 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:15:22,576 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:15:22,578 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:15:22,629 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:15:22,633 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:20:52,120 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:20:52,393 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:20:59,643 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 19:20:59,646 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 19:20:59,647 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 19:20:59,647 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 19:21:00,695 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 19:21:00,696 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 19:21:00,696 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 19:21:00,700 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 19:21:00,759 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:21:00,765 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:21:00,817 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:21:00,819 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:21:00,872 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:21:00,875 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:36:09,473 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:36:09,816 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:36:34,885 | INFO | aether2 | GOAL: train model
2025-05-27 19:36:35,071 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:36:38,093 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 19:36:38,559 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 19:36:38,559 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 19:36:38,559 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 19:36:38,689 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 19:36:38,689 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:36:38,859 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:36:38,863 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:37:43,782 | INFO | aether2 | GOAL: train model
2025-05-27 19:37:43,962 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:37:44,655 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 19:37:45,060 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 19:37:45,060 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 19:37:45,060 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 19:37:45,220 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 19:37:45,230 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:37:45,388 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:37:45,390 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:47:37,464 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:47:37,713 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:47:47,277 | INFO | aether2 | GOAL: train model
2025-05-27 19:48:43,959 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:48:44,204 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:51:27,384 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:51:52,585 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:52:12,741 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:52:13,201 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:52:16,211 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 19:52:16,518 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 19:52:16,518 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 19:52:16,528 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 19:52:16,641 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 19:52:16,645 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:52:26,444 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:52:26,879 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:52:29,767 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 19:52:30,069 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 19:52:30,069 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 19:52:30,069 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 19:52:30,207 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 19:52:30,212 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:52:44,968 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:52:45,245 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:54:22,304 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:59:34,498 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 19:59:34,764 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:59:44,778 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 19:59:44,778 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 19:59:44,778 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 19:59:44,778 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 19:59:45,826 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 19:59:45,826 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 19:59:45,828 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 19:59:45,832 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 19:59:45,890 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:59:45,894 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:59:45,948 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:59:45,948 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:59:45,999 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:59:46,008 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 19:59:51,604 | INFO | aether2 | GOAL: train model
2025-05-27 19:59:51,778 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:59:54,848 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 19:59:55,285 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 19:59:55,285 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 19:59:55,285 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 19:59:55,414 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 19:59:55,416 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 19:59:55,581 | INFO | DatabaseConnector | Database connection established.
2025-05-27 19:59:55,588 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:20,305 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-27 20:00:20,308 | WARNING | aether2 | Unrecognized word: 'how', replacing with <UNK>
2025-05-27 20:00:20,308 | WARNING | aether2 | Unrecognized word: 'do', replacing with <UNK>
2025-05-27 20:00:20,308 | WARNING | aether2 | Unrecognized word: 'i', replacing with <UNK>
2025-05-27 20:00:20,309 | WARNING | aether2 | Unrecognized word: 'loop', replacing with <UNK>
2025-05-27 20:00:20,309 | WARNING | aether2 | Unrecognized word: 'code', replacing with <UNK>
2025-05-27 20:00:20,309 | WARNING | aether2 | Unrecognized word: 'in', replacing with <UNK>
2025-05-27 20:00:20,309 | WARNING | aether2 | Unrecognized word: 'c', replacing with <UNK>
2025-05-27 20:00:21,367 | WARNING | aether2 | Unrecognized word: 'how', replacing with <UNK>
2025-05-27 20:00:21,367 | WARNING | aether2 | Unrecognized word: 'do', replacing with <UNK>
2025-05-27 20:00:21,367 | WARNING | aether2 | Unrecognized word: 'i', replacing with <UNK>
2025-05-27 20:00:21,369 | WARNING | aether2 | Unrecognized word: 'loop', replacing with <UNK>
2025-05-27 20:00:21,369 | WARNING | aether2 | Unrecognized word: 'code', replacing with <UNK>
2025-05-27 20:00:21,369 | WARNING | aether2 | Unrecognized word: 'in', replacing with <UNK>
2025-05-27 20:00:21,369 | WARNING | aether2 | Unrecognized word: 'c', replacing with <UNK>
2025-05-27 20:00:21,374 | INFO | aether2 | Memory added: 'how do i loop code in C++...'
2025-05-27 20:00:21,425 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:00:21,429 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:21,480 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:00:21,482 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:21,531 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:00:21,535 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:24,842 | INFO | aether2 | GOAL: wahts your name
2025-05-27 20:00:24,846 | WARNING | aether2 | Unrecognized word: 'wahts', replacing with <UNK>
2025-05-27 20:00:24,846 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 20:00:24,846 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 20:00:25,854 | WARNING | aether2 | Unrecognized word: 'wahts', replacing with <UNK>
2025-05-27 20:00:25,854 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 20:00:25,854 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 20:00:25,862 | INFO | aether2 | Memory added: 'wahts your name...'
2025-05-27 20:00:25,911 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:00:25,916 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:25,968 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:00:25,972 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:26,020 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:00:26,022 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:00:28,787 | INFO | aether2 | GOAL: corect my spelling 
2025-05-27 20:00:28,790 | WARNING | aether2 | Unrecognized word: 'corect', replacing with <UNK>
2025-05-27 20:00:28,791 | WARNING | aether2 | Unrecognized word: 'spelling', replacing with <UNK>
2025-05-27 20:00:29,754 | WARNING | aether2 | Unrecognized word: 'corect', replacing with <UNK>
2025-05-27 20:00:29,754 | WARNING | aether2 | Unrecognized word: 'spelling', replacing with <UNK>
2025-05-27 20:01:39,900 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:01:40,091 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:01:46,805 | INFO | aether2 | GOAL: train model
2025-05-27 20:01:46,959 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:01:49,741 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:01:50,191 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:01:50,191 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:01:50,191 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:01:50,311 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:01:50,311 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:01:50,481 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:01:50,486 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:05:20,436 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:05:20,652 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:05:26,627 | INFO | aether2 | GOAL: train model
2025-05-27 20:05:26,789 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:05:29,646 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:05:30,048 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:05:30,048 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:05:30,048 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:05:30,166 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:05:30,166 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:05:30,340 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:05:30,343 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:07:02,855 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:07:03,083 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:07:09,948 | INFO | aether2 | GOAL: train model
2025-05-27 20:07:10,106 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:07:12,888 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:07:13,318 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:07:13,318 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:07:13,318 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:07:13,448 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:07:13,450 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:07:13,620 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:07:13,623 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:07:57,559 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:07:57,769 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:08:05,600 | INFO | aether2 | GOAL: train model
2025-05-27 20:09:13,220 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:09:13,522 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:09:44,660 | INFO | aether2 | GOAL: train model
2025-05-27 20:09:44,951 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:09:47,901 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:09:48,223 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:09:48,223 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:09:48,223 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:09:48,341 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:09:48,341 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:09:48,514 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:09:48,519 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:16:11,373 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:16:11,600 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:16:16,384 | INFO | aether2 | GOAL: train model
2025-05-27 20:16:16,394 | ERROR | aether2 | Failed to load training data: local variable 'training_data' referenced before assignment
2025-05-27 20:16:16,526 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:16:16,530 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:18:06,608 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:18:06,814 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:18:16,639 | INFO | aether2 | GOAL: train model
2025-05-27 20:18:16,792 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:18:19,914 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:18:20,274 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:18:20,274 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:18:20,274 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:18:20,406 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:18:20,408 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:18:20,581 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:18:20,584 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:19:53,948 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:19:54,254 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:19:59,024 | INFO | aether2 | GOAL: train model
2025-05-27 20:20:01,644 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:20:01,644 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:20:01,644 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:20:01,775 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:20:01,782 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:20:01,782 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-27 20:20:48,878 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:20:49,085 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:21:17,396 | INFO | aether2 | GOAL: train model
2025-05-27 20:21:20,166 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:21:20,166 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:21:20,166 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:21:20,286 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:21:20,296 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:21:20,299 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-27 20:21:31,736 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:21:31,949 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:21:35,539 | INFO | aether2 | GOAL: train model
2025-05-27 20:21:38,017 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:21:38,017 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:21:38,017 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:21:38,136 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:21:38,136 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:21:38,136 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-27 20:22:24,348 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:22:24,548 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:22:28,277 | INFO | aether2 | GOAL: train model
2025-05-27 20:22:30,484 | INFO | aether2 |  Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:22:30,790 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:22:30,790 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:22:30,790 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:22:30,909 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 21
2025-05-27 20:22:30,913 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:22:30,913 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-27 20:23:41,049 | INFO | aether2 | Epoch 22: Loss = 4.5292
2025-05-27 20:23:41,334 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:24:52,715 | INFO | aether2 | Epoch 23: Loss = 4.5288
2025-05-27 20:24:52,973 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:26:08,865 | INFO | aether2 | Epoch 24: Loss = 4.5293
2025-05-27 20:26:09,135 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:27:26,686 | INFO | aether2 | Epoch 25: Loss = 4.5291
2025-05-27 20:27:26,931 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:28:40,403 | INFO | aether2 | Epoch 26: Loss = 4.5297
2025-05-27 20:28:40,651 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:29:53,199 | INFO | aether2 | Epoch 27: Loss = 4.5292
2025-05-27 20:29:53,442 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:31:07,650 | INFO | aether2 | Epoch 28: Loss = 4.5295
2025-05-27 20:31:07,907 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:32:23,007 | INFO | aether2 | Epoch 29: Loss = 4.5291
2025-05-27 20:32:23,303 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:33:37,189 | INFO | aether2 | Epoch 30: Loss = 4.5300
2025-05-27 20:33:37,438 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:33:37,441 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-27 20:33:37,678 | INFO | DatabaseConnector | Database connection established.
2025-05-27 20:33:37,688 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 20:34:30,471 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 20:34:30,685 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:34:41,953 | INFO | aether2 | GOAL: train model
2025-05-27 20:34:44,542 | INFO | aether2 |  Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 20:34:44,887 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 20:34:44,888 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 20:34:44,888 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 20:34:45,025 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 31
2025-05-27 20:34:45,029 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 20:34:45,030 | INFO | aether2 | Starting model training for 100 epochs...
2025-05-27 20:35:56,878 | INFO | aether2 | Epoch 32: Loss = 4.5287
2025-05-27 20:35:57,150 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:37:09,990 | INFO | aether2 | Epoch 33: Loss = 4.5301
2025-05-27 20:37:10,237 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:38:25,969 | INFO | aether2 | Epoch 34: Loss = 4.5281
2025-05-27 20:38:26,294 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:39:43,024 | INFO | aether2 | Epoch 35: Loss = 4.5293
2025-05-27 20:39:43,297 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:40:56,302 | INFO | aether2 | Epoch 36: Loss = 4.5300
2025-05-27 20:40:56,533 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:42:10,528 | INFO | aether2 | Epoch 37: Loss = 4.5294
2025-05-27 20:42:10,786 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:43:27,993 | INFO | aether2 | Epoch 38: Loss = 4.5297
2025-05-27 20:43:28,244 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:44:44,035 | INFO | aether2 | Epoch 39: Loss = 4.5288
2025-05-27 20:44:44,294 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:46:01,620 | INFO | aether2 | Epoch 40: Loss = 4.5277
2025-05-27 20:46:01,905 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:47:12,553 | INFO | aether2 | Epoch 41: Loss = 4.5288
2025-05-27 20:47:12,793 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:48:21,256 | INFO | aether2 | Epoch 42: Loss = 4.5289
2025-05-27 20:48:21,483 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:49:27,194 | INFO | aether2 | Epoch 43: Loss = 4.5299
2025-05-27 20:49:27,430 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:50:32,682 | INFO | aether2 | Epoch 44: Loss = 4.5299
2025-05-27 20:50:32,922 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:51:37,921 | INFO | aether2 | Epoch 45: Loss = 4.5291
2025-05-27 20:51:38,161 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:52:43,308 | INFO | aether2 | Epoch 46: Loss = 4.5297
2025-05-27 20:52:43,538 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:53:48,441 | INFO | aether2 | Epoch 47: Loss = 4.5286
2025-05-27 20:53:48,692 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:54:53,450 | INFO | aether2 | Epoch 48: Loss = 4.5282
2025-05-27 20:54:53,703 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:55:59,672 | INFO | aether2 | Epoch 49: Loss = 4.5285
2025-05-27 20:55:59,909 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:57:04,999 | INFO | aether2 | Epoch 50: Loss = 4.5295
2025-05-27 20:57:05,235 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:58:10,561 | INFO | aether2 | Epoch 51: Loss = 4.5291
2025-05-27 20:58:10,805 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 20:59:16,541 | INFO | aether2 | Epoch 52: Loss = 4.5284
2025-05-27 20:59:16,777 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:00:21,905 | INFO | aether2 | Epoch 53: Loss = 4.5293
2025-05-27 21:00:22,143 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:01:27,702 | INFO | aether2 | Epoch 54: Loss = 4.5292
2025-05-27 21:01:27,945 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:02:33,035 | INFO | aether2 | Epoch 55: Loss = 4.5297
2025-05-27 21:02:33,279 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:03:38,138 | INFO | aether2 | Epoch 56: Loss = 4.5284
2025-05-27 21:03:38,363 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:04:44,021 | INFO | aether2 | Epoch 57: Loss = 4.5300
2025-05-27 21:04:44,267 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:05:49,375 | INFO | aether2 | Epoch 58: Loss = 4.5286
2025-05-27 21:05:49,617 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:06:54,445 | INFO | aether2 | Epoch 59: Loss = 4.5301
2025-05-27 21:06:54,698 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:07:59,753 | INFO | aether2 | Epoch 60: Loss = 4.5291
2025-05-27 21:07:59,995 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:09:04,412 | INFO | aether2 | Epoch 61: Loss = 4.5301
2025-05-27 21:09:04,650 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:10:09,942 | INFO | aether2 | Epoch 62: Loss = 4.5291
2025-05-27 21:10:10,171 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:11:15,352 | INFO | aether2 | Epoch 63: Loss = 4.5292
2025-05-27 21:11:15,582 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:12:20,599 | INFO | aether2 | Epoch 64: Loss = 4.5298
2025-05-27 21:12:20,829 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:13:26,158 | INFO | aether2 | Epoch 65: Loss = 4.5305
2025-05-27 21:13:26,514 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:14:31,197 | INFO | aether2 | Epoch 66: Loss = 4.5292
2025-05-27 21:14:31,439 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:15:36,314 | INFO | aether2 | Epoch 67: Loss = 4.5297
2025-05-27 21:15:36,540 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:16:41,565 | INFO | aether2 | Epoch 68: Loss = 4.5286
2025-05-27 21:16:41,798 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:17:46,561 | INFO | aether2 | Epoch 69: Loss = 4.5292
2025-05-27 21:17:46,785 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:18:51,532 | INFO | aether2 | Epoch 70: Loss = 4.5294
2025-05-27 21:18:51,786 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:19:57,642 | INFO | aether2 | Epoch 71: Loss = 4.5299
2025-05-27 21:19:57,870 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:21:02,841 | INFO | aether2 | Epoch 72: Loss = 4.5293
2025-05-27 21:21:03,076 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:22:07,517 | INFO | aether2 | Epoch 73: Loss = 4.5285
2025-05-27 21:22:07,789 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:23:13,261 | INFO | aether2 | Epoch 74: Loss = 4.5296
2025-05-27 21:23:13,500 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:24:18,445 | INFO | aether2 | Epoch 75: Loss = 4.5299
2025-05-27 21:24:18,676 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:25:25,560 | INFO | aether2 | Epoch 76: Loss = 4.5293
2025-05-27 21:25:25,833 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:26:30,707 | INFO | aether2 | Epoch 77: Loss = 4.5297
2025-05-27 21:26:30,936 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:27:35,905 | INFO | aether2 | Epoch 78: Loss = 4.5302
2025-05-27 21:27:36,134 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:28:41,888 | INFO | aether2 | Epoch 79: Loss = 4.5289
2025-05-27 21:28:42,123 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:29:47,523 | INFO | aether2 | Epoch 80: Loss = 4.5288
2025-05-27 21:29:47,749 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:30:52,729 | INFO | aether2 | Epoch 81: Loss = 4.5293
2025-05-27 21:30:52,964 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:31:58,998 | INFO | aether2 | Epoch 82: Loss = 4.5299
2025-05-27 21:31:59,234 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:33:04,043 | INFO | aether2 | Epoch 83: Loss = 4.5287
2025-05-27 21:33:04,286 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:34:09,122 | INFO | aether2 | Epoch 84: Loss = 4.5297
2025-05-27 21:34:09,357 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:35:15,125 | INFO | aether2 | Epoch 85: Loss = 4.5286
2025-05-27 21:35:15,358 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:36:20,695 | INFO | aether2 | Epoch 86: Loss = 4.5304
2025-05-27 21:36:20,930 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:37:26,070 | INFO | aether2 | Epoch 87: Loss = 4.5276
2025-05-27 21:37:26,306 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:38:31,929 | INFO | aether2 | Epoch 88: Loss = 4.5286
2025-05-27 21:38:32,160 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:39:37,331 | INFO | aether2 | Epoch 89: Loss = 4.5288
2025-05-27 21:39:37,571 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:40:42,714 | INFO | aether2 | Epoch 90: Loss = 4.5292
2025-05-27 21:40:42,959 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:41:47,640 | INFO | aether2 | Epoch 91: Loss = 4.5293
2025-05-27 21:41:47,879 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:42:52,604 | INFO | aether2 | Epoch 92: Loss = 4.5297
2025-05-27 21:42:52,858 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:43:58,162 | INFO | aether2 | Epoch 93: Loss = 4.5291
2025-05-27 21:43:58,397 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:45:03,571 | INFO | aether2 | Epoch 94: Loss = 4.5293
2025-05-27 21:45:03,802 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:46:08,668 | INFO | aether2 | Epoch 95: Loss = 4.5297
2025-05-27 21:46:08,892 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:47:14,108 | INFO | aether2 | Epoch 96: Loss = 4.5280
2025-05-27 21:47:14,354 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:48:19,298 | INFO | aether2 | Epoch 97: Loss = 4.5282
2025-05-27 21:48:19,530 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:49:24,866 | INFO | aether2 | Epoch 98: Loss = 4.5292
2025-05-27 21:49:25,101 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:50:31,967 | INFO | aether2 | Epoch 99: Loss = 4.5287
2025-05-27 21:50:32,212 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:51:38,510 | INFO | aether2 | Epoch 100: Loss = 4.5293
2025-05-27 21:51:38,742 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-27 21:51:38,742 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-27 21:51:38,969 | INFO | DatabaseConnector | Database connection established.
2025-05-27 21:51:38,973 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:05:52,619 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 22:05:52,622 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 22:05:52,622 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:05:52,622 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:05:53,527 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 22:05:53,528 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:05:53,528 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:05:53,534 | INFO | aether2 | Memory added: 'whats your name ?...'
2025-05-27 22:05:53,583 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:05:53,586 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:05:53,635 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:05:53,637 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:05:53,682 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:05:53,685 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:16:15,803 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:16:16,057 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:16:25,661 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-27 22:16:25,665 | WARNING | aether2 | Unrecognized word: 'what', replacing with <UNK>
2025-05-27 22:16:25,666 | WARNING | aether2 | Unrecognized word: 's', replacing with <UNK>
2025-05-27 22:16:25,666 | WARNING | aether2 | Unrecognized word: 'the', replacing with <UNK>
2025-05-27 22:16:25,666 | WARNING | aether2 | Unrecognized word: 'capital', replacing with <UNK>
2025-05-27 22:16:25,666 | WARNING | aether2 | Unrecognized word: 'of', replacing with <UNK>
2025-05-27 22:16:25,667 | WARNING | aether2 | Unrecognized word: 'sweden', replacing with <UNK>
2025-05-27 22:24:20,475 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:24:20,666 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:24:46,033 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:24:46,319 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:25:08,074 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 22:25:08,077 | WARNING | aether2 | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 22:25:08,078 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:25:08,078 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:29:40,440 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:29:40,644 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:37:32,824 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:37:33,035 | INFO | aether2 |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:37:39,633 | INFO | aether2 | GOAL: wahts your name
2025-05-27 22:37:39,636 | WARNING | aether2 | Unrecognized word: 'wahts', replacing with <UNK>
2025-05-27 22:37:39,636 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:37:39,636 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:37:40,503 | WARNING | aether2 | Unrecognized word: 'wahts', replacing with <UNK>
2025-05-27 22:37:40,503 | WARNING | aether2 | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:37:40,503 | WARNING | aether2 | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:37:40,508 | INFO | aether2 | Memory added: 'wahts your name...'
2025-05-27 22:37:40,562 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:37:40,564 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:37:40,708 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:37:40,710 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:37:40,750 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:37:40,752 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:39:48,169 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:39:48,375 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:40:02,400 | INFO | aether2 | GOAL: wahts your name
2025-05-27 22:40:02,404 | WARNING | SimpleTokenizer | Unrecognized word: 'wahts', replacing with <UNK>
2025-05-27 22:40:02,404 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:40:02,404 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:40:03,313 | WARNING | SimpleTokenizer | Unrecognized word: 'wahts', replacing with <UNK>
2025-05-27 22:40:03,313 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:40:03,314 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:40:03,318 | INFO | aether2 | Memory added: 'wahts your name...'
2025-05-27 22:40:03,368 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:40:03,371 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:40:03,417 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:40:03,420 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:40:03,468 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:40:03,470 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:56:26,594 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:56:50,429 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:56:50,612 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:57:15,054 | INFO | aether2 | GOAL: wahts your name
2025-05-27 22:57:27,331 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 22:58:05,347 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:58:05,537 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 22:58:10,721 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 22:58:10,724 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 22:58:10,724 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:58:10,724 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:58:11,612 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 22:58:11,612 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 22:58:11,612 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 22:58:11,617 | INFO | AetherMemory | Memory added: 'whats your name ?...'
2025-05-27 22:58:11,668 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:58:11,670 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:58:11,816 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:58:11,819 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:58:11,858 | INFO | DatabaseConnector | Database connection established.
2025-05-27 22:58:11,861 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 22:59:53,629 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 22:59:53,836 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:02:44,649 | INFO | aether2 | GOAL: what's your name ?
2025-05-27 23:02:44,653 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:02:44,653 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:02:44,653 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:02:45,564 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:02:45,565 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:02:45,565 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:02:45,570 | INFO | AetherMemory | Memory added: 'what's your name ?...'
2025-05-27 23:02:45,620 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:02:45,622 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:02:45,668 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:02:45,670 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:02:45,717 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:02:45,719 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:17:07,268 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:17:10,026 | INFO | aether2 |  Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 23:17:10,322 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 23:17:10,322 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 23:17:10,322 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 23:17:10,427 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-27 23:17:10,431 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:17:10,431 | INFO | aether2 | Starting model training for 100 epochs...
2025-05-27 23:17:10,431 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-27 23:17:36,810 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:17:39,087 | INFO | aether2 |  Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 23:17:39,384 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 23:17:39,384 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 23:17:39,384 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 23:17:39,490 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-27 23:17:39,493 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:17:39,494 | INFO | aether2 | Starting model training for 100 epochs...
2025-05-27 23:17:39,494 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-27 23:18:54,079 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:18:56,417 | INFO | aether2 |  Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 23:18:56,721 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 23:18:56,721 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 23:18:56,721 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 23:18:56,841 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-27 23:18:56,845 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:18:56,846 | INFO | aether2 | Starting model training for 100 epochs...
2025-05-27 23:18:56,846 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-27 23:19:15,614 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:19:17,995 | INFO | aether2 |  Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-27 23:19:18,291 | INFO | aether2 | Tokenizer vocab loaded from file with size 87
2025-05-27 23:19:18,291 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-27 23:19:18,291 | INFO | aether2 | Setting vocab_size to 87 before loading checkpoint.
2025-05-27 23:19:18,394 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-27 23:19:18,398 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:19:18,398 | INFO | aether2 | Starting model training for 100 epochs...
2025-05-27 23:19:18,399 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-27 23:21:32,259 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:21:32,498 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-27 23:23:21,581 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:23:40,162 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:23:47,588 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 23:23:47,591 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:23:47,591 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:23:47,592 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:23:48,447 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:23:48,447 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:23:48,447 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:23:48,451 | INFO | AetherMemory | Memory added: 'whats your name ?...'
2025-05-27 23:23:48,601 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:23:48,606 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:23:48,652 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:23:48,655 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:23:48,695 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:23:48,697 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:24:24,847 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:29:03,840 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:32:48,185 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:34:59,955 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:35:17,045 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:37:52,089 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:38:45,147 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:39:30,352 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:40:48,744 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:43:01,012 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:45:57,374 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:47:23,721 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:50:16,224 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:50:16,431 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:50:16,641 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([4, 256]) from checkpoint, the shape in current model is torch.Size([87, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([4, 256]) from checkpoint, the shape in current model is torch.Size([87, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([87]).)
2025-05-27 23:50:16,642 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-27 23:50:16,719 | INFO | EndPoints | Starting Flask server...
2025-05-27 23:50:58,057 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:50:58,252 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:50:58,501 | INFO | EndPoints | Starting Flask server...
2025-05-27 23:51:31,950 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:51:32,157 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:51:32,373 | INFO | EndPoints | Model loaded.
2025-05-27 23:51:32,376 | INFO | EndPoints | Starting Flask server...
2025-05-27 23:52:04,433 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:52:04,631 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:52:04,631 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-27 23:52:04,839 | INFO | EndPoints | Model loaded.
2025-05-27 23:52:04,850 | INFO | EndPoints | Starting Flask server...
2025-05-27 23:52:16,246 | INFO | EndPoints | Received input: whats your name ?
2025-05-27 23:52:16,246 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 23:52:16,249 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:52:16,249 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:52:16,249 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:52:17,108 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:52:17,109 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:52:17,109 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:52:17,113 | INFO | AetherMemory | Memory added: 'whats your name ?...'
2025-05-27 23:52:17,159 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:52:17,165 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:52:17,210 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:52:17,214 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:52:17,264 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:52:17,266 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:57:12,573 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:57:12,782 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:57:12,783 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-27 23:57:13,036 | INFO | EndPoints | Model loaded.
2025-05-27 23:57:13,038 | INFO | EndPoints | Starting Flask server...
2025-05-27 23:57:22,936 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 23:57:22,940 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:57:22,940 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:57:22,940 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:57:23,861 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:57:23,862 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:57:23,862 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:57:23,867 | INFO | AetherMemory | Memory added: 'whats your name ?...'
2025-05-27 23:57:23,914 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:57:23,917 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:57:24,056 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:57:24,059 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:57:58,946 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-27 23:57:59,148 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-27 23:57:59,149 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-27 23:57:59,345 | INFO | EndPoints | Model loaded.
2025-05-27 23:57:59,347 | INFO | EndPoints | Starting Flask server...
2025-05-27 23:58:05,554 | INFO | aether2 | GOAL: whats your name ?
2025-05-27 23:58:05,559 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:58:05,559 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:58:05,559 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:58:06,421 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-27 23:58:06,421 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-27 23:58:06,421 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-27 23:58:06,426 | INFO | AetherMemory | Memory added: 'whats your name ?...'
2025-05-27 23:58:06,496 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:58:06,498 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:58:06,539 | INFO | DatabaseConnector | Database connection established.
2025-05-27 23:58:06,541 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-27 23:58:06,542 | ERROR | EndPoints | Error occurred: local variable 'cleaned_response' referenced before assignment
2025-05-28 00:00:19,013 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:00:19,202 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:00:19,202 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-28 00:00:19,400 | INFO | EndPoints | Model loaded.
2025-05-28 00:00:19,402 | INFO | EndPoints | Starting Flask server...
2025-05-28 00:00:27,751 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 00:00:27,754 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-28 00:00:27,755 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-28 00:00:27,755 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-28 00:00:28,587 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-28 00:00:28,588 | WARNING | SimpleTokenizer | Unrecognized word: 'your', replacing with <UNK>
2025-05-28 00:00:28,588 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-28 00:00:28,592 | INFO | AetherMemory | Memory added: 'whats your name ?...'
2025-05-28 00:00:28,746 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:00:28,749 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:00:28,900 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:00:28,902 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:00:28,940 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:00:28,942 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:02:09,775 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:02:09,963 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:02:09,964 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-28 00:02:10,165 | INFO | EndPoints | Model loaded.
2025-05-28 00:02:10,166 | INFO | EndPoints | Starting Flask server...
2025-05-28 00:02:16,320 | INFO | aether2 | GOAL: train model
2025-05-28 00:02:16,323 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-28 00:02:16,395 | ERROR | EndPoints | Error occurred: module 'flask.json' has no attribute 'JSONDecodeError'
2025-05-28 00:05:16,040 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:05:16,249 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:05:16,249 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-28 00:05:16,464 | INFO | EndPoints | Model loaded.
2025-05-28 00:05:16,466 | INFO | EndPoints | Starting Flask server...
2025-05-28 00:05:25,316 | INFO | aether2 | GOAL: train model
2025-05-28 00:05:25,317 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-28 00:05:25,435 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:05:25,437 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:08:58,964 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:08:59,161 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:08:59,161 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-28 00:08:59,392 | INFO | EndPoints | Model loaded.
2025-05-28 00:08:59,394 | INFO | EndPoints | Starting Flask server...
2025-05-28 00:09:07,769 | INFO | aether2 | GOAL: train model
2025-05-28 00:09:07,770 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-28 00:09:07,890 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:09:07,892 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:11:36,788 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:11:36,996 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:11:36,997 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-28 00:11:37,236 | INFO | EndPoints | Model loaded.
2025-05-28 00:11:37,238 | INFO | EndPoints | Starting Flask server...
2025-05-28 00:11:42,691 | INFO | aether2 | GOAL: train model
2025-05-28 00:11:42,692 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-28 00:11:42,813 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:11:42,815 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:17:10,200 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:17:10,407 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:17:10,407 | INFO | EndPoints | Tokenizer vocabulary updated with 20000 examples.
2025-05-28 00:17:10,622 | INFO | EndPoints | Model loaded.
2025-05-28 00:17:10,625 | INFO | EndPoints | Starting Flask server...
2025-05-28 00:17:21,642 | INFO | aether2 | GOAL: train model
2025-05-28 00:17:21,645 | ERROR | aether2 | No valid training data loaded. Aborting training...
2025-05-28 00:17:21,765 | INFO | DatabaseConnector | Database connection established.
2025-05-28 00:17:21,768 | INFO | DatabaseConnector | Conversation saved successfully.
2025-05-28 00:19:22,793 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:19:23,177 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:19:23,177 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:19:23,426 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:19:26,112 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:19:26,455 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:19:26,455 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:19:26,455 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:19:26,577 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-28 00:19:26,583 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:21:53,154 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:21:53,490 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:21:53,491 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:21:53,705 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:21:56,008 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:21:56,339 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:21:56,339 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:21:56,340 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:21:56,452 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-28 00:21:56,456 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:26:13,699 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:26:14,051 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:26:14,052 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:26:14,251 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:26:16,450 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:26:16,764 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:26:16,764 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:26:16,764 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:26:16,874 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-28 00:26:16,880 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:30:25,569 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:30:25,913 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:30:25,914 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:30:26,148 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:30:28,916 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:30:28,916 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:30:28,916 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:30:29,021 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-28 00:30:29,027 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:31:19,193 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:31:19,555 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:31:19,555 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:31:19,761 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:31:22,073 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:31:22,384 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:31:22,385 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:31:22,385 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:31:22,492 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-28 00:31:22,496 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:31:49,379 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:31:49,742 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:31:49,742 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:31:49,934 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:31:52,312 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:31:52,621 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:31:52,621 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:31:52,621 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:31:52,736 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 101
2025-05-28 00:31:52,742 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:31:52,816 | INFO | aether2 | Starting model training for 700 epochs...
2025-05-28 00:35:18,408 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 00:37:04,080 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:37:04,427 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:37:04,428 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:37:04,633 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:37:06,900 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:37:07,224 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:37:07,224 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:37:07,224 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:37:07,332 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 103
2025-05-28 00:37:07,338 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:37:07,411 | INFO | aether2 | Starting model training for 700 epochs...
2025-05-28 00:38:59,495 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:38:59,877 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:38:59,877 | INFO | EndPoints | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 00:39:00,127 | WARNING | EndPoints |  Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([87, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([87]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 00:39:02,649 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-28 00:39:02,727 | INFO | aether2 | Starting model training for 700 epochs...
2025-05-28 00:42:33,552 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 00:43:21,646 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:43:45,256 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:46:58,058 | INFO | DatabaseConnector | DatabaseConnector initialized: host=localhost, port=5432, dbname=backenddb, user=springuser
2025-05-28 00:47:00,453 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 00:47:00,564 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 00:47:00,564 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 00:47:00,565 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 00:47:00,669 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 2
2025-05-28 00:47:00,675 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 00:47:00,675 | INFO | aether2 |  Starting model training for 700 epochs...
2025-05-28 00:50:15,450 | INFO | aether2 | Epoch 3: Loss = 5.3561
2025-05-28 00:50:15,517 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 00:53:29,449 | INFO | aether2 | Epoch 4: Loss = 5.3575
2025-05-28 00:53:29,522 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 00:56:53,077 | INFO | aether2 | Epoch 5: Loss = 5.3542
2025-05-28 00:56:53,157 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:00:12,879 | INFO | aether2 | Epoch 6: Loss = 5.3559
2025-05-28 01:00:12,955 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:03:29,949 | INFO | aether2 | Epoch 7: Loss = 5.3554
2025-05-28 01:03:30,051 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:06:46,800 | INFO | aether2 | Epoch 8: Loss = 5.3572
2025-05-28 01:06:46,876 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:10:03,048 | INFO | aether2 | Epoch 9: Loss = 5.3570
2025-05-28 01:10:03,121 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:13:19,781 | INFO | aether2 | Epoch 10: Loss = 5.3576
2025-05-28 01:13:19,862 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:16:35,643 | INFO | aether2 | Epoch 11: Loss = 5.3544
2025-05-28 01:16:35,722 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:19:53,701 | INFO | aether2 | Epoch 12: Loss = 5.3586
2025-05-28 01:19:53,776 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:23:10,889 | INFO | aether2 | Epoch 13: Loss = 5.3560
2025-05-28 01:23:10,961 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:26:28,512 | INFO | aether2 | Epoch 14: Loss = 5.3564
2025-05-28 01:26:28,581 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:29:46,437 | INFO | aether2 | Epoch 15: Loss = 5.3565
2025-05-28 01:29:46,518 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:33:02,368 | INFO | aether2 | Epoch 16: Loss = 5.3552
2025-05-28 01:33:02,436 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:36:19,924 | INFO | aether2 | Epoch 17: Loss = 5.3560
2025-05-28 01:36:20,007 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:39:45,835 | INFO | aether2 | Epoch 18: Loss = 5.3566
2025-05-28 01:39:45,914 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:43:09,558 | INFO | aether2 | Epoch 19: Loss = 5.3573
2025-05-28 01:43:09,661 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:46:28,329 | INFO | aether2 | Epoch 20: Loss = 5.3536
2025-05-28 01:46:28,410 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:49:48,413 | INFO | aether2 | Epoch 21: Loss = 5.3564
2025-05-28 01:49:48,486 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:53:07,930 | INFO | aether2 | Epoch 22: Loss = 5.3563
2025-05-28 01:53:08,012 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:56:26,032 | INFO | aether2 | Epoch 23: Loss = 5.3544
2025-05-28 01:56:26,110 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 01:59:46,235 | INFO | aether2 | Epoch 24: Loss = 5.3565
2025-05-28 01:59:46,316 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:03:04,808 | INFO | aether2 | Epoch 25: Loss = 5.3557
2025-05-28 02:03:04,887 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:06:23,720 | INFO | aether2 | Epoch 26: Loss = 5.3566
2025-05-28 02:06:23,816 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:09:43,454 | INFO | aether2 | Epoch 27: Loss = 5.3557
2025-05-28 02:09:43,534 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:13:02,618 | INFO | aether2 | Epoch 28: Loss = 5.3559
2025-05-28 02:13:02,695 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:16:19,998 | INFO | aether2 | Epoch 29: Loss = 5.3550
2025-05-28 02:16:20,072 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:19:39,519 | INFO | aether2 | Epoch 30: Loss = 5.3560
2025-05-28 02:19:39,620 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:22:58,412 | INFO | aether2 | Epoch 31: Loss = 5.3558
2025-05-28 02:22:58,509 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:26:15,998 | INFO | aether2 | Epoch 32: Loss = 5.3572
2025-05-28 02:26:16,072 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:29:35,564 | INFO | aether2 | Epoch 33: Loss = 5.3576
2025-05-28 02:29:35,642 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:32:53,283 | INFO | aether2 | Epoch 34: Loss = 5.3552
2025-05-28 02:32:53,361 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:36:11,810 | INFO | aether2 | Epoch 35: Loss = 5.3555
2025-05-28 02:36:11,902 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:39:31,040 | INFO | aether2 | Epoch 36: Loss = 5.3575
2025-05-28 02:39:31,114 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:42:50,078 | INFO | aether2 | Epoch 37: Loss = 5.3564
2025-05-28 02:42:50,157 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:46:06,510 | INFO | aether2 | Epoch 38: Loss = 5.3573
2025-05-28 02:46:06,583 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:49:20,916 | INFO | aether2 | Epoch 39: Loss = 5.3566
2025-05-28 02:49:20,991 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:52:36,253 | INFO | aether2 | Epoch 40: Loss = 5.3574
2025-05-28 02:52:36,331 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:55:50,811 | INFO | aether2 | Epoch 41: Loss = 5.3569
2025-05-28 02:55:50,893 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 02:59:04,587 | INFO | aether2 | Epoch 42: Loss = 5.3571
2025-05-28 02:59:04,675 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:02:17,929 | INFO | aether2 | Epoch 43: Loss = 5.3551
2025-05-28 03:02:18,007 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:05:32,562 | INFO | aether2 | Epoch 44: Loss = 5.3563
2025-05-28 03:05:32,632 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:08:49,545 | INFO | aether2 | Epoch 45: Loss = 5.3566
2025-05-28 03:08:49,621 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:12:17,516 | INFO | aether2 | Epoch 46: Loss = 5.3538
2025-05-28 03:12:17,604 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:15:41,502 | INFO | aether2 | Epoch 47: Loss = 5.3579
2025-05-28 03:15:41,583 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:18:57,036 | INFO | aether2 | Epoch 48: Loss = 5.3549
2025-05-28 03:18:57,116 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:22:12,595 | INFO | aether2 | Epoch 49: Loss = 5.3569
2025-05-28 03:22:12,665 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:25:28,135 | INFO | aether2 | Epoch 50: Loss = 5.3567
2025-05-28 03:25:28,214 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:28:43,687 | INFO | aether2 | Epoch 51: Loss = 5.3562
2025-05-28 03:28:43,788 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:31:58,823 | INFO | aether2 | Epoch 52: Loss = 5.3593
2025-05-28 03:31:58,905 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:35:13,880 | INFO | aether2 | Epoch 53: Loss = 5.3558
2025-05-28 03:35:13,951 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:38:28,789 | INFO | aether2 | Epoch 54: Loss = 5.3560
2025-05-28 03:38:28,895 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:41:44,789 | INFO | aether2 | Epoch 55: Loss = 5.3570
2025-05-28 03:41:44,866 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:44:59,181 | INFO | aether2 | Epoch 56: Loss = 5.3567
2025-05-28 03:44:59,253 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:48:14,079 | INFO | aether2 | Epoch 57: Loss = 5.3568
2025-05-28 03:48:14,162 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:51:29,729 | INFO | aether2 | Epoch 58: Loss = 5.3576
2025-05-28 03:51:29,804 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:54:44,652 | INFO | aether2 | Epoch 59: Loss = 5.3555
2025-05-28 03:54:44,733 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 03:58:00,058 | INFO | aether2 | Epoch 60: Loss = 5.3581
2025-05-28 03:58:00,139 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:01:15,113 | INFO | aether2 | Epoch 61: Loss = 5.3573
2025-05-28 04:01:15,186 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:04:29,630 | INFO | aether2 | Epoch 62: Loss = 5.3569
2025-05-28 04:04:29,706 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:07:45,242 | INFO | aether2 | Epoch 63: Loss = 5.3587
2025-05-28 04:07:45,319 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:11:00,156 | INFO | aether2 | Epoch 64: Loss = 5.3552
2025-05-28 04:11:00,229 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:14:15,003 | INFO | aether2 | Epoch 65: Loss = 5.3570
2025-05-28 04:14:15,112 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:17:31,708 | INFO | aether2 | Epoch 66: Loss = 5.3573
2025-05-28 04:17:31,801 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:20:46,760 | INFO | aether2 | Epoch 67: Loss = 5.3583
2025-05-28 04:20:46,832 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:24:01,669 | INFO | aether2 | Epoch 68: Loss = 5.3556
2025-05-28 04:24:01,750 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:27:17,458 | INFO | aether2 | Epoch 69: Loss = 5.3573
2025-05-28 04:27:17,540 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:30:32,379 | INFO | aether2 | Epoch 70: Loss = 5.3565
2025-05-28 04:30:32,460 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:33:47,196 | INFO | aether2 | Epoch 71: Loss = 5.3555
2025-05-28 04:33:47,291 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:37:01,188 | INFO | aether2 | Epoch 72: Loss = 5.3570
2025-05-28 04:37:01,270 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:40:16,693 | INFO | aether2 | Epoch 73: Loss = 5.3575
2025-05-28 04:40:16,765 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:43:31,119 | INFO | aether2 | Epoch 74: Loss = 5.3547
2025-05-28 04:43:31,201 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:46:46,794 | INFO | aether2 | Epoch 75: Loss = 5.3573
2025-05-28 04:46:46,875 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:50:02,012 | INFO | aether2 | Epoch 76: Loss = 5.3578
2025-05-28 04:50:02,087 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:53:17,918 | INFO | aether2 | Epoch 77: Loss = 5.3588
2025-05-28 04:53:17,995 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:56:32,707 | INFO | aether2 | Epoch 78: Loss = 5.3582
2025-05-28 04:56:32,780 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 04:59:46,927 | INFO | aether2 | Epoch 79: Loss = 5.3565
2025-05-28 04:59:47,025 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:03:02,453 | INFO | aether2 | Epoch 80: Loss = 5.3559
2025-05-28 05:03:02,559 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:06:16,853 | INFO | aether2 | Epoch 81: Loss = 5.3562
2025-05-28 05:06:16,928 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:09:30,961 | INFO | aether2 | Epoch 82: Loss = 5.3557
2025-05-28 05:09:31,035 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:12:45,543 | INFO | aether2 | Epoch 83: Loss = 5.3547
2025-05-28 05:12:45,624 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:16:01,176 | INFO | aether2 | Epoch 84: Loss = 5.3562
2025-05-28 05:16:01,253 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:19:15,916 | INFO | aether2 | Epoch 85: Loss = 5.3556
2025-05-28 05:19:15,996 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:22:31,730 | INFO | aether2 | Epoch 86: Loss = 5.3570
2025-05-28 05:22:31,826 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:25:47,001 | INFO | aether2 | Epoch 87: Loss = 5.3560
2025-05-28 05:25:47,098 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:29:01,372 | INFO | aether2 | Epoch 88: Loss = 5.3554
2025-05-28 05:29:01,441 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:32:17,322 | INFO | aether2 | Epoch 89: Loss = 5.3574
2025-05-28 05:32:17,416 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:35:32,647 | INFO | aether2 | Epoch 90: Loss = 5.3565
2025-05-28 05:35:32,724 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:38:48,070 | INFO | aether2 | Epoch 91: Loss = 5.3589
2025-05-28 05:38:48,173 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:42:03,270 | INFO | aether2 | Epoch 92: Loss = 5.3549
2025-05-28 05:42:03,350 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:45:18,162 | INFO | aether2 | Epoch 93: Loss = 5.3568
2025-05-28 05:45:18,256 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:48:33,896 | INFO | aether2 | Epoch 94: Loss = 5.3569
2025-05-28 05:48:33,970 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:51:49,867 | INFO | aether2 | Epoch 95: Loss = 5.3570
2025-05-28 05:51:49,961 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:55:05,804 | INFO | aether2 | Epoch 96: Loss = 5.3561
2025-05-28 05:55:05,885 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 05:58:20,134 | INFO | aether2 | Epoch 97: Loss = 5.3548
2025-05-28 05:58:20,225 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:01:36,223 | INFO | aether2 | Epoch 98: Loss = 5.3562
2025-05-28 06:01:36,305 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:04:51,408 | INFO | aether2 | Epoch 99: Loss = 5.3569
2025-05-28 06:04:51,483 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:08:05,897 | INFO | aether2 | Epoch 100: Loss = 5.3566
2025-05-28 06:08:05,974 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:11:20,460 | INFO | aether2 | Epoch 101: Loss = 5.3569
2025-05-28 06:11:20,540 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:14:34,496 | INFO | aether2 | Epoch 102: Loss = 5.3558
2025-05-28 06:14:34,603 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:17:49,044 | INFO | aether2 | Epoch 103: Loss = 5.3555
2025-05-28 06:17:49,145 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:21:04,368 | INFO | aether2 | Epoch 104: Loss = 5.3580
2025-05-28 06:21:04,439 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:24:19,263 | INFO | aether2 | Epoch 105: Loss = 5.3545
2025-05-28 06:24:19,335 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:27:33,282 | INFO | aether2 | Epoch 106: Loss = 5.3558
2025-05-28 06:27:33,355 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:30:47,613 | INFO | aether2 | Epoch 107: Loss = 5.3547
2025-05-28 06:30:47,687 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:34:02,599 | INFO | aether2 | Epoch 108: Loss = 5.3560
2025-05-28 06:34:02,680 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:37:18,570 | INFO | aether2 | Epoch 109: Loss = 5.3554
2025-05-28 06:37:18,646 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:40:34,662 | INFO | aether2 | Epoch 110: Loss = 5.3553
2025-05-28 06:40:34,733 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:43:50,309 | INFO | aether2 | Epoch 111: Loss = 5.3579
2025-05-28 06:43:50,408 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:47:04,451 | INFO | aether2 | Epoch 112: Loss = 5.3548
2025-05-28 06:47:04,532 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:50:18,783 | INFO | aether2 | Epoch 113: Loss = 5.3563
2025-05-28 06:50:18,858 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:53:33,864 | INFO | aether2 | Epoch 114: Loss = 5.3573
2025-05-28 06:53:33,944 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 06:56:48,903 | INFO | aether2 | Epoch 115: Loss = 5.3565
2025-05-28 06:56:49,011 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:00:03,135 | INFO | aether2 | Epoch 116: Loss = 5.3582
2025-05-28 07:00:03,215 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:03:18,732 | INFO | aether2 | Epoch 117: Loss = 5.3583
2025-05-28 07:03:18,810 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:06:32,959 | INFO | aether2 | Epoch 118: Loss = 5.3574
2025-05-28 07:06:33,061 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:09:46,635 | INFO | aether2 | Epoch 119: Loss = 5.3537
2025-05-28 07:09:46,714 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:13:02,136 | INFO | aether2 | Epoch 120: Loss = 5.3563
2025-05-28 07:13:02,208 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:16:17,155 | INFO | aether2 | Epoch 121: Loss = 5.3556
2025-05-28 07:16:17,230 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:19:33,267 | INFO | aether2 | Epoch 122: Loss = 5.3589
2025-05-28 07:19:33,343 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:22:48,642 | INFO | aether2 | Epoch 123: Loss = 5.3575
2025-05-28 07:22:48,718 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:26:03,541 | INFO | aether2 | Epoch 124: Loss = 5.3556
2025-05-28 07:26:03,614 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:29:17,422 | INFO | aether2 | Epoch 125: Loss = 5.3547
2025-05-28 07:29:17,498 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:32:32,932 | INFO | aether2 | Epoch 126: Loss = 5.3551
2025-05-28 07:32:33,007 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:35:48,723 | INFO | aether2 | Epoch 127: Loss = 5.3574
2025-05-28 07:35:48,801 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:39:02,939 | INFO | aether2 | Epoch 128: Loss = 5.3557
2025-05-28 07:39:03,016 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:42:18,162 | INFO | aether2 | Epoch 129: Loss = 5.3559
2025-05-28 07:42:18,241 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:45:34,743 | INFO | aether2 | Epoch 130: Loss = 5.3583
2025-05-28 07:45:34,817 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:48:47,675 | INFO | aether2 | Epoch 131: Loss = 5.3548
2025-05-28 07:48:47,760 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:52:01,943 | INFO | aether2 | Epoch 132: Loss = 5.3559
2025-05-28 07:52:02,025 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:55:17,535 | INFO | aether2 | Epoch 133: Loss = 5.3552
2025-05-28 07:55:17,623 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 07:58:32,765 | INFO | aether2 | Epoch 134: Loss = 5.3582
2025-05-28 07:58:32,868 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:01:47,462 | INFO | aether2 | Epoch 135: Loss = 5.3561
2025-05-28 08:01:47,537 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:05:01,928 | INFO | aether2 | Epoch 136: Loss = 5.3576
2025-05-28 08:05:01,999 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:08:15,669 | INFO | aether2 | Epoch 137: Loss = 5.3565
2025-05-28 08:08:15,741 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:11:30,585 | INFO | aether2 | Epoch 138: Loss = 5.3569
2025-05-28 08:11:30,660 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:14:43,412 | INFO | aether2 | Epoch 139: Loss = 5.3534
2025-05-28 08:14:43,488 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:17:59,034 | INFO | aether2 | Epoch 140: Loss = 5.3567
2025-05-28 08:17:59,115 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:21:14,062 | INFO | aether2 | Epoch 141: Loss = 5.3574
2025-05-28 08:21:14,157 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:24:28,337 | INFO | aether2 | Epoch 142: Loss = 5.3593
2025-05-28 08:24:28,436 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:27:42,676 | INFO | aether2 | Epoch 143: Loss = 5.3549
2025-05-28 08:27:42,759 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:30:57,581 | INFO | aether2 | Epoch 144: Loss = 5.3555
2025-05-28 08:30:57,681 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:34:12,130 | INFO | aether2 | Epoch 145: Loss = 5.3569
2025-05-28 08:34:12,205 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:37:26,507 | INFO | aether2 | Epoch 146: Loss = 5.3554
2025-05-28 08:37:26,588 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:40:42,502 | INFO | aether2 | Epoch 147: Loss = 5.3553
2025-05-28 08:40:42,578 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:43:58,028 | INFO | aether2 | Epoch 148: Loss = 5.3573
2025-05-28 08:43:58,124 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:47:11,965 | INFO | aether2 | Epoch 149: Loss = 5.3552
2025-05-28 08:47:12,042 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:50:27,131 | INFO | aether2 | Epoch 150: Loss = 5.3586
2025-05-28 08:50:27,212 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:53:41,777 | INFO | aether2 | Epoch 151: Loss = 5.3585
2025-05-28 08:53:41,852 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 08:56:57,411 | INFO | aether2 | Epoch 152: Loss = 5.3574
2025-05-28 08:56:57,487 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:00:11,987 | INFO | aether2 | Epoch 153: Loss = 5.3548
2025-05-28 09:00:12,067 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:03:27,065 | INFO | aether2 | Epoch 154: Loss = 5.3563
2025-05-28 09:03:27,146 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:06:42,741 | INFO | aether2 | Epoch 155: Loss = 5.3576
2025-05-28 09:06:42,812 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:09:57,130 | INFO | aether2 | Epoch 156: Loss = 5.3563
2025-05-28 09:09:57,207 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:13:11,168 | INFO | aether2 | Epoch 157: Loss = 5.3543
2025-05-28 09:13:11,241 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:16:26,807 | INFO | aether2 | Epoch 158: Loss = 5.3578
2025-05-28 09:16:26,887 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:19:41,210 | INFO | aether2 | Epoch 159: Loss = 5.3586
2025-05-28 09:19:41,315 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:22:56,822 | INFO | aether2 | Epoch 160: Loss = 5.3576
2025-05-28 09:22:56,898 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:26:10,865 | INFO | aether2 | Epoch 161: Loss = 5.3547
2025-05-28 09:26:10,944 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:29:26,148 | INFO | aether2 | Epoch 162: Loss = 5.3570
2025-05-28 09:29:26,239 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:32:40,512 | INFO | aether2 | Epoch 163: Loss = 5.3578
2025-05-28 09:32:40,590 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:35:54,820 | INFO | aether2 | Epoch 164: Loss = 5.3570
2025-05-28 09:35:54,911 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:39:09,511 | INFO | aether2 | Epoch 165: Loss = 5.3568
2025-05-28 09:39:09,588 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:42:24,065 | INFO | aether2 | Epoch 166: Loss = 5.3550
2025-05-28 09:42:24,164 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:45:39,011 | INFO | aether2 | Epoch 167: Loss = 5.3559
2025-05-28 09:45:39,099 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:48:53,175 | INFO | aether2 | Epoch 168: Loss = 5.3554
2025-05-28 09:48:53,252 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:52:06,919 | INFO | aether2 | Epoch 169: Loss = 5.3551
2025-05-28 09:52:06,992 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:55:20,248 | INFO | aether2 | Epoch 170: Loss = 5.3552
2025-05-28 09:55:20,321 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 09:58:37,156 | INFO | aether2 | Epoch 171: Loss = 5.3568
2025-05-28 09:58:37,243 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:01:55,928 | INFO | aether2 | Epoch 172: Loss = 5.3567
2025-05-28 10:01:56,003 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:05:14,888 | INFO | aether2 | Epoch 173: Loss = 5.3579
2025-05-28 10:05:14,962 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:08:34,053 | INFO | aether2 | Epoch 174: Loss = 5.3569
2025-05-28 10:08:34,129 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:11:52,865 | INFO | aether2 | Epoch 175: Loss = 5.3558
2025-05-28 10:11:52,937 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:15:10,104 | INFO | aether2 | Epoch 176: Loss = 5.3548
2025-05-28 10:15:10,176 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:18:27,152 | INFO | aether2 | Epoch 177: Loss = 5.3570
2025-05-28 10:18:27,233 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:21:46,497 | INFO | aether2 | Epoch 178: Loss = 5.3570
2025-05-28 10:21:46,581 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:25:06,178 | INFO | aether2 | Epoch 179: Loss = 5.3580
2025-05-28 10:25:06,254 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:28:24,596 | INFO | aether2 | Epoch 180: Loss = 5.3561
2025-05-28 10:28:24,693 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:31:42,002 | INFO | aether2 | Epoch 181: Loss = 5.3562
2025-05-28 10:31:42,083 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:34:59,683 | INFO | aether2 | Epoch 182: Loss = 5.3564
2025-05-28 10:34:59,764 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:38:18,069 | INFO | aether2 | Epoch 183: Loss = 5.3537
2025-05-28 10:38:18,143 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:41:36,653 | INFO | aether2 | Epoch 184: Loss = 5.3571
2025-05-28 10:41:36,726 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:44:55,371 | INFO | aether2 | Epoch 185: Loss = 5.3562
2025-05-28 10:44:55,458 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:48:14,999 | INFO | aether2 | Epoch 186: Loss = 5.3568
2025-05-28 10:48:15,095 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:51:33,506 | INFO | aether2 | Epoch 187: Loss = 5.3570
2025-05-28 10:51:33,606 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:54:52,196 | INFO | aether2 | Epoch 188: Loss = 5.3562
2025-05-28 10:54:52,271 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 10:58:10,423 | INFO | aether2 | Epoch 189: Loss = 5.3567
2025-05-28 10:58:10,523 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:01:28,374 | INFO | aether2 | Epoch 190: Loss = 5.3561
2025-05-28 11:01:28,449 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:04:46,346 | INFO | aether2 | Epoch 191: Loss = 5.3555
2025-05-28 11:04:46,419 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:08:05,697 | INFO | aether2 | Epoch 192: Loss = 5.3586
2025-05-28 11:08:05,779 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:11:22,368 | INFO | aether2 | Epoch 193: Loss = 5.3557
2025-05-28 11:11:22,446 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:14:37,738 | INFO | aether2 | Epoch 194: Loss = 5.3580
2025-05-28 11:14:37,817 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:17:51,519 | INFO | aether2 | Epoch 195: Loss = 5.3540
2025-05-28 11:17:51,591 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:21:06,464 | INFO | aether2 | Epoch 196: Loss = 5.3576
2025-05-28 11:21:06,542 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:24:21,747 | INFO | aether2 | Epoch 197: Loss = 5.3598
2025-05-28 11:24:21,819 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:27:37,117 | INFO | aether2 | Epoch 198: Loss = 5.3552
2025-05-28 11:27:37,208 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:30:52,144 | INFO | aether2 | Epoch 199: Loss = 5.3565
2025-05-28 11:30:52,218 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:34:06,894 | INFO | aether2 | Epoch 200: Loss = 5.3548
2025-05-28 11:34:06,969 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:37:22,547 | INFO | aether2 | Epoch 201: Loss = 5.3591
2025-05-28 11:37:22,643 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:40:37,637 | INFO | aether2 | Epoch 202: Loss = 5.3568
2025-05-28 11:40:37,711 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:43:52,481 | INFO | aether2 | Epoch 203: Loss = 5.3546
2025-05-28 11:43:52,559 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:47:07,741 | INFO | aether2 | Epoch 204: Loss = 5.3556
2025-05-28 11:47:07,823 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:50:21,531 | INFO | aether2 | Epoch 205: Loss = 5.3553
2025-05-28 11:50:21,608 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:53:35,808 | INFO | aether2 | Epoch 206: Loss = 5.3551
2025-05-28 11:53:35,881 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 11:56:51,575 | INFO | aether2 | Epoch 207: Loss = 5.3581
2025-05-28 11:56:51,655 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:00:05,936 | INFO | aether2 | Epoch 208: Loss = 5.3556
2025-05-28 12:00:06,011 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:03:22,032 | INFO | aether2 | Epoch 209: Loss = 5.3586
2025-05-28 12:03:22,106 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:06:37,135 | INFO | aether2 | Epoch 210: Loss = 5.3562
2025-05-28 12:06:37,207 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:09:50,407 | INFO | aether2 | Epoch 211: Loss = 5.3566
2025-05-28 12:09:50,485 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:13:05,192 | INFO | aether2 | Epoch 212: Loss = 5.3557
2025-05-28 12:13:05,266 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:16:20,541 | INFO | aether2 | Epoch 213: Loss = 5.3530
2025-05-28 12:16:20,615 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:19:35,437 | INFO | aether2 | Epoch 214: Loss = 5.3545
2025-05-28 12:19:35,515 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:22:51,012 | INFO | aether2 | Epoch 215: Loss = 5.3559
2025-05-28 12:22:51,082 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:26:04,979 | INFO | aether2 | Epoch 216: Loss = 5.3551
2025-05-28 12:26:05,054 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:29:20,320 | INFO | aether2 | Epoch 217: Loss = 5.3582
2025-05-28 12:29:20,392 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:32:35,650 | INFO | aether2 | Epoch 218: Loss = 5.3572
2025-05-28 12:32:35,726 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:35:49,726 | INFO | aether2 | Epoch 219: Loss = 5.3559
2025-05-28 12:35:49,804 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:39:03,134 | INFO | aether2 | Epoch 220: Loss = 5.3547
2025-05-28 12:39:03,204 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:42:17,414 | INFO | aether2 | Epoch 221: Loss = 5.3552
2025-05-28 12:42:17,494 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:45:33,457 | INFO | aether2 | Epoch 222: Loss = 5.3562
2025-05-28 12:45:33,535 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:48:48,635 | INFO | aether2 | Epoch 223: Loss = 5.3559
2025-05-28 12:48:48,706 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:52:03,219 | INFO | aether2 | Epoch 224: Loss = 5.3571
2025-05-28 12:52:03,296 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:55:17,372 | INFO | aether2 | Epoch 225: Loss = 5.3563
2025-05-28 12:55:17,443 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 12:58:31,111 | INFO | aether2 | Epoch 226: Loss = 5.3561
2025-05-28 12:58:31,184 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:01:47,579 | INFO | aether2 | Epoch 227: Loss = 5.3577
2025-05-28 13:01:47,657 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:05:02,821 | INFO | aether2 | Epoch 228: Loss = 5.3564
2025-05-28 13:05:02,895 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:08:18,299 | INFO | aether2 | Epoch 229: Loss = 5.3545
2025-05-28 13:08:18,379 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:11:32,143 | INFO | aether2 | Epoch 230: Loss = 5.3547
2025-05-28 13:11:32,220 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:14:47,550 | INFO | aether2 | Epoch 231: Loss = 5.3573
2025-05-28 13:14:47,623 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:18:03,952 | INFO | aether2 | Epoch 232: Loss = 5.3567
2025-05-28 13:18:04,039 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:21:18,980 | INFO | aether2 | Epoch 233: Loss = 5.3592
2025-05-28 13:21:19,055 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:24:34,212 | INFO | aether2 | Epoch 234: Loss = 5.3567
2025-05-28 13:24:34,310 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:27:47,797 | INFO | aether2 | Epoch 235: Loss = 5.3533
2025-05-28 13:27:47,874 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:31:02,980 | INFO | aether2 | Epoch 236: Loss = 5.3548
2025-05-28 13:31:03,060 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:34:17,748 | INFO | aether2 | Epoch 237: Loss = 5.3543
2025-05-28 13:34:17,826 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:37:32,790 | INFO | aether2 | Epoch 238: Loss = 5.3576
2025-05-28 13:37:32,865 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:40:46,233 | INFO | aether2 | Epoch 239: Loss = 5.3561
2025-05-28 13:40:46,302 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:44:01,174 | INFO | aether2 | Epoch 240: Loss = 5.3553
2025-05-28 13:44:01,253 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:47:15,815 | INFO | aether2 | Epoch 241: Loss = 5.3541
2025-05-28 13:47:15,887 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:50:29,554 | INFO | aether2 | Epoch 242: Loss = 5.3545
2025-05-28 13:50:29,633 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:53:43,307 | INFO | aether2 | Epoch 243: Loss = 5.3550
2025-05-28 13:53:43,385 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 13:56:57,604 | INFO | aether2 | Epoch 244: Loss = 5.3555
2025-05-28 13:56:57,693 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:00:12,674 | INFO | aether2 | Epoch 245: Loss = 5.3565
2025-05-28 14:00:12,767 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:03:29,469 | INFO | aether2 | Epoch 246: Loss = 5.3575
2025-05-28 14:03:29,560 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:06:43,606 | INFO | aether2 | Epoch 247: Loss = 5.3560
2025-05-28 14:06:43,701 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:09:58,749 | INFO | aether2 | Epoch 248: Loss = 5.3559
2025-05-28 14:09:58,825 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:13:13,328 | INFO | aether2 | Epoch 249: Loss = 5.3548
2025-05-28 14:13:13,410 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:16:28,596 | INFO | aether2 | Epoch 250: Loss = 5.3563
2025-05-28 14:16:28,686 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:19:43,919 | INFO | aether2 | Epoch 251: Loss = 5.3572
2025-05-28 14:19:43,995 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:22:58,224 | INFO | aether2 | Epoch 252: Loss = 5.3557
2025-05-28 14:22:58,299 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:26:12,155 | INFO | aether2 | Epoch 253: Loss = 5.3536
2025-05-28 14:26:12,227 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:29:27,366 | INFO | aether2 | Epoch 254: Loss = 5.3576
2025-05-28 14:29:27,436 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:32:42,917 | INFO | aether2 | Epoch 255: Loss = 5.3580
2025-05-28 14:32:42,997 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:35:58,448 | INFO | aether2 | Epoch 256: Loss = 5.3575
2025-05-28 14:35:58,520 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:39:12,940 | INFO | aether2 | Epoch 257: Loss = 5.3541
2025-05-28 14:39:13,016 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:42:26,627 | INFO | aether2 | Epoch 258: Loss = 5.3551
2025-05-28 14:42:26,704 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:45:41,678 | INFO | aether2 | Epoch 259: Loss = 5.3548
2025-05-28 14:45:41,756 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:48:57,098 | INFO | aether2 | Epoch 260: Loss = 5.3556
2025-05-28 14:48:57,197 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:52:12,368 | INFO | aether2 | Epoch 261: Loss = 5.3570
2025-05-28 14:52:12,442 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:55:27,391 | INFO | aether2 | Epoch 262: Loss = 5.3559
2025-05-28 14:55:27,468 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 14:58:41,812 | INFO | aether2 | Epoch 263: Loss = 5.3582
2025-05-28 14:58:41,887 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:01:56,627 | INFO | aether2 | Epoch 264: Loss = 5.3564
2025-05-28 15:01:56,723 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:05:11,286 | INFO | aether2 | Epoch 265: Loss = 5.3565
2025-05-28 15:05:11,367 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:08:25,552 | INFO | aether2 | Epoch 266: Loss = 5.3569
2025-05-28 15:08:25,622 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:11:37,444 | INFO | aether2 | Epoch 267: Loss = 5.3565
2025-05-28 15:11:37,518 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:14:30,084 | INFO | aether2 | Epoch 268: Loss = 5.3560
2025-05-28 15:14:30,160 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:17:24,462 | INFO | aether2 | Epoch 269: Loss = 5.3564
2025-05-28 15:17:24,544 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:20:17,143 | INFO | aether2 | Epoch 270: Loss = 5.3579
2025-05-28 15:20:17,229 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:23:09,517 | INFO | aether2 | Epoch 271: Loss = 5.3559
2025-05-28 15:23:09,585 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:26:01,780 | INFO | aether2 | Epoch 272: Loss = 5.3573
2025-05-28 15:26:01,852 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:28:52,947 | INFO | aether2 | Epoch 273: Loss = 5.3538
2025-05-28 15:28:53,019 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:31:45,445 | INFO | aether2 | Epoch 274: Loss = 5.3549
2025-05-28 15:31:45,519 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:34:38,459 | INFO | aether2 | Epoch 275: Loss = 5.3579
2025-05-28 15:34:38,555 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:37:29,626 | INFO | aether2 | Epoch 276: Loss = 5.3553
2025-05-28 15:37:29,702 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:40:22,307 | INFO | aether2 | Epoch 277: Loss = 5.3587
2025-05-28 15:40:22,382 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:43:16,357 | INFO | aether2 | Epoch 278: Loss = 5.3578
2025-05-28 15:43:16,433 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:46:09,654 | INFO | aether2 | Epoch 279: Loss = 5.3559
2025-05-28 15:46:09,725 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:49:01,748 | INFO | aether2 | Epoch 280: Loss = 5.3569
2025-05-28 15:49:01,817 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:51:53,817 | INFO | aether2 | Epoch 281: Loss = 5.3551
2025-05-28 15:51:53,904 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:54:46,797 | INFO | aether2 | Epoch 282: Loss = 5.3570
2025-05-28 15:54:46,867 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 15:57:39,001 | INFO | aether2 | Epoch 283: Loss = 5.3576
2025-05-28 15:57:39,093 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:00:32,097 | INFO | aether2 | Epoch 284: Loss = 5.3567
2025-05-28 16:00:32,165 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:03:24,787 | INFO | aether2 | Epoch 285: Loss = 5.3540
2025-05-28 16:03:24,855 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:06:17,256 | INFO | aether2 | Epoch 286: Loss = 5.3547
2025-05-28 16:06:17,323 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:09:10,346 | INFO | aether2 | Epoch 287: Loss = 5.3565
2025-05-28 16:09:10,433 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:12:06,400 | INFO | aether2 | Epoch 288: Loss = 5.3558
2025-05-28 16:12:06,469 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:15:00,035 | INFO | aether2 | Epoch 289: Loss = 5.3575
2025-05-28 16:15:00,101 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:17:59,592 | INFO | aether2 | Epoch 290: Loss = 5.3569
2025-05-28 16:17:59,664 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:21:06,214 | INFO | aether2 | Epoch 291: Loss = 5.3548
2025-05-28 16:21:06,305 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:24:11,078 | INFO | aether2 | Epoch 292: Loss = 5.3550
2025-05-28 16:24:11,171 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:27:17,069 | INFO | aether2 | Epoch 293: Loss = 5.3562
2025-05-28 16:27:17,153 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:30:20,851 | INFO | aether2 | Epoch 294: Loss = 5.3562
2025-05-28 16:30:20,930 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:33:20,786 | INFO | aether2 | Epoch 295: Loss = 5.3552
2025-05-28 16:33:20,856 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:36:19,642 | INFO | aether2 | Epoch 296: Loss = 5.3535
2025-05-28 16:36:19,712 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:39:41,375 | INFO | aether2 | Epoch 297: Loss = 5.3560
2025-05-28 16:39:41,471 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:42:57,794 | INFO | aether2 | Epoch 298: Loss = 5.3562
2025-05-28 16:42:57,880 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:46:09,406 | INFO | aether2 | Epoch 299: Loss = 5.3553
2025-05-28 16:46:09,476 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:49:21,467 | INFO | aether2 | Epoch 300: Loss = 5.3574
2025-05-28 16:49:21,533 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:52:42,950 | INFO | aether2 | Epoch 301: Loss = 5.3587
2025-05-28 16:52:43,038 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:55:49,456 | INFO | aether2 | Epoch 302: Loss = 5.3554
2025-05-28 16:55:49,529 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 16:58:53,329 | INFO | aether2 | Epoch 303: Loss = 5.3543
2025-05-28 16:58:53,405 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:02:01,958 | INFO | aether2 | Epoch 304: Loss = 5.3556
2025-05-28 17:02:02,028 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:05:13,784 | INFO | aether2 | Epoch 305: Loss = 5.3559
2025-05-28 17:05:13,884 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:08:20,255 | INFO | aether2 | Epoch 306: Loss = 5.3580
2025-05-28 17:08:20,331 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:11:30,065 | INFO | aether2 | Epoch 307: Loss = 5.3546
2025-05-28 17:11:30,144 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:14:24,220 | INFO | aether2 | Epoch 308: Loss = 5.3547
2025-05-28 17:14:24,300 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:17:17,738 | INFO | aether2 | Epoch 309: Loss = 5.3572
2025-05-28 17:17:17,812 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:20:08,017 | INFO | aether2 | Epoch 310: Loss = 5.3583
2025-05-28 17:20:08,083 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:22:59,363 | INFO | aether2 | Epoch 311: Loss = 5.3553
2025-05-28 17:22:59,435 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:25:50,984 | INFO | aether2 | Epoch 312: Loss = 5.3556
2025-05-28 17:25:51,051 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:28:42,781 | INFO | aether2 | Epoch 313: Loss = 5.3557
2025-05-28 17:28:42,869 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:31:35,528 | INFO | aether2 | Epoch 314: Loss = 5.3544
2025-05-28 17:31:35,600 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:34:26,467 | INFO | aether2 | Epoch 315: Loss = 5.3544
2025-05-28 17:34:26,539 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:37:17,956 | INFO | aether2 | Epoch 316: Loss = 5.3572
2025-05-28 17:37:18,023 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:40:09,062 | INFO | aether2 | Epoch 317: Loss = 5.3557
2025-05-28 17:40:09,152 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:43:01,018 | INFO | aether2 | Epoch 318: Loss = 5.3544
2025-05-28 17:43:01,085 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:45:54,507 | INFO | aether2 | Epoch 319: Loss = 5.3589
2025-05-28 17:45:54,579 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:48:49,026 | INFO | aether2 | Epoch 320: Loss = 5.3579
2025-05-28 17:48:49,096 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:52:09,688 | INFO | aether2 | Epoch 321: Loss = 5.3548
2025-05-28 17:52:09,775 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:55:36,431 | INFO | aether2 | Epoch 322: Loss = 5.3547
2025-05-28 17:55:36,506 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-28 17:56:31,536 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 17:56:31,658 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 17:56:31,658 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 17:56:31,658 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 17:56:31,764 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 17:56:31,769 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 17:56:31,770 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 17:56:31,770 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 17:56:32,149 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 17:56:32,493 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 17:56:32,604 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 17:56:32,605 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 17:56:32,605 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 17:56:32,698 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 17:56:32,702 | INFO | SimpleTokenizer |  Vocab saved to tokenizer_vocab.json
2025-05-28 17:56:32,703 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 17:56:32,703 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 17:57:15,721 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 17:57:15,726 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-28 17:57:15,727 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-28 17:57:16,782 | WARNING | SimpleTokenizer | Unrecognized word: 'whats', replacing with <UNK>
2025-05-28 17:57:16,782 | WARNING | SimpleTokenizer | Unrecognized word: 'name', replacing with <UNK>
2025-05-28 18:07:54,818 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:07:54,924 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 18:07:54,924 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 18:07:54,925 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 18:07:55,024 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 18:07:55,029 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 18:07:55,029 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 18:09:13,805 | INFO | aether2 | GOAL: hi
2025-05-28 18:09:37,813 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 18:09:43,651 | INFO | aether2 | GOAL: Hi 
2025-05-28 18:09:48,998 | INFO | aether2 | GOAL: hi
2025-05-28 18:32:40,461 | ERROR | aether2 |  Could not read the config file.: name 'path' is not defined
2025-05-28 18:38:05,333 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:42:49,283 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:46:19,495 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:46:52,963 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:51:19,093 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:51:19,196 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 18:51:19,196 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 18:51:19,196 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 18:51:19,309 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 18:51:19,323 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 18:51:59,916 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:52:00,023 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 18:52:00,023 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 18:52:00,024 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 18:52:00,126 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 18:52:00,131 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 18:52:00,131 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 18:52:17,568 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 18:53:09,078 | INFO | aether2 | GOAL: hi
2025-05-28 18:56:43,676 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:56:43,777 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 18:56:43,777 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 18:56:43,777 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 18:56:43,876 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 18:56:43,881 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 18:56:43,881 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 18:57:02,562 | INFO | aether2 | GOAL: hi
2025-05-28 18:57:27,793 | INFO | aether2 | GOAL: hi
2025-05-28 18:58:26,336 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:58:26,439 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 18:58:26,439 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 18:58:26,439 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 18:58:26,552 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 18:58:26,559 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 18:58:26,559 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 18:59:59,417 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 18:59:59,517 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 18:59:59,517 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 18:59:59,518 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 18:59:59,629 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 18:59:59,634 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 18:59:59,634 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:00:17,481 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:00:17,592 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:00:17,592 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:00:17,593 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:00:17,689 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:00:17,696 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:00:17,696 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:00:23,869 | INFO | aether2 | GOAL: hi
2025-05-28 19:04:52,416 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:04:52,516 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:04:52,516 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:04:52,516 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:04:52,619 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:04:52,626 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:04:52,626 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:05:02,590 | INFO | aether2 | GOAL: hi
2025-05-28 19:07:11,678 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:07:11,776 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:07:11,777 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:07:11,777 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:07:11,886 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:07:11,892 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:07:11,893 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:07:17,609 | INFO | aether2 | GOAL: hi
2025-05-28 19:08:52,713 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:08:52,818 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:08:52,818 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:08:52,818 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:08:52,917 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:08:52,923 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:08:52,923 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:08:58,251 | INFO | aether2 | GOAL: hi
2025-05-28 19:12:46,178 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:12:46,278 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:12:46,278 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:12:46,278 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:12:46,393 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:12:46,398 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:12:46,398 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:12:51,022 | INFO | aether2 | GOAL: hi
2025-05-28 19:13:55,650 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:13:55,748 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:13:55,749 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:13:55,749 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:13:55,848 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:13:55,853 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:13:55,853 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:14:06,927 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 19:14:16,438 | INFO | aether2 | GOAL: HI
2025-05-28 19:14:21,873 | INFO | aether2 | GOAL: hi
2025-05-28 19:16:47,098 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:16:47,204 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:16:47,204 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:16:47,205 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:16:47,320 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:16:47,326 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:16:47,327 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:17:01,483 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:17:01,583 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:17:01,583 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:17:01,584 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:17:01,681 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:17:01,687 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:17:01,687 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:17:25,572 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:17:25,669 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:17:25,669 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:17:25,669 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:17:25,778 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:17:25,784 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:17:25,784 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:17:51,239 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:17:51,343 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:17:51,343 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:17:51,343 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:17:51,444 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:17:51,449 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:17:51,449 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:18:00,279 | INFO | aether2 | GOAL: Hi
2025-05-28 19:20:39,210 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:20:39,309 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:20:39,309 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:20:39,309 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:20:39,411 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:20:39,416 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:20:39,417 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:20:58,378 | INFO | aether2 | GOAL: hi
2025-05-28 19:20:58,381 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:20:58,399 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:20:58,412 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:20:58,428 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:20:58,451 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:20:58,468 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:20:58,488 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:20:58,505 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:20:58,523 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:20:58,539 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:20:58,554 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:20:58,571 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:20:58,588 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:20:58,606 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:20:58,626 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:20:58,645 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:20:58,661 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:20:58,677 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:20:58,696 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:20:58,713 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:20:58,729 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:20:58,746 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:20:58,764 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:20:58,782 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:20:58,800 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:20:58,820 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:20:58,841 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:20:58,860 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:20:58,880 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:20:58,901 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:20:58,921 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:20:58,943 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:20:58,962 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:20:58,982 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:20:59,002 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:20:59,025 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:20:59,047 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:20:59,065 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:20:59,085 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:20:59,106 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:20:59,127 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:20:59,150 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:20:59,170 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:20:59,189 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:20:59,212 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:20:59,236 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:20:59,260 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:20:59,284 | INFO | aether2 | Model output shape: torch.Size([1, 49, 153])
2025-05-28 19:20:59,304 | INFO | aether2 | Model output shape: torch.Size([1, 50, 153])
2025-05-28 19:20:59,327 | INFO | aether2 | Model output shape: torch.Size([1, 51, 153])
2025-05-28 19:20:59,348 | INFO | aether2 | Model output shape: torch.Size([1, 52, 153])
2025-05-28 19:25:37,553 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:25:37,671 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:25:37,671 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:25:37,672 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:25:37,805 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:25:37,812 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:25:37,812 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:25:43,737 | INFO | aether2 | GOAL: hi
2025-05-28 19:25:43,742 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:25:43,768 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:25:43,805 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:25:43,827 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:25:43,850 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:25:43,866 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:25:43,882 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:25:43,899 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:25:43,915 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:25:43,932 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:25:43,950 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:25:43,968 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:25:43,986 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:25:44,003 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:25:44,022 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:25:44,039 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:25:44,056 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:25:44,074 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:25:44,092 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:25:44,112 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:25:44,131 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:25:44,150 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:25:44,170 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:25:44,188 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:25:44,208 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:25:44,226 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:25:44,247 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:25:44,268 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:25:44,288 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:25:44,308 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:25:44,329 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:25:44,348 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:25:44,367 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:25:44,387 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:25:44,408 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:25:44,430 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:25:44,451 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:25:44,472 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:25:44,497 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:25:44,517 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:25:44,537 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:25:44,557 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:25:44,580 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:25:44,599 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:25:44,620 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:25:44,643 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:25:44,666 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:25:44,689 | INFO | aether2 | Model output shape: torch.Size([1, 49, 153])
2025-05-28 19:25:44,711 | INFO | aether2 | Model output shape: torch.Size([1, 50, 153])
2025-05-28 19:25:44,735 | INFO | aether2 | Model output shape: torch.Size([1, 51, 153])
2025-05-28 19:25:44,756 | INFO | aether2 | Model output shape: torch.Size([1, 52, 153])
2025-05-28 19:28:09,929 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:28:10,033 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:28:10,033 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:28:10,033 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:28:10,185 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:28:10,192 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:28:10,192 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:28:14,504 | INFO | aether2 | GOAL: hi
2025-05-28 19:28:14,508 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:28:14,526 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:28:14,538 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:28:14,552 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:28:14,575 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:28:14,588 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:28:14,603 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:28:14,621 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:28:14,635 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:28:14,650 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:28:14,667 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:28:14,686 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:28:14,703 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:28:14,722 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:28:14,738 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:28:14,753 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:28:14,772 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:28:14,790 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:28:14,807 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:28:14,824 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:28:14,842 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:28:14,860 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:28:14,889 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:28:14,908 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:28:14,925 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:28:14,945 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:28:14,965 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:28:14,987 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:28:15,010 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:28:15,032 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:28:15,050 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:28:15,069 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:28:15,089 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:28:15,107 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:28:15,126 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:28:15,148 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:28:15,168 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:28:15,190 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:28:15,211 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:28:15,234 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:28:15,256 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:28:15,278 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:28:15,301 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:28:15,333 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:28:15,355 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:28:15,378 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:28:15,401 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:28:15,422 | INFO | aether2 | Model output shape: torch.Size([1, 49, 153])
2025-05-28 19:28:15,423 | ERROR | aether2 | Input sequence exceeded max_length (50)!
2025-05-28 19:30:00,700 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:30:00,810 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:30:00,810 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:30:00,810 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:30:00,938 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:30:00,945 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:30:00,945 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:30:06,418 | INFO | aether2 | GOAL: hi
2025-05-28 19:30:06,422 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:30:06,440 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:30:06,440 | ERROR | aether2 | Model output is too short! Output shape: torch.Size([1, 3, 153])
2025-05-28 19:31:59,260 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:31:59,358 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:31:59,358 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:31:59,358 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:31:59,476 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:31:59,483 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:31:59,483 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:32:03,976 | INFO | aether2 | GOAL: hi
2025-05-28 19:32:03,981 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:32:03,998 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:32:04,013 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:32:04,028 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:32:04,048 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:32:04,064 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:32:04,081 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:32:04,097 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:32:04,167 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:32:04,192 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:32:04,209 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:32:04,226 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:32:04,243 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:32:04,260 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:32:04,274 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:32:04,292 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:32:04,310 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:32:04,327 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:32:04,345 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:32:04,362 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:32:04,380 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:32:04,398 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:32:04,414 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:32:04,433 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:32:04,450 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:32:04,467 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:32:04,484 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:32:04,503 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:32:04,522 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:32:04,540 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:32:04,558 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:32:04,577 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:32:04,596 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:32:04,613 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:32:04,633 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:32:04,653 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:32:04,672 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:32:04,692 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:32:04,712 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:32:04,732 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:32:04,751 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:32:04,771 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:32:04,793 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:32:04,815 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:32:04,837 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:32:04,859 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:32:04,879 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:32:04,880 | ERROR | aether2 | Input sequence exceeded max_length (50)! Stopping.
2025-05-28 19:35:15,560 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:35:15,661 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:35:15,662 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:35:15,662 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:35:15,767 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:35:15,773 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:35:15,773 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:35:20,472 | INFO | aether2 | GOAL: hi
2025-05-28 19:35:20,476 | INFO | aether2 | Using dynamic max_length: 50
2025-05-28 19:35:20,476 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:35:20,495 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:35:20,509 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:35:20,525 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:35:20,545 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:35:20,561 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:35:20,578 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:35:20,595 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:35:20,611 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:35:20,628 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:35:20,645 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:35:20,662 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:35:20,679 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:35:20,698 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:35:20,717 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:35:20,735 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:35:20,752 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:35:20,770 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:35:20,788 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:35:20,806 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:35:20,824 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:35:20,842 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:35:20,861 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:35:20,880 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:35:20,896 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:35:20,916 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:35:20,933 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:35:20,950 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:35:20,968 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:35:20,987 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:35:21,007 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:35:21,025 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:35:21,045 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:35:21,065 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:35:21,086 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:35:21,105 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:35:21,127 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:35:21,148 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:35:21,169 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:35:21,189 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:35:21,209 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:35:21,230 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:35:21,250 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:35:21,271 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:35:21,293 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:35:21,316 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:35:21,335 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:35:21,336 | WARNING | aether2 | Input sequence exceeded dynamic max_length (50)! Stopping.
2025-05-28 19:36:20,979 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:36:21,092 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:36:21,092 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:36:21,092 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:36:21,214 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:36:21,221 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:36:21,221 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:36:28,630 | INFO | aether2 | GOAL: hi
2025-05-28 19:36:28,634 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:36:28,653 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:36:28,665 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:36:28,681 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:36:28,705 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:36:28,721 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:36:28,739 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:36:28,756 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:36:28,772 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:36:28,788 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:36:28,807 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:36:28,825 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:36:28,843 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:36:28,860 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:36:28,879 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:36:28,895 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:36:28,914 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:36:28,933 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:36:28,950 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:36:28,968 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:36:28,986 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:36:29,006 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:36:29,025 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:36:29,045 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:36:29,064 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:36:29,082 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:36:29,100 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:36:29,121 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:36:29,141 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:36:29,162 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:36:29,180 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:36:29,202 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:36:29,224 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:36:29,245 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:36:29,267 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:36:29,285 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:36:29,306 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:36:29,328 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:36:29,349 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:36:29,367 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:36:29,389 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:36:29,410 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:36:29,430 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:36:29,450 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:36:29,470 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:36:29,492 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:36:29,513 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:36:29,535 | INFO | aether2 | Model output shape: torch.Size([1, 49, 153])
2025-05-28 19:36:29,557 | INFO | aether2 | Model output shape: torch.Size([1, 50, 153])
2025-05-28 19:36:29,580 | INFO | aether2 | Model output shape: torch.Size([1, 51, 153])
2025-05-28 19:36:29,601 | INFO | aether2 | Model output shape: torch.Size([1, 52, 153])
2025-05-28 19:36:29,624 | INFO | aether2 | Model output shape: torch.Size([1, 53, 153])
2025-05-28 19:36:29,647 | INFO | aether2 | Model output shape: torch.Size([1, 54, 153])
2025-05-28 19:36:29,670 | INFO | aether2 | Model output shape: torch.Size([1, 55, 153])
2025-05-28 19:36:29,692 | INFO | aether2 | Model output shape: torch.Size([1, 56, 153])
2025-05-28 19:36:29,718 | INFO | aether2 | Model output shape: torch.Size([1, 57, 153])
2025-05-28 19:36:29,741 | INFO | aether2 | Model output shape: torch.Size([1, 58, 153])
2025-05-28 19:36:29,764 | INFO | aether2 | Model output shape: torch.Size([1, 59, 153])
2025-05-28 19:36:29,787 | INFO | aether2 | Model output shape: torch.Size([1, 60, 153])
2025-05-28 19:36:29,809 | INFO | aether2 | Model output shape: torch.Size([1, 61, 153])
2025-05-28 19:36:29,834 | INFO | aether2 | Model output shape: torch.Size([1, 62, 153])
2025-05-28 19:36:29,858 | INFO | aether2 | Model output shape: torch.Size([1, 63, 153])
2025-05-28 19:36:29,882 | INFO | aether2 | Model output shape: torch.Size([1, 64, 153])
2025-05-28 19:36:29,905 | INFO | aether2 | Model output shape: torch.Size([1, 65, 153])
2025-05-28 19:36:29,927 | INFO | aether2 | Model output shape: torch.Size([1, 66, 153])
2025-05-28 19:36:29,952 | INFO | aether2 | Model output shape: torch.Size([1, 67, 153])
2025-05-28 19:36:29,974 | INFO | aether2 | Model output shape: torch.Size([1, 68, 153])
2025-05-28 19:36:30,000 | INFO | aether2 | Model output shape: torch.Size([1, 69, 153])
2025-05-28 19:36:30,023 | INFO | aether2 | Model output shape: torch.Size([1, 70, 153])
2025-05-28 19:36:30,045 | INFO | aether2 | Model output shape: torch.Size([1, 71, 153])
2025-05-28 19:36:30,070 | INFO | aether2 | Model output shape: torch.Size([1, 72, 153])
2025-05-28 19:36:30,093 | INFO | aether2 | Model output shape: torch.Size([1, 73, 153])
2025-05-28 19:36:30,118 | INFO | aether2 | Model output shape: torch.Size([1, 74, 153])
2025-05-28 19:36:30,146 | INFO | aether2 | Model output shape: torch.Size([1, 75, 153])
2025-05-28 19:36:30,170 | INFO | aether2 | Model output shape: torch.Size([1, 76, 153])
2025-05-28 19:36:30,196 | INFO | aether2 | Model output shape: torch.Size([1, 77, 153])
2025-05-28 19:36:30,222 | INFO | aether2 | Model output shape: torch.Size([1, 78, 153])
2025-05-28 19:36:30,250 | INFO | aether2 | Model output shape: torch.Size([1, 79, 153])
2025-05-28 19:36:30,273 | INFO | aether2 | Model output shape: torch.Size([1, 80, 153])
2025-05-28 19:36:30,302 | INFO | aether2 | Model output shape: torch.Size([1, 81, 153])
2025-05-28 19:36:30,327 | INFO | aether2 | Model output shape: torch.Size([1, 82, 153])
2025-05-28 19:36:30,353 | INFO | aether2 | Model output shape: torch.Size([1, 83, 153])
2025-05-28 19:36:30,380 | INFO | aether2 | Model output shape: torch.Size([1, 84, 153])
2025-05-28 19:36:30,407 | INFO | aether2 | Model output shape: torch.Size([1, 85, 153])
2025-05-28 19:36:30,432 | INFO | aether2 | Model output shape: torch.Size([1, 86, 153])
2025-05-28 19:36:30,462 | INFO | aether2 | Model output shape: torch.Size([1, 87, 153])
2025-05-28 19:36:30,486 | INFO | aether2 | Model output shape: torch.Size([1, 88, 153])
2025-05-28 19:36:30,512 | INFO | aether2 | Model output shape: torch.Size([1, 89, 153])
2025-05-28 19:36:30,540 | INFO | aether2 | Model output shape: torch.Size([1, 90, 153])
2025-05-28 19:36:30,570 | INFO | aether2 | Model output shape: torch.Size([1, 91, 153])
2025-05-28 19:36:30,599 | INFO | aether2 | Model output shape: torch.Size([1, 92, 153])
2025-05-28 19:36:30,627 | INFO | aether2 | Model output shape: torch.Size([1, 93, 153])
2025-05-28 19:36:30,655 | INFO | aether2 | Model output shape: torch.Size([1, 94, 153])
2025-05-28 19:36:30,683 | INFO | aether2 | Model output shape: torch.Size([1, 95, 153])
2025-05-28 19:36:30,711 | INFO | aether2 | Model output shape: torch.Size([1, 96, 153])
2025-05-28 19:36:30,740 | INFO | aether2 | Model output shape: torch.Size([1, 97, 153])
2025-05-28 19:36:30,768 | INFO | aether2 | Model output shape: torch.Size([1, 98, 153])
2025-05-28 19:36:30,796 | INFO | aether2 | Model output shape: torch.Size([1, 99, 153])
2025-05-28 19:36:30,828 | INFO | aether2 | Model output shape: torch.Size([1, 100, 153])
2025-05-28 19:36:30,856 | INFO | aether2 | Model output shape: torch.Size([1, 101, 153])
2025-05-28 19:36:30,886 | INFO | aether2 | Model output shape: torch.Size([1, 102, 153])
2025-05-28 19:36:30,918 | INFO | aether2 | Model output shape: torch.Size([1, 103, 153])
2025-05-28 19:36:30,948 | INFO | aether2 | Model output shape: torch.Size([1, 104, 153])
2025-05-28 19:36:30,977 | INFO | aether2 | Model output shape: torch.Size([1, 105, 153])
2025-05-28 19:36:31,005 | INFO | aether2 | Model output shape: torch.Size([1, 106, 153])
2025-05-28 19:36:31,036 | INFO | aether2 | Model output shape: torch.Size([1, 107, 153])
2025-05-28 19:36:31,068 | INFO | aether2 | Model output shape: torch.Size([1, 108, 153])
2025-05-28 19:36:31,101 | INFO | aether2 | Model output shape: torch.Size([1, 109, 153])
2025-05-28 19:36:31,134 | INFO | aether2 | Model output shape: torch.Size([1, 110, 153])
2025-05-28 19:36:31,167 | INFO | aether2 | Model output shape: torch.Size([1, 111, 153])
2025-05-28 19:36:31,198 | INFO | aether2 | Model output shape: torch.Size([1, 112, 153])
2025-05-28 19:36:31,229 | INFO | aether2 | Model output shape: torch.Size([1, 113, 153])
2025-05-28 19:36:31,262 | INFO | aether2 | Model output shape: torch.Size([1, 114, 153])
2025-05-28 19:36:31,293 | INFO | aether2 | Model output shape: torch.Size([1, 115, 153])
2025-05-28 19:36:31,326 | INFO | aether2 | Model output shape: torch.Size([1, 116, 153])
2025-05-28 19:36:31,358 | INFO | aether2 | Model output shape: torch.Size([1, 117, 153])
2025-05-28 19:36:31,390 | INFO | aether2 | Model output shape: torch.Size([1, 118, 153])
2025-05-28 19:36:31,422 | INFO | aether2 | Model output shape: torch.Size([1, 119, 153])
2025-05-28 19:36:31,454 | INFO | aether2 | Model output shape: torch.Size([1, 120, 153])
2025-05-28 19:36:31,489 | INFO | aether2 | Model output shape: torch.Size([1, 121, 153])
2025-05-28 19:36:31,523 | INFO | aether2 | Model output shape: torch.Size([1, 122, 153])
2025-05-28 19:36:31,558 | INFO | aether2 | Model output shape: torch.Size([1, 123, 153])
2025-05-28 19:36:31,591 | INFO | aether2 | Model output shape: torch.Size([1, 124, 153])
2025-05-28 19:36:31,624 | INFO | aether2 | Model output shape: torch.Size([1, 125, 153])
2025-05-28 19:36:31,656 | INFO | aether2 | Model output shape: torch.Size([1, 126, 153])
2025-05-28 19:36:31,691 | INFO | aether2 | Model output shape: torch.Size([1, 127, 153])
2025-05-28 19:36:31,728 | INFO | aether2 | Model output shape: torch.Size([1, 128, 153])
2025-05-28 19:36:31,761 | INFO | aether2 | Model output shape: torch.Size([1, 129, 153])
2025-05-28 19:36:31,794 | INFO | aether2 | Model output shape: torch.Size([1, 130, 153])
2025-05-28 19:36:31,830 | INFO | aether2 | Model output shape: torch.Size([1, 131, 153])
2025-05-28 19:36:31,865 | INFO | aether2 | Model output shape: torch.Size([1, 132, 153])
2025-05-28 19:36:31,898 | INFO | aether2 | Model output shape: torch.Size([1, 133, 153])
2025-05-28 19:36:31,932 | INFO | aether2 | Model output shape: torch.Size([1, 134, 153])
2025-05-28 19:36:31,966 | INFO | aether2 | Model output shape: torch.Size([1, 135, 153])
2025-05-28 19:36:32,000 | INFO | aether2 | Model output shape: torch.Size([1, 136, 153])
2025-05-28 19:36:32,034 | INFO | aether2 | Model output shape: torch.Size([1, 137, 153])
2025-05-28 19:36:32,071 | INFO | aether2 | Model output shape: torch.Size([1, 138, 153])
2025-05-28 19:36:32,107 | INFO | aether2 | Model output shape: torch.Size([1, 139, 153])
2025-05-28 19:36:32,141 | INFO | aether2 | Model output shape: torch.Size([1, 140, 153])
2025-05-28 19:36:32,173 | INFO | aether2 | Model output shape: torch.Size([1, 141, 153])
2025-05-28 19:36:32,208 | INFO | aether2 | Model output shape: torch.Size([1, 142, 153])
2025-05-28 19:36:32,245 | INFO | aether2 | Model output shape: torch.Size([1, 143, 153])
2025-05-28 19:36:32,285 | INFO | aether2 | Model output shape: torch.Size([1, 144, 153])
2025-05-28 19:36:32,322 | INFO | aether2 | Model output shape: torch.Size([1, 145, 153])
2025-05-28 19:36:32,358 | INFO | aether2 | Model output shape: torch.Size([1, 146, 153])
2025-05-28 19:36:32,397 | INFO | aether2 | Model output shape: torch.Size([1, 147, 153])
2025-05-28 19:36:32,433 | INFO | aether2 | Model output shape: torch.Size([1, 148, 153])
2025-05-28 19:36:32,469 | INFO | aether2 | Model output shape: torch.Size([1, 149, 153])
2025-05-28 19:36:32,505 | INFO | aether2 | Model output shape: torch.Size([1, 150, 153])
2025-05-28 19:36:32,543 | INFO | aether2 | Model output shape: torch.Size([1, 151, 153])
2025-05-28 19:36:32,582 | INFO | aether2 | Model output shape: torch.Size([1, 152, 153])
2025-05-28 19:36:32,621 | INFO | aether2 | Model output shape: torch.Size([1, 153, 153])
2025-05-28 19:36:32,660 | INFO | aether2 | Model output shape: torch.Size([1, 154, 153])
2025-05-28 19:36:32,699 | INFO | aether2 | Model output shape: torch.Size([1, 155, 153])
2025-05-28 19:36:32,737 | INFO | aether2 | Model output shape: torch.Size([1, 156, 153])
2025-05-28 19:36:32,776 | INFO | aether2 | Model output shape: torch.Size([1, 157, 153])
2025-05-28 19:36:32,879 | INFO | aether2 | Model output shape: torch.Size([1, 158, 153])
2025-05-28 19:36:32,925 | INFO | aether2 | Model output shape: torch.Size([1, 159, 153])
2025-05-28 19:36:32,969 | INFO | aether2 | Model output shape: torch.Size([1, 160, 153])
2025-05-28 19:36:33,014 | INFO | aether2 | Model output shape: torch.Size([1, 161, 153])
2025-05-28 19:36:33,057 | INFO | aether2 | Model output shape: torch.Size([1, 162, 153])
2025-05-28 19:36:33,098 | INFO | aether2 | Model output shape: torch.Size([1, 163, 153])
2025-05-28 19:36:33,139 | INFO | aether2 | Model output shape: torch.Size([1, 164, 153])
2025-05-28 19:36:33,181 | INFO | aether2 | Model output shape: torch.Size([1, 165, 153])
2025-05-28 19:36:33,221 | INFO | aether2 | Model output shape: torch.Size([1, 166, 153])
2025-05-28 19:36:33,259 | INFO | aether2 | Model output shape: torch.Size([1, 167, 153])
2025-05-28 19:36:33,298 | INFO | aether2 | Model output shape: torch.Size([1, 168, 153])
2025-05-28 19:36:33,338 | INFO | aether2 | Model output shape: torch.Size([1, 169, 153])
2025-05-28 19:36:33,378 | INFO | aether2 | Model output shape: torch.Size([1, 170, 153])
2025-05-28 19:36:33,420 | INFO | aether2 | Model output shape: torch.Size([1, 171, 153])
2025-05-28 19:36:33,462 | INFO | aether2 | Model output shape: torch.Size([1, 172, 153])
2025-05-28 19:36:33,505 | INFO | aether2 | Model output shape: torch.Size([1, 173, 153])
2025-05-28 19:36:33,548 | INFO | aether2 | Model output shape: torch.Size([1, 174, 153])
2025-05-28 19:36:33,588 | INFO | aether2 | Model output shape: torch.Size([1, 175, 153])
2025-05-28 19:36:33,629 | INFO | aether2 | Model output shape: torch.Size([1, 176, 153])
2025-05-28 19:36:33,675 | INFO | aether2 | Model output shape: torch.Size([1, 177, 153])
2025-05-28 19:36:33,718 | INFO | aether2 | Model output shape: torch.Size([1, 178, 153])
2025-05-28 19:36:33,763 | INFO | aether2 | Model output shape: torch.Size([1, 179, 153])
2025-05-28 19:36:33,807 | INFO | aether2 | Model output shape: torch.Size([1, 180, 153])
2025-05-28 19:36:33,848 | INFO | aether2 | Model output shape: torch.Size([1, 181, 153])
2025-05-28 19:36:33,890 | INFO | aether2 | Model output shape: torch.Size([1, 182, 153])
2025-05-28 19:36:33,935 | INFO | aether2 | Model output shape: torch.Size([1, 183, 153])
2025-05-28 19:36:33,979 | INFO | aether2 | Model output shape: torch.Size([1, 184, 153])
2025-05-28 19:36:34,022 | INFO | aether2 | Model output shape: torch.Size([1, 185, 153])
2025-05-28 19:36:34,068 | INFO | aether2 | Model output shape: torch.Size([1, 186, 153])
2025-05-28 19:36:34,113 | INFO | aether2 | Model output shape: torch.Size([1, 187, 153])
2025-05-28 19:36:34,159 | INFO | aether2 | Model output shape: torch.Size([1, 188, 153])
2025-05-28 19:36:34,204 | INFO | aether2 | Model output shape: torch.Size([1, 189, 153])
2025-05-28 19:36:34,247 | INFO | aether2 | Model output shape: torch.Size([1, 190, 153])
2025-05-28 19:36:34,292 | INFO | aether2 | Model output shape: torch.Size([1, 191, 153])
2025-05-28 19:36:34,337 | INFO | aether2 | Model output shape: torch.Size([1, 192, 153])
2025-05-28 19:36:34,384 | INFO | aether2 | Model output shape: torch.Size([1, 193, 153])
2025-05-28 19:36:34,431 | INFO | aether2 | Model output shape: torch.Size([1, 194, 153])
2025-05-28 19:36:34,478 | INFO | aether2 | Model output shape: torch.Size([1, 195, 153])
2025-05-28 19:36:34,524 | INFO | aether2 | Model output shape: torch.Size([1, 196, 153])
2025-05-28 19:36:34,571 | INFO | aether2 | Model output shape: torch.Size([1, 197, 153])
2025-05-28 19:36:34,618 | INFO | aether2 | Model output shape: torch.Size([1, 198, 153])
2025-05-28 19:36:34,665 | INFO | aether2 | Model output shape: torch.Size([1, 199, 153])
2025-05-28 19:36:34,714 | INFO | aether2 | Model output shape: torch.Size([1, 200, 153])
2025-05-28 19:36:34,764 | INFO | aether2 | Model output shape: torch.Size([1, 201, 153])
2025-05-28 19:36:34,813 | INFO | aether2 | Model output shape: torch.Size([1, 202, 153])
2025-05-28 19:36:34,863 | INFO | aether2 | Model output shape: torch.Size([1, 203, 153])
2025-05-28 19:36:34,910 | INFO | aether2 | Model output shape: torch.Size([1, 204, 153])
2025-05-28 19:36:34,961 | INFO | aether2 | Model output shape: torch.Size([1, 205, 153])
2025-05-28 19:36:35,005 | INFO | aether2 | Model output shape: torch.Size([1, 206, 153])
2025-05-28 19:36:35,055 | INFO | aether2 | Model output shape: torch.Size([1, 207, 153])
2025-05-28 19:36:35,106 | INFO | aether2 | Model output shape: torch.Size([1, 208, 153])
2025-05-28 19:36:35,153 | INFO | aether2 | Model output shape: torch.Size([1, 209, 153])
2025-05-28 19:36:35,200 | INFO | aether2 | Model output shape: torch.Size([1, 210, 153])
2025-05-28 19:36:35,250 | INFO | aether2 | Model output shape: torch.Size([1, 211, 153])
2025-05-28 19:36:35,298 | INFO | aether2 | Model output shape: torch.Size([1, 212, 153])
2025-05-28 19:36:35,348 | INFO | aether2 | Model output shape: torch.Size([1, 213, 153])
2025-05-28 19:36:35,402 | INFO | aether2 | Model output shape: torch.Size([1, 214, 153])
2025-05-28 19:36:35,457 | INFO | aether2 | Model output shape: torch.Size([1, 215, 153])
2025-05-28 19:36:35,507 | INFO | aether2 | Model output shape: torch.Size([1, 216, 153])
2025-05-28 19:36:35,562 | INFO | aether2 | Model output shape: torch.Size([1, 217, 153])
2025-05-28 19:36:35,614 | INFO | aether2 | Model output shape: torch.Size([1, 218, 153])
2025-05-28 19:36:35,665 | INFO | aether2 | Model output shape: torch.Size([1, 219, 153])
2025-05-28 19:36:35,719 | INFO | aether2 | Model output shape: torch.Size([1, 220, 153])
2025-05-28 19:36:35,772 | INFO | aether2 | Model output shape: torch.Size([1, 221, 153])
2025-05-28 19:36:35,825 | INFO | aether2 | Model output shape: torch.Size([1, 222, 153])
2025-05-28 19:36:35,879 | INFO | aether2 | Model output shape: torch.Size([1, 223, 153])
2025-05-28 19:36:35,935 | INFO | aether2 | Model output shape: torch.Size([1, 224, 153])
2025-05-28 19:36:35,991 | INFO | aether2 | Model output shape: torch.Size([1, 225, 153])
2025-05-28 19:36:36,045 | INFO | aether2 | Model output shape: torch.Size([1, 226, 153])
2025-05-28 19:36:36,098 | INFO | aether2 | Model output shape: torch.Size([1, 227, 153])
2025-05-28 19:36:36,149 | INFO | aether2 | Model output shape: torch.Size([1, 228, 153])
2025-05-28 19:36:36,201 | INFO | aether2 | Model output shape: torch.Size([1, 229, 153])
2025-05-28 19:36:36,249 | INFO | aether2 | Model output shape: torch.Size([1, 230, 153])
2025-05-28 19:36:36,303 | INFO | aether2 | Model output shape: torch.Size([1, 231, 153])
2025-05-28 19:36:36,351 | INFO | aether2 | Model output shape: torch.Size([1, 232, 153])
2025-05-28 19:36:36,405 | INFO | aether2 | Model output shape: torch.Size([1, 233, 153])
2025-05-28 19:36:36,455 | INFO | aether2 | Model output shape: torch.Size([1, 234, 153])
2025-05-28 19:36:36,508 | INFO | aether2 | Model output shape: torch.Size([1, 235, 153])
2025-05-28 19:36:36,562 | INFO | aether2 | Model output shape: torch.Size([1, 236, 153])
2025-05-28 19:36:36,618 | INFO | aether2 | Model output shape: torch.Size([1, 237, 153])
2025-05-28 19:36:36,670 | INFO | aether2 | Model output shape: torch.Size([1, 238, 153])
2025-05-28 19:36:36,726 | INFO | aether2 | Model output shape: torch.Size([1, 239, 153])
2025-05-28 19:36:36,781 | INFO | aether2 | Model output shape: torch.Size([1, 240, 153])
2025-05-28 19:36:36,836 | INFO | aether2 | Model output shape: torch.Size([1, 241, 153])
2025-05-28 19:36:36,886 | INFO | aether2 | Model output shape: torch.Size([1, 242, 153])
2025-05-28 19:36:36,939 | INFO | aether2 | Model output shape: torch.Size([1, 243, 153])
2025-05-28 19:36:36,993 | INFO | aether2 | Model output shape: torch.Size([1, 244, 153])
2025-05-28 19:36:37,048 | INFO | aether2 | Model output shape: torch.Size([1, 245, 153])
2025-05-28 19:36:37,103 | INFO | aether2 | Model output shape: torch.Size([1, 246, 153])
2025-05-28 19:36:37,162 | INFO | aether2 | Model output shape: torch.Size([1, 247, 153])
2025-05-28 19:36:37,216 | INFO | aether2 | Model output shape: torch.Size([1, 248, 153])
2025-05-28 19:36:37,269 | INFO | aether2 | Model output shape: torch.Size([1, 249, 153])
2025-05-28 19:36:37,323 | INFO | aether2 | Model output shape: torch.Size([1, 250, 153])
2025-05-28 19:36:37,379 | INFO | aether2 | Model output shape: torch.Size([1, 251, 153])
2025-05-28 19:36:37,432 | INFO | aether2 | Model output shape: torch.Size([1, 252, 153])
2025-05-28 19:36:37,490 | INFO | aether2 | Model output shape: torch.Size([1, 253, 153])
2025-05-28 19:36:37,548 | INFO | aether2 | Model output shape: torch.Size([1, 254, 153])
2025-05-28 19:36:37,603 | INFO | aether2 | Model output shape: torch.Size([1, 255, 153])
2025-05-28 19:36:37,658 | INFO | aether2 | Model output shape: torch.Size([1, 256, 153])
2025-05-28 19:36:37,720 | INFO | aether2 | Model output shape: torch.Size([1, 257, 153])
2025-05-28 19:36:37,776 | INFO | aether2 | Model output shape: torch.Size([1, 258, 153])
2025-05-28 19:36:37,835 | INFO | aether2 | Model output shape: torch.Size([1, 259, 153])
2025-05-28 19:36:37,895 | INFO | aether2 | Model output shape: torch.Size([1, 260, 153])
2025-05-28 19:36:37,961 | INFO | aether2 | Model output shape: torch.Size([1, 261, 153])
2025-05-28 19:36:38,027 | INFO | aether2 | Model output shape: torch.Size([1, 262, 153])
2025-05-28 19:36:38,095 | INFO | aether2 | Model output shape: torch.Size([1, 263, 153])
2025-05-28 19:36:38,156 | INFO | aether2 | Model output shape: torch.Size([1, 264, 153])
2025-05-28 19:36:38,218 | INFO | aether2 | Model output shape: torch.Size([1, 265, 153])
2025-05-28 19:36:38,279 | INFO | aether2 | Model output shape: torch.Size([1, 266, 153])
2025-05-28 19:36:38,343 | INFO | aether2 | Model output shape: torch.Size([1, 267, 153])
2025-05-28 19:36:38,399 | INFO | aether2 | Model output shape: torch.Size([1, 268, 153])
2025-05-28 19:36:38,463 | INFO | aether2 | Model output shape: torch.Size([1, 269, 153])
2025-05-28 19:36:38,522 | INFO | aether2 | Model output shape: torch.Size([1, 270, 153])
2025-05-28 19:36:38,584 | INFO | aether2 | Model output shape: torch.Size([1, 271, 153])
2025-05-28 19:36:38,642 | INFO | aether2 | Model output shape: torch.Size([1, 272, 153])
2025-05-28 19:36:38,705 | INFO | aether2 | Model output shape: torch.Size([1, 273, 153])
2025-05-28 19:36:38,769 | INFO | aether2 | Model output shape: torch.Size([1, 274, 153])
2025-05-28 19:36:38,839 | INFO | aether2 | Model output shape: torch.Size([1, 275, 153])
2025-05-28 19:36:38,902 | INFO | aether2 | Model output shape: torch.Size([1, 276, 153])
2025-05-28 19:36:38,964 | INFO | aether2 | Model output shape: torch.Size([1, 277, 153])
2025-05-28 19:36:39,028 | INFO | aether2 | Model output shape: torch.Size([1, 278, 153])
2025-05-28 19:36:39,088 | INFO | aether2 | Model output shape: torch.Size([1, 279, 153])
2025-05-28 19:36:39,147 | INFO | aether2 | Model output shape: torch.Size([1, 280, 153])
2025-05-28 19:36:39,208 | INFO | aether2 | Model output shape: torch.Size([1, 281, 153])
2025-05-28 19:36:39,273 | INFO | aether2 | Model output shape: torch.Size([1, 282, 153])
2025-05-28 19:36:39,339 | INFO | aether2 | Model output shape: torch.Size([1, 283, 153])
2025-05-28 19:36:39,399 | INFO | aether2 | Model output shape: torch.Size([1, 284, 153])
2025-05-28 19:36:39,459 | INFO | aether2 | Model output shape: torch.Size([1, 285, 153])
2025-05-28 19:36:39,523 | INFO | aether2 | Model output shape: torch.Size([1, 286, 153])
2025-05-28 19:36:39,589 | INFO | aether2 | Model output shape: torch.Size([1, 287, 153])
2025-05-28 19:36:39,650 | INFO | aether2 | Model output shape: torch.Size([1, 288, 153])
2025-05-28 19:36:39,714 | INFO | aether2 | Model output shape: torch.Size([1, 289, 153])
2025-05-28 19:36:39,781 | INFO | aether2 | Model output shape: torch.Size([1, 290, 153])
2025-05-28 19:36:39,846 | INFO | aether2 | Model output shape: torch.Size([1, 291, 153])
2025-05-28 19:36:39,913 | INFO | aether2 | Model output shape: torch.Size([1, 292, 153])
2025-05-28 19:36:39,980 | INFO | aether2 | Model output shape: torch.Size([1, 293, 153])
2025-05-28 19:36:40,044 | INFO | aether2 | Model output shape: torch.Size([1, 294, 153])
2025-05-28 19:36:40,112 | INFO | aether2 | Model output shape: torch.Size([1, 295, 153])
2025-05-28 19:36:40,181 | INFO | aether2 | Model output shape: torch.Size([1, 296, 153])
2025-05-28 19:36:40,250 | INFO | aether2 | Model output shape: torch.Size([1, 297, 153])
2025-05-28 19:36:40,320 | INFO | aether2 | Model output shape: torch.Size([1, 298, 153])
2025-05-28 19:36:40,386 | INFO | aether2 | Model output shape: torch.Size([1, 299, 153])
2025-05-28 19:36:40,453 | INFO | aether2 | Model output shape: torch.Size([1, 300, 153])
2025-05-28 19:36:40,517 | INFO | aether2 | Model output shape: torch.Size([1, 301, 153])
2025-05-28 19:36:40,584 | INFO | aether2 | Model output shape: torch.Size([1, 302, 153])
2025-05-28 19:36:40,654 | INFO | aether2 | Model output shape: torch.Size([1, 303, 153])
2025-05-28 19:36:40,726 | INFO | aether2 | Model output shape: torch.Size([1, 304, 153])
2025-05-28 19:36:40,797 | INFO | aether2 | Model output shape: torch.Size([1, 305, 153])
2025-05-28 19:36:40,869 | INFO | aether2 | Model output shape: torch.Size([1, 306, 153])
2025-05-28 19:36:40,944 | INFO | aether2 | Model output shape: torch.Size([1, 307, 153])
2025-05-28 19:36:41,014 | INFO | aether2 | Model output shape: torch.Size([1, 308, 153])
2025-05-28 19:36:41,087 | INFO | aether2 | Model output shape: torch.Size([1, 309, 153])
2025-05-28 19:36:41,160 | INFO | aether2 | Model output shape: torch.Size([1, 310, 153])
2025-05-28 19:36:41,234 | INFO | aether2 | Model output shape: torch.Size([1, 311, 153])
2025-05-28 19:36:41,306 | INFO | aether2 | Model output shape: torch.Size([1, 312, 153])
2025-05-28 19:36:41,379 | INFO | aether2 | Model output shape: torch.Size([1, 313, 153])
2025-05-28 19:36:41,454 | INFO | aether2 | Model output shape: torch.Size([1, 314, 153])
2025-05-28 19:36:41,529 | INFO | aether2 | Model output shape: torch.Size([1, 315, 153])
2025-05-28 19:36:41,605 | INFO | aether2 | Model output shape: torch.Size([1, 316, 153])
2025-05-28 19:36:41,678 | INFO | aether2 | Model output shape: torch.Size([1, 317, 153])
2025-05-28 19:36:41,755 | INFO | aether2 | Model output shape: torch.Size([1, 318, 153])
2025-05-28 19:36:41,834 | INFO | aether2 | Model output shape: torch.Size([1, 319, 153])
2025-05-28 19:36:41,908 | INFO | aether2 | Model output shape: torch.Size([1, 320, 153])
2025-05-28 19:36:41,984 | INFO | aether2 | Model output shape: torch.Size([1, 321, 153])
2025-05-28 19:36:42,056 | INFO | aether2 | Model output shape: torch.Size([1, 322, 153])
2025-05-28 19:36:42,133 | INFO | aether2 | Model output shape: torch.Size([1, 323, 153])
2025-05-28 19:36:42,209 | INFO | aether2 | Model output shape: torch.Size([1, 324, 153])
2025-05-28 19:36:42,285 | INFO | aether2 | Model output shape: torch.Size([1, 325, 153])
2025-05-28 19:36:42,361 | INFO | aether2 | Model output shape: torch.Size([1, 326, 153])
2025-05-28 19:36:42,439 | INFO | aether2 | Model output shape: torch.Size([1, 327, 153])
2025-05-28 19:36:42,514 | INFO | aether2 | Model output shape: torch.Size([1, 328, 153])
2025-05-28 19:36:42,593 | INFO | aether2 | Model output shape: torch.Size([1, 329, 153])
2025-05-28 19:36:42,671 | INFO | aether2 | Model output shape: torch.Size([1, 330, 153])
2025-05-28 19:36:42,746 | INFO | aether2 | Model output shape: torch.Size([1, 331, 153])
2025-05-28 19:36:42,824 | INFO | aether2 | Model output shape: torch.Size([1, 332, 153])
2025-05-28 19:36:42,903 | INFO | aether2 | Model output shape: torch.Size([1, 333, 153])
2025-05-28 19:36:42,991 | INFO | aether2 | Model output shape: torch.Size([1, 334, 153])
2025-05-28 19:36:43,079 | INFO | aether2 | Model output shape: torch.Size([1, 335, 153])
2025-05-28 19:36:43,159 | INFO | aether2 | Model output shape: torch.Size([1, 336, 153])
2025-05-28 19:36:43,237 | INFO | aether2 | Model output shape: torch.Size([1, 337, 153])
2025-05-28 19:36:43,315 | INFO | aether2 | Model output shape: torch.Size([1, 338, 153])
2025-05-28 19:36:43,399 | INFO | aether2 | Model output shape: torch.Size([1, 339, 153])
2025-05-28 19:36:43,476 | INFO | aether2 | Model output shape: torch.Size([1, 340, 153])
2025-05-28 19:36:43,557 | INFO | aether2 | Model output shape: torch.Size([1, 341, 153])
2025-05-28 19:36:43,640 | INFO | aether2 | Model output shape: torch.Size([1, 342, 153])
2025-05-28 19:36:43,720 | INFO | aether2 | Model output shape: torch.Size([1, 343, 153])
2025-05-28 19:36:43,799 | INFO | aether2 | Model output shape: torch.Size([1, 344, 153])
2025-05-28 19:36:43,883 | INFO | aether2 | Model output shape: torch.Size([1, 345, 153])
2025-05-28 19:36:43,970 | INFO | aether2 | Model output shape: torch.Size([1, 346, 153])
2025-05-28 19:36:44,054 | INFO | aether2 | Model output shape: torch.Size([1, 347, 153])
2025-05-28 19:36:44,139 | INFO | aether2 | Model output shape: torch.Size([1, 348, 153])
2025-05-28 19:36:44,221 | INFO | aether2 | Model output shape: torch.Size([1, 349, 153])
2025-05-28 19:36:44,306 | INFO | aether2 | Model output shape: torch.Size([1, 350, 153])
2025-05-28 19:36:44,392 | INFO | aether2 | Model output shape: torch.Size([1, 351, 153])
2025-05-28 19:36:44,481 | INFO | aether2 | Model output shape: torch.Size([1, 352, 153])
2025-05-28 19:36:44,573 | INFO | aether2 | Model output shape: torch.Size([1, 353, 153])
2025-05-28 19:36:44,666 | INFO | aether2 | Model output shape: torch.Size([1, 354, 153])
2025-05-28 19:36:44,746 | INFO | aether2 | Model output shape: torch.Size([1, 355, 153])
2025-05-28 19:36:44,834 | INFO | aether2 | Model output shape: torch.Size([1, 356, 153])
2025-05-28 19:36:44,923 | INFO | aether2 | Model output shape: torch.Size([1, 357, 153])
2025-05-28 19:36:45,009 | INFO | aether2 | Model output shape: torch.Size([1, 358, 153])
2025-05-28 19:36:45,095 | INFO | aether2 | Model output shape: torch.Size([1, 359, 153])
2025-05-28 19:36:45,183 | INFO | aether2 | Model output shape: torch.Size([1, 360, 153])
2025-05-28 19:36:45,271 | INFO | aether2 | Model output shape: torch.Size([1, 361, 153])
2025-05-28 19:36:45,359 | INFO | aether2 | Model output shape: torch.Size([1, 362, 153])
2025-05-28 19:36:45,450 | INFO | aether2 | Model output shape: torch.Size([1, 363, 153])
2025-05-28 19:36:45,541 | INFO | aether2 | Model output shape: torch.Size([1, 364, 153])
2025-05-28 19:36:45,633 | INFO | aether2 | Model output shape: torch.Size([1, 365, 153])
2025-05-28 19:36:45,724 | INFO | aether2 | Model output shape: torch.Size([1, 366, 153])
2025-05-28 19:36:45,818 | INFO | aether2 | Model output shape: torch.Size([1, 367, 153])
2025-05-28 19:36:45,906 | INFO | aether2 | Model output shape: torch.Size([1, 368, 153])
2025-05-28 19:36:46,047 | INFO | aether2 | Model output shape: torch.Size([1, 369, 153])
2025-05-28 19:36:46,134 | INFO | aether2 | Model output shape: torch.Size([1, 370, 153])
2025-05-28 19:36:46,229 | INFO | aether2 | Model output shape: torch.Size([1, 371, 153])
2025-05-28 19:36:46,320 | INFO | aether2 | Model output shape: torch.Size([1, 372, 153])
2025-05-28 19:36:46,407 | INFO | aether2 | Model output shape: torch.Size([1, 373, 153])
2025-05-28 19:36:46,497 | INFO | aether2 | Model output shape: torch.Size([1, 374, 153])
2025-05-28 19:36:46,588 | INFO | aether2 | Model output shape: torch.Size([1, 375, 153])
2025-05-28 19:36:46,682 | INFO | aether2 | Model output shape: torch.Size([1, 376, 153])
2025-05-28 19:36:46,773 | INFO | aether2 | Model output shape: torch.Size([1, 377, 153])
2025-05-28 19:36:46,871 | INFO | aether2 | Model output shape: torch.Size([1, 378, 153])
2025-05-28 19:36:46,967 | INFO | aether2 | Model output shape: torch.Size([1, 379, 153])
2025-05-28 19:36:47,062 | INFO | aether2 | Model output shape: torch.Size([1, 380, 153])
2025-05-28 19:36:47,155 | INFO | aether2 | Model output shape: torch.Size([1, 381, 153])
2025-05-28 19:36:47,252 | INFO | aether2 | Model output shape: torch.Size([1, 382, 153])
2025-05-28 19:36:47,347 | INFO | aether2 | Model output shape: torch.Size([1, 383, 153])
2025-05-28 19:36:47,441 | INFO | aether2 | Model output shape: torch.Size([1, 384, 153])
2025-05-28 19:36:47,537 | INFO | aether2 | Model output shape: torch.Size([1, 385, 153])
2025-05-28 19:36:47,635 | INFO | aether2 | Model output shape: torch.Size([1, 386, 153])
2025-05-28 19:36:47,731 | INFO | aether2 | Model output shape: torch.Size([1, 387, 153])
2025-05-28 19:36:47,829 | INFO | aether2 | Model output shape: torch.Size([1, 388, 153])
2025-05-28 19:36:47,930 | INFO | aether2 | Model output shape: torch.Size([1, 389, 153])
2025-05-28 19:36:48,036 | INFO | aether2 | Model output shape: torch.Size([1, 390, 153])
2025-05-28 19:36:48,143 | INFO | aether2 | Model output shape: torch.Size([1, 391, 153])
2025-05-28 19:36:48,239 | INFO | aether2 | Model output shape: torch.Size([1, 392, 153])
2025-05-28 19:36:48,341 | INFO | aether2 | Model output shape: torch.Size([1, 393, 153])
2025-05-28 19:36:48,436 | INFO | aether2 | Model output shape: torch.Size([1, 394, 153])
2025-05-28 19:36:48,530 | INFO | aether2 | Model output shape: torch.Size([1, 395, 153])
2025-05-28 19:36:48,629 | INFO | aether2 | Model output shape: torch.Size([1, 396, 153])
2025-05-28 19:36:48,733 | INFO | aether2 | Model output shape: torch.Size([1, 397, 153])
2025-05-28 19:36:48,834 | INFO | aether2 | Model output shape: torch.Size([1, 398, 153])
2025-05-28 19:36:48,931 | INFO | aether2 | Model output shape: torch.Size([1, 399, 153])
2025-05-28 19:36:49,028 | INFO | aether2 | Model output shape: torch.Size([1, 400, 153])
2025-05-28 19:36:49,132 | INFO | aether2 | Model output shape: torch.Size([1, 401, 153])
2025-05-28 19:36:49,237 | INFO | aether2 | Model output shape: torch.Size([1, 402, 153])
2025-05-28 19:36:49,339 | INFO | aether2 | Model output shape: torch.Size([1, 403, 153])
2025-05-28 19:36:49,444 | INFO | aether2 | Model output shape: torch.Size([1, 404, 153])
2025-05-28 19:36:49,553 | INFO | aether2 | Model output shape: torch.Size([1, 405, 153])
2025-05-28 19:36:49,657 | INFO | aether2 | Model output shape: torch.Size([1, 406, 153])
2025-05-28 19:36:49,759 | INFO | aether2 | Model output shape: torch.Size([1, 407, 153])
2025-05-28 19:36:49,860 | INFO | aether2 | Model output shape: torch.Size([1, 408, 153])
2025-05-28 19:36:49,965 | INFO | aether2 | Model output shape: torch.Size([1, 409, 153])
2025-05-28 19:36:50,071 | INFO | aether2 | Model output shape: torch.Size([1, 410, 153])
2025-05-28 19:36:50,173 | INFO | aether2 | Model output shape: torch.Size([1, 411, 153])
2025-05-28 19:36:50,277 | INFO | aether2 | Model output shape: torch.Size([1, 412, 153])
2025-05-28 19:36:50,383 | INFO | aether2 | Model output shape: torch.Size([1, 413, 153])
2025-05-28 19:36:50,489 | INFO | aether2 | Model output shape: torch.Size([1, 414, 153])
2025-05-28 19:36:50,596 | INFO | aether2 | Model output shape: torch.Size([1, 415, 153])
2025-05-28 19:36:50,701 | INFO | aether2 | Model output shape: torch.Size([1, 416, 153])
2025-05-28 19:36:50,807 | INFO | aether2 | Model output shape: torch.Size([1, 417, 153])
2025-05-28 19:36:50,919 | INFO | aether2 | Model output shape: torch.Size([1, 418, 153])
2025-05-28 19:36:51,027 | INFO | aether2 | Model output shape: torch.Size([1, 419, 153])
2025-05-28 19:36:51,136 | INFO | aether2 | Model output shape: torch.Size([1, 420, 153])
2025-05-28 19:36:51,244 | INFO | aether2 | Model output shape: torch.Size([1, 421, 153])
2025-05-28 19:36:51,355 | INFO | aether2 | Model output shape: torch.Size([1, 422, 153])
2025-05-28 19:36:51,466 | INFO | aether2 | Model output shape: torch.Size([1, 423, 153])
2025-05-28 19:36:51,574 | INFO | aether2 | Model output shape: torch.Size([1, 424, 153])
2025-05-28 19:36:51,685 | INFO | aether2 | Model output shape: torch.Size([1, 425, 153])
2025-05-28 19:36:51,798 | INFO | aether2 | Model output shape: torch.Size([1, 426, 153])
2025-05-28 19:36:51,911 | INFO | aether2 | Model output shape: torch.Size([1, 427, 153])
2025-05-28 19:36:52,026 | INFO | aether2 | Model output shape: torch.Size([1, 428, 153])
2025-05-28 19:36:52,136 | INFO | aether2 | Model output shape: torch.Size([1, 429, 153])
2025-05-28 19:36:52,252 | INFO | aether2 | Model output shape: torch.Size([1, 430, 153])
2025-05-28 19:36:52,368 | INFO | aether2 | Model output shape: torch.Size([1, 431, 153])
2025-05-28 19:36:52,484 | INFO | aether2 | Model output shape: torch.Size([1, 432, 153])
2025-05-28 19:36:52,598 | INFO | aether2 | Model output shape: torch.Size([1, 433, 153])
2025-05-28 19:36:52,713 | INFO | aether2 | Model output shape: torch.Size([1, 434, 153])
2025-05-28 19:36:52,827 | INFO | aether2 | Model output shape: torch.Size([1, 435, 153])
2025-05-28 19:36:52,942 | INFO | aether2 | Model output shape: torch.Size([1, 436, 153])
2025-05-28 19:36:53,069 | INFO | aether2 | Model output shape: torch.Size([1, 437, 153])
2025-05-28 19:36:53,191 | INFO | aether2 | Model output shape: torch.Size([1, 438, 153])
2025-05-28 19:36:53,305 | INFO | aether2 | Model output shape: torch.Size([1, 439, 153])
2025-05-28 19:36:53,419 | INFO | aether2 | Model output shape: torch.Size([1, 440, 153])
2025-05-28 19:36:53,537 | INFO | aether2 | Model output shape: torch.Size([1, 441, 153])
2025-05-28 19:36:53,660 | INFO | aether2 | Model output shape: torch.Size([1, 442, 153])
2025-05-28 19:36:53,775 | INFO | aether2 | Model output shape: torch.Size([1, 443, 153])
2025-05-28 19:36:53,896 | INFO | aether2 | Model output shape: torch.Size([1, 444, 153])
2025-05-28 19:36:54,017 | INFO | aether2 | Model output shape: torch.Size([1, 445, 153])
2025-05-28 19:36:54,132 | INFO | aether2 | Model output shape: torch.Size([1, 446, 153])
2025-05-28 19:36:54,250 | INFO | aether2 | Model output shape: torch.Size([1, 447, 153])
2025-05-28 19:36:54,365 | INFO | aether2 | Model output shape: torch.Size([1, 448, 153])
2025-05-28 19:36:54,488 | INFO | aether2 | Model output shape: torch.Size([1, 449, 153])
2025-05-28 19:36:54,603 | INFO | aether2 | Model output shape: torch.Size([1, 450, 153])
2025-05-28 19:36:54,730 | INFO | aether2 | Model output shape: torch.Size([1, 451, 153])
2025-05-28 19:36:54,852 | INFO | aether2 | Model output shape: torch.Size([1, 452, 153])
2025-05-28 19:36:54,977 | INFO | aether2 | Model output shape: torch.Size([1, 453, 153])
2025-05-28 19:36:55,100 | INFO | aether2 | Model output shape: torch.Size([1, 454, 153])
2025-05-28 19:36:55,219 | INFO | aether2 | Model output shape: torch.Size([1, 455, 153])
2025-05-28 19:36:55,340 | INFO | aether2 | Model output shape: torch.Size([1, 456, 153])
2025-05-28 19:36:55,465 | INFO | aether2 | Model output shape: torch.Size([1, 457, 153])
2025-05-28 19:36:55,588 | INFO | aether2 | Model output shape: torch.Size([1, 458, 153])
2025-05-28 19:36:55,713 | INFO | aether2 | Model output shape: torch.Size([1, 459, 153])
2025-05-28 19:36:55,833 | INFO | aether2 | Model output shape: torch.Size([1, 460, 153])
2025-05-28 19:36:55,959 | INFO | aether2 | Model output shape: torch.Size([1, 461, 153])
2025-05-28 19:36:56,085 | INFO | aether2 | Model output shape: torch.Size([1, 462, 153])
2025-05-28 19:36:56,210 | INFO | aether2 | Model output shape: torch.Size([1, 463, 153])
2025-05-28 19:36:56,334 | INFO | aether2 | Model output shape: torch.Size([1, 464, 153])
2025-05-28 19:36:56,462 | INFO | aether2 | Model output shape: torch.Size([1, 465, 153])
2025-05-28 19:36:56,586 | INFO | aether2 | Model output shape: torch.Size([1, 466, 153])
2025-05-28 19:36:56,716 | INFO | aether2 | Model output shape: torch.Size([1, 467, 153])
2025-05-28 19:36:56,842 | INFO | aether2 | Model output shape: torch.Size([1, 468, 153])
2025-05-28 19:36:56,968 | INFO | aether2 | Model output shape: torch.Size([1, 469, 153])
2025-05-28 19:36:57,104 | INFO | aether2 | Model output shape: torch.Size([1, 470, 153])
2025-05-28 19:36:57,230 | INFO | aether2 | Model output shape: torch.Size([1, 471, 153])
2025-05-28 19:36:57,357 | INFO | aether2 | Model output shape: torch.Size([1, 472, 153])
2025-05-28 19:36:57,486 | INFO | aether2 | Model output shape: torch.Size([1, 473, 153])
2025-05-28 19:36:57,617 | INFO | aether2 | Model output shape: torch.Size([1, 474, 153])
2025-05-28 19:36:57,743 | INFO | aether2 | Model output shape: torch.Size([1, 475, 153])
2025-05-28 19:36:57,875 | INFO | aether2 | Model output shape: torch.Size([1, 476, 153])
2025-05-28 19:36:58,014 | INFO | aether2 | Model output shape: torch.Size([1, 477, 153])
2025-05-28 19:36:58,152 | INFO | aether2 | Model output shape: torch.Size([1, 478, 153])
2025-05-28 19:36:58,283 | INFO | aether2 | Model output shape: torch.Size([1, 479, 153])
2025-05-28 19:36:58,410 | INFO | aether2 | Model output shape: torch.Size([1, 480, 153])
2025-05-28 19:36:58,545 | INFO | aether2 | Model output shape: torch.Size([1, 481, 153])
2025-05-28 19:36:58,671 | INFO | aether2 | Model output shape: torch.Size([1, 482, 153])
2025-05-28 19:36:58,804 | INFO | aether2 | Model output shape: torch.Size([1, 483, 153])
2025-05-28 19:36:58,937 | INFO | aether2 | Model output shape: torch.Size([1, 484, 153])
2025-05-28 19:36:59,069 | INFO | aether2 | Model output shape: torch.Size([1, 485, 153])
2025-05-28 19:36:59,203 | INFO | aether2 | Model output shape: torch.Size([1, 486, 153])
2025-05-28 19:36:59,335 | INFO | aether2 | Model output shape: torch.Size([1, 487, 153])
2025-05-28 19:36:59,469 | INFO | aether2 | Model output shape: torch.Size([1, 488, 153])
2025-05-28 19:36:59,609 | INFO | aether2 | Model output shape: torch.Size([1, 489, 153])
2025-05-28 19:36:59,743 | INFO | aether2 | Model output shape: torch.Size([1, 490, 153])
2025-05-28 19:36:59,883 | INFO | aether2 | Model output shape: torch.Size([1, 491, 153])
2025-05-28 19:37:00,009 | INFO | aether2 | Model output shape: torch.Size([1, 492, 153])
2025-05-28 19:37:00,152 | INFO | aether2 | Model output shape: torch.Size([1, 493, 153])
2025-05-28 19:37:00,290 | INFO | aether2 | Model output shape: torch.Size([1, 494, 153])
2025-05-28 19:37:00,425 | INFO | aether2 | Model output shape: torch.Size([1, 495, 153])
2025-05-28 19:37:00,563 | INFO | aether2 | Model output shape: torch.Size([1, 496, 153])
2025-05-28 19:37:00,706 | INFO | aether2 | Model output shape: torch.Size([1, 497, 153])
2025-05-28 19:37:00,846 | INFO | aether2 | Model output shape: torch.Size([1, 498, 153])
2025-05-28 19:37:00,988 | INFO | aether2 | Model output shape: torch.Size([1, 499, 153])
2025-05-28 19:37:01,125 | INFO | aether2 | Model output shape: torch.Size([1, 500, 153])
2025-05-28 19:37:01,266 | INFO | aether2 | Model output shape: torch.Size([1, 501, 153])
2025-05-28 19:37:01,403 | INFO | aether2 | Model output shape: torch.Size([1, 502, 153])
2025-05-28 19:37:01,543 | INFO | aether2 | Model output shape: torch.Size([1, 503, 153])
2025-05-28 19:37:01,680 | INFO | aether2 | Model output shape: torch.Size([1, 504, 153])
2025-05-28 19:37:01,824 | INFO | aether2 | Model output shape: torch.Size([1, 505, 153])
2025-05-28 19:37:01,966 | INFO | aether2 | Model output shape: torch.Size([1, 506, 153])
2025-05-28 19:37:02,104 | INFO | aether2 | Model output shape: torch.Size([1, 507, 153])
2025-05-28 19:37:02,247 | INFO | aether2 | Model output shape: torch.Size([1, 508, 153])
2025-05-28 19:37:02,392 | INFO | aether2 | Model output shape: torch.Size([1, 509, 153])
2025-05-28 19:37:02,538 | INFO | aether2 | Model output shape: torch.Size([1, 510, 153])
2025-05-28 19:37:02,690 | INFO | aether2 | Model output shape: torch.Size([1, 511, 153])
2025-05-28 19:37:02,833 | INFO | aether2 | Model output shape: torch.Size([1, 512, 153])
2025-05-28 19:37:02,984 | INFO | aether2 | Model output shape: torch.Size([1, 513, 153])
2025-05-28 19:37:03,143 | INFO | aether2 | Model output shape: torch.Size([1, 514, 153])
2025-05-28 19:37:03,295 | INFO | aether2 | Model output shape: torch.Size([1, 515, 153])
2025-05-28 19:37:03,440 | INFO | aether2 | Model output shape: torch.Size([1, 516, 153])
2025-05-28 19:37:03,589 | INFO | aether2 | Model output shape: torch.Size([1, 517, 153])
2025-05-28 19:37:03,736 | INFO | aether2 | Model output shape: torch.Size([1, 518, 153])
2025-05-28 19:37:03,885 | INFO | aether2 | Model output shape: torch.Size([1, 519, 153])
2025-05-28 19:37:04,039 | INFO | aether2 | Model output shape: torch.Size([1, 520, 153])
2025-05-28 19:37:04,191 | INFO | aether2 | Model output shape: torch.Size([1, 521, 153])
2025-05-28 19:37:04,340 | INFO | aether2 | Model output shape: torch.Size([1, 522, 153])
2025-05-28 19:37:04,489 | INFO | aether2 | Model output shape: torch.Size([1, 523, 153])
2025-05-28 19:37:04,641 | INFO | aether2 | Model output shape: torch.Size([1, 524, 153])
2025-05-28 19:37:04,801 | INFO | aether2 | Model output shape: torch.Size([1, 525, 153])
2025-05-28 19:37:04,963 | INFO | aether2 | Model output shape: torch.Size([1, 526, 153])
2025-05-28 19:37:05,121 | INFO | aether2 | Model output shape: torch.Size([1, 527, 153])
2025-05-28 19:37:05,272 | INFO | aether2 | Model output shape: torch.Size([1, 528, 153])
2025-05-28 19:37:05,424 | INFO | aether2 | Model output shape: torch.Size([1, 529, 153])
2025-05-28 19:37:05,582 | INFO | aether2 | Model output shape: torch.Size([1, 530, 153])
2025-05-28 19:37:05,736 | INFO | aether2 | Model output shape: torch.Size([1, 531, 153])
2025-05-28 19:37:05,890 | INFO | aether2 | Model output shape: torch.Size([1, 532, 153])
2025-05-28 19:37:06,052 | INFO | aether2 | Model output shape: torch.Size([1, 533, 153])
2025-05-28 19:37:06,211 | INFO | aether2 | Model output shape: torch.Size([1, 534, 153])
2025-05-28 19:37:06,367 | INFO | aether2 | Model output shape: torch.Size([1, 535, 153])
2025-05-28 19:37:06,522 | INFO | aether2 | Model output shape: torch.Size([1, 536, 153])
2025-05-28 19:37:06,681 | INFO | aether2 | Model output shape: torch.Size([1, 537, 153])
2025-05-28 19:37:06,843 | INFO | aether2 | Model output shape: torch.Size([1, 538, 153])
2025-05-28 19:37:07,005 | INFO | aether2 | Model output shape: torch.Size([1, 539, 153])
2025-05-28 19:37:07,205 | INFO | aether2 | Model output shape: torch.Size([1, 540, 153])
2025-05-28 19:37:07,368 | INFO | aether2 | Model output shape: torch.Size([1, 541, 153])
2025-05-28 19:37:07,531 | INFO | aether2 | Model output shape: torch.Size([1, 542, 153])
2025-05-28 19:37:07,693 | INFO | aether2 | Model output shape: torch.Size([1, 543, 153])
2025-05-28 19:37:07,851 | INFO | aether2 | Model output shape: torch.Size([1, 544, 153])
2025-05-28 19:37:08,023 | INFO | aether2 | Model output shape: torch.Size([1, 545, 153])
2025-05-28 19:37:08,199 | INFO | aether2 | Model output shape: torch.Size([1, 546, 153])
2025-05-28 19:37:08,365 | INFO | aether2 | Model output shape: torch.Size([1, 547, 153])
2025-05-28 19:37:08,532 | INFO | aether2 | Model output shape: torch.Size([1, 548, 153])
2025-05-28 19:37:08,698 | INFO | aether2 | Model output shape: torch.Size([1, 549, 153])
2025-05-28 19:37:08,866 | INFO | aether2 | Model output shape: torch.Size([1, 550, 153])
2025-05-28 19:37:09,034 | INFO | aether2 | Model output shape: torch.Size([1, 551, 153])
2025-05-28 19:37:09,195 | INFO | aether2 | Model output shape: torch.Size([1, 552, 153])
2025-05-28 19:37:09,359 | INFO | aether2 | Model output shape: torch.Size([1, 553, 153])
2025-05-28 19:37:09,529 | INFO | aether2 | Model output shape: torch.Size([1, 554, 153])
2025-05-28 19:37:09,695 | INFO | aether2 | Model output shape: torch.Size([1, 555, 153])
2025-05-28 19:37:09,861 | INFO | aether2 | Model output shape: torch.Size([1, 556, 153])
2025-05-28 19:37:10,038 | INFO | aether2 | Model output shape: torch.Size([1, 557, 153])
2025-05-28 19:37:10,207 | INFO | aether2 | Model output shape: torch.Size([1, 558, 153])
2025-05-28 19:37:10,386 | INFO | aether2 | Model output shape: torch.Size([1, 559, 153])
2025-05-28 19:37:10,556 | INFO | aether2 | Model output shape: torch.Size([1, 560, 153])
2025-05-28 19:37:10,729 | INFO | aether2 | Model output shape: torch.Size([1, 561, 153])
2025-05-28 19:37:10,904 | INFO | aether2 | Model output shape: torch.Size([1, 562, 153])
2025-05-28 19:37:11,080 | INFO | aether2 | Model output shape: torch.Size([1, 563, 153])
2025-05-28 19:37:11,247 | INFO | aether2 | Model output shape: torch.Size([1, 564, 153])
2025-05-28 19:37:11,417 | INFO | aether2 | Model output shape: torch.Size([1, 565, 153])
2025-05-28 19:37:11,598 | INFO | aether2 | Model output shape: torch.Size([1, 566, 153])
2025-05-28 19:37:11,768 | INFO | aether2 | Model output shape: torch.Size([1, 567, 153])
2025-05-28 19:37:11,934 | INFO | aether2 | Model output shape: torch.Size([1, 568, 153])
2025-05-28 19:37:12,115 | INFO | aether2 | Model output shape: torch.Size([1, 569, 153])
2025-05-28 19:37:12,286 | INFO | aether2 | Model output shape: torch.Size([1, 570, 153])
2025-05-28 19:37:12,458 | INFO | aether2 | Model output shape: torch.Size([1, 571, 153])
2025-05-28 19:37:12,640 | INFO | aether2 | Model output shape: torch.Size([1, 572, 153])
2025-05-28 19:37:12,823 | INFO | aether2 | Model output shape: torch.Size([1, 573, 153])
2025-05-28 19:37:13,008 | INFO | aether2 | Model output shape: torch.Size([1, 574, 153])
2025-05-28 19:37:13,205 | INFO | aether2 | Model output shape: torch.Size([1, 575, 153])
2025-05-28 19:37:13,394 | INFO | aether2 | Model output shape: torch.Size([1, 576, 153])
2025-05-28 19:37:13,608 | INFO | aether2 | Model output shape: torch.Size([1, 577, 153])
2025-05-28 19:37:13,808 | INFO | aether2 | Model output shape: torch.Size([1, 578, 153])
2025-05-28 19:37:14,006 | INFO | aether2 | Model output shape: torch.Size([1, 579, 153])
2025-05-28 19:37:14,221 | INFO | aether2 | Model output shape: torch.Size([1, 580, 153])
2025-05-28 19:37:14,426 | INFO | aether2 | Model output shape: torch.Size([1, 581, 153])
2025-05-28 19:37:14,626 | INFO | aether2 | Model output shape: torch.Size([1, 582, 153])
2025-05-28 19:37:14,836 | INFO | aether2 | Model output shape: torch.Size([1, 583, 153])
2025-05-28 19:37:15,030 | INFO | aether2 | Model output shape: torch.Size([1, 584, 153])
2025-05-28 19:37:15,305 | INFO | aether2 | Model output shape: torch.Size([1, 585, 153])
2025-05-28 19:37:15,518 | INFO | aether2 | Model output shape: torch.Size([1, 586, 153])
2025-05-28 19:37:15,726 | INFO | aether2 | Model output shape: torch.Size([1, 587, 153])
2025-05-28 19:37:15,927 | INFO | aether2 | Model output shape: torch.Size([1, 588, 153])
2025-05-28 19:37:16,126 | INFO | aether2 | Model output shape: torch.Size([1, 589, 153])
2025-05-28 19:37:16,322 | INFO | aether2 | Model output shape: torch.Size([1, 590, 153])
2025-05-28 19:37:16,521 | INFO | aether2 | Model output shape: torch.Size([1, 591, 153])
2025-05-28 19:37:16,727 | INFO | aether2 | Model output shape: torch.Size([1, 592, 153])
2025-05-28 19:37:16,951 | INFO | aether2 | Model output shape: torch.Size([1, 593, 153])
2025-05-28 19:37:17,176 | INFO | aether2 | Model output shape: torch.Size([1, 594, 153])
2025-05-28 19:37:17,404 | INFO | aether2 | Model output shape: torch.Size([1, 595, 153])
2025-05-28 19:37:17,609 | INFO | aether2 | Model output shape: torch.Size([1, 596, 153])
2025-05-28 19:37:17,822 | INFO | aether2 | Model output shape: torch.Size([1, 597, 153])
2025-05-28 19:37:18,036 | INFO | aether2 | Model output shape: torch.Size([1, 598, 153])
2025-05-28 19:37:18,283 | INFO | aether2 | Model output shape: torch.Size([1, 599, 153])
2025-05-28 19:37:18,493 | INFO | aether2 | Model output shape: torch.Size([1, 600, 153])
2025-05-28 19:37:18,711 | INFO | aether2 | Model output shape: torch.Size([1, 601, 153])
2025-05-28 19:37:18,930 | INFO | aether2 | Model output shape: torch.Size([1, 602, 153])
2025-05-28 19:37:19,142 | INFO | aether2 | Model output shape: torch.Size([1, 603, 153])
2025-05-28 19:37:19,329 | INFO | aether2 | Model output shape: torch.Size([1, 604, 153])
2025-05-28 19:37:19,520 | INFO | aether2 | Model output shape: torch.Size([1, 605, 153])
2025-05-28 19:37:19,728 | INFO | aether2 | Model output shape: torch.Size([1, 606, 153])
2025-05-28 19:37:19,932 | INFO | aether2 | Model output shape: torch.Size([1, 607, 153])
2025-05-28 19:37:20,126 | INFO | aether2 | Model output shape: torch.Size([1, 608, 153])
2025-05-28 19:37:20,333 | INFO | aether2 | Model output shape: torch.Size([1, 609, 153])
2025-05-28 19:37:20,529 | INFO | aether2 | Model output shape: torch.Size([1, 610, 153])
2025-05-28 19:37:20,732 | INFO | aether2 | Model output shape: torch.Size([1, 611, 153])
2025-05-28 19:37:20,928 | INFO | aether2 | Model output shape: torch.Size([1, 612, 153])
2025-05-28 19:37:21,142 | INFO | aether2 | Model output shape: torch.Size([1, 613, 153])
2025-05-28 19:37:21,344 | INFO | aether2 | Model output shape: torch.Size([1, 614, 153])
2025-05-28 19:37:21,614 | INFO | aether2 | Model output shape: torch.Size([1, 615, 153])
2025-05-28 19:37:21,879 | INFO | aether2 | Model output shape: torch.Size([1, 616, 153])
2025-05-28 19:37:22,089 | INFO | aether2 | Model output shape: torch.Size([1, 617, 153])
2025-05-28 19:37:22,292 | INFO | aether2 | Model output shape: torch.Size([1, 618, 153])
2025-05-28 19:37:22,507 | INFO | aether2 | Model output shape: torch.Size([1, 619, 153])
2025-05-28 19:37:22,745 | INFO | aether2 | Model output shape: torch.Size([1, 620, 153])
2025-05-28 19:37:23,012 | INFO | aether2 | Model output shape: torch.Size([1, 621, 153])
2025-05-28 19:37:23,315 | INFO | aether2 | Model output shape: torch.Size([1, 622, 153])
2025-05-28 19:37:23,527 | INFO | aether2 | Model output shape: torch.Size([1, 623, 153])
2025-05-28 19:37:23,759 | INFO | aether2 | Model output shape: torch.Size([1, 624, 153])
2025-05-28 19:37:23,981 | INFO | aether2 | Model output shape: torch.Size([1, 625, 153])
2025-05-28 19:37:24,195 | INFO | aether2 | Model output shape: torch.Size([1, 626, 153])
2025-05-28 19:37:24,479 | INFO | aether2 | Model output shape: torch.Size([1, 627, 153])
2025-05-28 19:37:24,746 | INFO | aether2 | Model output shape: torch.Size([1, 628, 153])
2025-05-28 19:37:24,969 | INFO | aether2 | Model output shape: torch.Size([1, 629, 153])
2025-05-28 19:37:25,200 | INFO | aether2 | Model output shape: torch.Size([1, 630, 153])
2025-05-28 19:37:25,450 | INFO | aether2 | Model output shape: torch.Size([1, 631, 153])
2025-05-28 19:37:25,672 | INFO | aether2 | Model output shape: torch.Size([1, 632, 153])
2025-05-28 19:37:25,885 | INFO | aether2 | Model output shape: torch.Size([1, 633, 153])
2025-05-28 19:37:26,192 | INFO | aether2 | Model output shape: torch.Size([1, 634, 153])
2025-05-28 19:37:26,483 | INFO | aether2 | Model output shape: torch.Size([1, 635, 153])
2025-05-28 19:37:26,753 | INFO | aether2 | Model output shape: torch.Size([1, 636, 153])
2025-05-28 19:37:27,014 | INFO | aether2 | Model output shape: torch.Size([1, 637, 153])
2025-05-28 19:37:27,261 | INFO | aether2 | Model output shape: torch.Size([1, 638, 153])
2025-05-28 19:37:27,486 | INFO | aether2 | Model output shape: torch.Size([1, 639, 153])
2025-05-28 19:37:27,688 | INFO | aether2 | Model output shape: torch.Size([1, 640, 153])
2025-05-28 19:37:27,904 | INFO | aether2 | Model output shape: torch.Size([1, 641, 153])
2025-05-28 19:37:28,157 | INFO | aether2 | Model output shape: torch.Size([1, 642, 153])
2025-05-28 19:37:28,391 | INFO | aether2 | Model output shape: torch.Size([1, 643, 153])
2025-05-28 19:37:28,608 | INFO | aether2 | Model output shape: torch.Size([1, 644, 153])
2025-05-28 19:37:28,837 | INFO | aether2 | Model output shape: torch.Size([1, 645, 153])
2025-05-28 19:37:29,234 | INFO | aether2 | Model output shape: torch.Size([1, 646, 153])
2025-05-28 19:37:29,527 | INFO | aether2 | Model output shape: torch.Size([1, 647, 153])
2025-05-28 19:37:29,782 | INFO | aether2 | Model output shape: torch.Size([1, 648, 153])
2025-05-28 19:37:30,032 | INFO | aether2 | Model output shape: torch.Size([1, 649, 153])
2025-05-28 19:37:30,270 | INFO | aether2 | Model output shape: torch.Size([1, 650, 153])
2025-05-28 19:37:30,500 | INFO | aether2 | Model output shape: torch.Size([1, 651, 153])
2025-05-28 19:37:30,730 | INFO | aether2 | Model output shape: torch.Size([1, 652, 153])
2025-05-28 19:37:30,957 | INFO | aether2 | Model output shape: torch.Size([1, 653, 153])
2025-05-28 19:37:31,183 | INFO | aether2 | Model output shape: torch.Size([1, 654, 153])
2025-05-28 19:37:31,400 | INFO | aether2 | Model output shape: torch.Size([1, 655, 153])
2025-05-28 19:37:31,621 | INFO | aether2 | Model output shape: torch.Size([1, 656, 153])
2025-05-28 19:37:31,879 | INFO | aether2 | Model output shape: torch.Size([1, 657, 153])
2025-05-28 19:37:32,124 | INFO | aether2 | Model output shape: torch.Size([1, 658, 153])
2025-05-28 19:37:32,380 | INFO | aether2 | Model output shape: torch.Size([1, 659, 153])
2025-05-28 19:37:32,642 | INFO | aether2 | Model output shape: torch.Size([1, 660, 153])
2025-05-28 19:37:32,949 | INFO | aether2 | Model output shape: torch.Size([1, 661, 153])
2025-05-28 19:37:33,241 | INFO | aether2 | Model output shape: torch.Size([1, 662, 153])
2025-05-28 19:37:33,511 | INFO | aether2 | Model output shape: torch.Size([1, 663, 153])
2025-05-28 19:37:33,758 | INFO | aether2 | Model output shape: torch.Size([1, 664, 153])
2025-05-28 19:37:34,022 | INFO | aether2 | Model output shape: torch.Size([1, 665, 153])
2025-05-28 19:37:34,310 | INFO | aether2 | Model output shape: torch.Size([1, 666, 153])
2025-05-28 19:37:34,592 | INFO | aether2 | Model output shape: torch.Size([1, 667, 153])
2025-05-28 19:37:34,874 | INFO | aether2 | Model output shape: torch.Size([1, 668, 153])
2025-05-28 19:37:35,140 | INFO | aether2 | Model output shape: torch.Size([1, 669, 153])
2025-05-28 19:37:35,388 | INFO | aether2 | Model output shape: torch.Size([1, 670, 153])
2025-05-28 19:37:35,649 | INFO | aether2 | Model output shape: torch.Size([1, 671, 153])
2025-05-28 19:37:35,894 | INFO | aether2 | Model output shape: torch.Size([1, 672, 153])
2025-05-28 19:37:36,193 | INFO | aether2 | Model output shape: torch.Size([1, 673, 153])
2025-05-28 19:37:36,477 | INFO | aether2 | Model output shape: torch.Size([1, 674, 153])
2025-05-28 19:37:36,728 | INFO | aether2 | Model output shape: torch.Size([1, 675, 153])
2025-05-28 19:37:36,981 | INFO | aether2 | Model output shape: torch.Size([1, 676, 153])
2025-05-28 19:37:37,641 | INFO | aether2 | Model output shape: torch.Size([1, 677, 153])
2025-05-28 19:37:37,964 | INFO | aether2 | Model output shape: torch.Size([1, 678, 153])
2025-05-28 19:37:38,294 | INFO | aether2 | Model output shape: torch.Size([1, 679, 153])
2025-05-28 19:37:38,549 | INFO | aether2 | Model output shape: torch.Size([1, 680, 153])
2025-05-28 19:37:38,807 | INFO | aether2 | Model output shape: torch.Size([1, 681, 153])
2025-05-28 19:37:39,057 | INFO | aether2 | Model output shape: torch.Size([1, 682, 153])
2025-05-28 19:37:39,316 | INFO | aether2 | Model output shape: torch.Size([1, 683, 153])
2025-05-28 19:37:39,578 | INFO | aether2 | Model output shape: torch.Size([1, 684, 153])
2025-05-28 19:37:39,837 | INFO | aether2 | Model output shape: torch.Size([1, 685, 153])
2025-05-28 19:37:40,088 | INFO | aether2 | Model output shape: torch.Size([1, 686, 153])
2025-05-28 19:37:40,347 | INFO | aether2 | Model output shape: torch.Size([1, 687, 153])
2025-05-28 19:37:40,594 | INFO | aether2 | Model output shape: torch.Size([1, 688, 153])
2025-05-28 19:37:40,860 | INFO | aether2 | Model output shape: torch.Size([1, 689, 153])
2025-05-28 19:37:41,112 | INFO | aether2 | Model output shape: torch.Size([1, 690, 153])
2025-05-28 19:37:41,367 | INFO | aether2 | Model output shape: torch.Size([1, 691, 153])
2025-05-28 19:37:41,624 | INFO | aether2 | Model output shape: torch.Size([1, 692, 153])
2025-05-28 19:37:41,884 | INFO | aether2 | Model output shape: torch.Size([1, 693, 153])
2025-05-28 19:37:42,149 | INFO | aether2 | Model output shape: torch.Size([1, 694, 153])
2025-05-28 19:37:42,411 | INFO | aether2 | Model output shape: torch.Size([1, 695, 153])
2025-05-28 19:37:42,658 | INFO | aether2 | Model output shape: torch.Size([1, 696, 153])
2025-05-28 19:37:42,921 | INFO | aether2 | Model output shape: torch.Size([1, 697, 153])
2025-05-28 19:37:43,224 | INFO | aether2 | Model output shape: torch.Size([1, 698, 153])
2025-05-28 19:37:43,507 | INFO | aether2 | Model output shape: torch.Size([1, 699, 153])
2025-05-28 19:37:43,778 | INFO | aether2 | Model output shape: torch.Size([1, 700, 153])
2025-05-28 19:37:44,064 | INFO | aether2 | Model output shape: torch.Size([1, 701, 153])
2025-05-28 19:37:44,342 | INFO | aether2 | Model output shape: torch.Size([1, 702, 153])
2025-05-28 19:37:44,611 | INFO | aether2 | Model output shape: torch.Size([1, 703, 153])
2025-05-28 19:37:44,875 | INFO | aether2 | Model output shape: torch.Size([1, 704, 153])
2025-05-28 19:37:45,165 | INFO | aether2 | Model output shape: torch.Size([1, 705, 153])
2025-05-28 19:37:45,444 | INFO | aether2 | Model output shape: torch.Size([1, 706, 153])
2025-05-28 19:37:45,736 | INFO | aether2 | Model output shape: torch.Size([1, 707, 153])
2025-05-28 19:37:46,031 | INFO | aether2 | Model output shape: torch.Size([1, 708, 153])
2025-05-28 19:37:46,329 | INFO | aether2 | Model output shape: torch.Size([1, 709, 153])
2025-05-28 19:37:46,614 | INFO | aether2 | Model output shape: torch.Size([1, 710, 153])
2025-05-28 19:37:46,909 | INFO | aether2 | Model output shape: torch.Size([1, 711, 153])
2025-05-28 19:37:47,193 | INFO | aether2 | Model output shape: torch.Size([1, 712, 153])
2025-05-28 19:37:47,495 | INFO | aether2 | Model output shape: torch.Size([1, 713, 153])
2025-05-28 19:37:48,192 | INFO | aether2 | Model output shape: torch.Size([1, 714, 153])
2025-05-28 19:37:48,583 | INFO | aether2 | Model output shape: torch.Size([1, 715, 153])
2025-05-28 19:37:48,913 | INFO | aether2 | Model output shape: torch.Size([1, 716, 153])
2025-05-28 19:37:49,217 | INFO | aether2 | Model output shape: torch.Size([1, 717, 153])
2025-05-28 19:37:49,516 | INFO | aether2 | Model output shape: torch.Size([1, 718, 153])
2025-05-28 19:37:49,856 | INFO | aether2 | Model output shape: torch.Size([1, 719, 153])
2025-05-28 19:37:50,150 | INFO | aether2 | Model output shape: torch.Size([1, 720, 153])
2025-05-28 19:37:50,471 | INFO | aether2 | Model output shape: torch.Size([1, 721, 153])
2025-05-28 19:37:50,786 | INFO | aether2 | Model output shape: torch.Size([1, 722, 153])
2025-05-28 19:37:51,087 | INFO | aether2 | Model output shape: torch.Size([1, 723, 153])
2025-05-28 19:37:51,387 | INFO | aether2 | Model output shape: torch.Size([1, 724, 153])
2025-05-28 19:37:51,700 | INFO | aether2 | Model output shape: torch.Size([1, 725, 153])
2025-05-28 19:37:52,011 | INFO | aether2 | Model output shape: torch.Size([1, 726, 153])
2025-05-28 19:37:52,337 | INFO | aether2 | Model output shape: torch.Size([1, 727, 153])
2025-05-28 19:37:52,615 | INFO | aether2 | Model output shape: torch.Size([1, 728, 153])
2025-05-28 19:37:52,921 | INFO | aether2 | Model output shape: torch.Size([1, 729, 153])
2025-05-28 19:37:53,246 | INFO | aether2 | Model output shape: torch.Size([1, 730, 153])
2025-05-28 19:37:53,546 | INFO | aether2 | Model output shape: torch.Size([1, 731, 153])
2025-05-28 19:37:53,840 | INFO | aether2 | Model output shape: torch.Size([1, 732, 153])
2025-05-28 19:37:54,157 | INFO | aether2 | Model output shape: torch.Size([1, 733, 153])
2025-05-28 19:37:54,460 | INFO | aether2 | Model output shape: torch.Size([1, 734, 153])
2025-05-28 19:37:54,767 | INFO | aether2 | Model output shape: torch.Size([1, 735, 153])
2025-05-28 19:37:55,046 | INFO | aether2 | Model output shape: torch.Size([1, 736, 153])
2025-05-28 19:37:55,344 | INFO | aether2 | Model output shape: torch.Size([1, 737, 153])
2025-05-28 19:37:55,642 | INFO | aether2 | Model output shape: torch.Size([1, 738, 153])
2025-05-28 19:37:55,941 | INFO | aether2 | Model output shape: torch.Size([1, 739, 153])
2025-05-28 19:37:56,250 | INFO | aether2 | Model output shape: torch.Size([1, 740, 153])
2025-05-28 19:37:56,563 | INFO | aether2 | Model output shape: torch.Size([1, 741, 153])
2025-05-28 19:37:56,866 | INFO | aether2 | Model output shape: torch.Size([1, 742, 153])
2025-05-28 19:37:57,196 | INFO | aether2 | Model output shape: torch.Size([1, 743, 153])
2025-05-28 19:37:57,478 | INFO | aether2 | Model output shape: torch.Size([1, 744, 153])
2025-05-28 19:37:57,774 | INFO | aether2 | Model output shape: torch.Size([1, 745, 153])
2025-05-28 19:37:58,087 | INFO | aether2 | Model output shape: torch.Size([1, 746, 153])
2025-05-28 19:37:58,402 | INFO | aether2 | Model output shape: torch.Size([1, 747, 153])
2025-05-28 19:37:58,699 | INFO | aether2 | Model output shape: torch.Size([1, 748, 153])
2025-05-28 19:37:58,995 | INFO | aether2 | Model output shape: torch.Size([1, 749, 153])
2025-05-28 19:37:59,294 | INFO | aether2 | Model output shape: torch.Size([1, 750, 153])
2025-05-28 19:37:59,595 | INFO | aether2 | Model output shape: torch.Size([1, 751, 153])
2025-05-28 19:37:59,869 | INFO | aether2 | Model output shape: torch.Size([1, 752, 153])
2025-05-28 19:38:00,181 | INFO | aether2 | Model output shape: torch.Size([1, 753, 153])
2025-05-28 19:38:00,491 | INFO | aether2 | Model output shape: torch.Size([1, 754, 153])
2025-05-28 19:38:00,803 | INFO | aether2 | Model output shape: torch.Size([1, 755, 153])
2025-05-28 19:38:01,094 | INFO | aether2 | Model output shape: torch.Size([1, 756, 153])
2025-05-28 19:38:01,405 | INFO | aether2 | Model output shape: torch.Size([1, 757, 153])
2025-05-28 19:38:01,710 | INFO | aether2 | Model output shape: torch.Size([1, 758, 153])
2025-05-28 19:38:02,013 | INFO | aether2 | Model output shape: torch.Size([1, 759, 153])
2025-05-28 19:38:02,298 | INFO | aether2 | Model output shape: torch.Size([1, 760, 153])
2025-05-28 19:38:02,611 | INFO | aether2 | Model output shape: torch.Size([1, 761, 153])
2025-05-28 19:38:02,928 | INFO | aether2 | Model output shape: torch.Size([1, 762, 153])
2025-05-28 19:38:03,261 | INFO | aether2 | Model output shape: torch.Size([1, 763, 153])
2025-05-28 19:38:03,578 | INFO | aether2 | Model output shape: torch.Size([1, 764, 153])
2025-05-28 19:38:03,890 | INFO | aether2 | Model output shape: torch.Size([1, 765, 153])
2025-05-28 19:38:04,209 | INFO | aether2 | Model output shape: torch.Size([1, 766, 153])
2025-05-28 19:38:04,899 | INFO | aether2 | Model output shape: torch.Size([1, 767, 153])
2025-05-28 19:38:05,214 | INFO | aether2 | Model output shape: torch.Size([1, 768, 153])
2025-05-28 19:38:05,523 | INFO | aether2 | Model output shape: torch.Size([1, 769, 153])
2025-05-28 19:38:05,830 | INFO | aether2 | Model output shape: torch.Size([1, 770, 153])
2025-05-28 19:38:06,145 | INFO | aether2 | Model output shape: torch.Size([1, 771, 153])
2025-05-28 19:38:06,453 | INFO | aether2 | Model output shape: torch.Size([1, 772, 153])
2025-05-28 19:38:06,766 | INFO | aether2 | Model output shape: torch.Size([1, 773, 153])
2025-05-28 19:38:07,082 | INFO | aether2 | Model output shape: torch.Size([1, 774, 153])
2025-05-28 19:38:07,390 | INFO | aether2 | Model output shape: torch.Size([1, 775, 153])
2025-05-28 19:38:07,685 | INFO | aether2 | Model output shape: torch.Size([1, 776, 153])
2025-05-28 19:38:08,016 | INFO | aether2 | Model output shape: torch.Size([1, 777, 153])
2025-05-28 19:38:08,376 | INFO | aether2 | Model output shape: torch.Size([1, 778, 153])
2025-05-28 19:38:08,708 | INFO | aether2 | Model output shape: torch.Size([1, 779, 153])
2025-05-28 19:38:09,037 | INFO | aether2 | Model output shape: torch.Size([1, 780, 153])
2025-05-28 19:38:09,364 | INFO | aether2 | Model output shape: torch.Size([1, 781, 153])
2025-05-28 19:38:09,688 | INFO | aether2 | Model output shape: torch.Size([1, 782, 153])
2025-05-28 19:38:10,019 | INFO | aether2 | Model output shape: torch.Size([1, 783, 153])
2025-05-28 19:38:10,309 | INFO | aether2 | Model output shape: torch.Size([1, 784, 153])
2025-05-28 19:38:10,629 | INFO | aether2 | Model output shape: torch.Size([1, 785, 153])
2025-05-28 19:38:10,948 | INFO | aether2 | Model output shape: torch.Size([1, 786, 153])
2025-05-28 19:38:11,272 | INFO | aether2 | Model output shape: torch.Size([1, 787, 153])
2025-05-28 19:38:11,658 | INFO | aether2 | Model output shape: torch.Size([1, 788, 153])
2025-05-28 19:38:11,993 | INFO | aether2 | Model output shape: torch.Size([1, 789, 153])
2025-05-28 19:38:12,361 | INFO | aether2 | Model output shape: torch.Size([1, 790, 153])
2025-05-28 19:38:12,723 | INFO | aether2 | Model output shape: torch.Size([1, 791, 153])
2025-05-28 19:38:13,056 | INFO | aether2 | Model output shape: torch.Size([1, 792, 153])
2025-05-28 19:38:13,410 | INFO | aether2 | Model output shape: torch.Size([1, 793, 153])
2025-05-28 19:38:13,741 | INFO | aether2 | Model output shape: torch.Size([1, 794, 153])
2025-05-28 19:38:14,066 | INFO | aether2 | Model output shape: torch.Size([1, 795, 153])
2025-05-28 19:38:14,387 | INFO | aether2 | Model output shape: torch.Size([1, 796, 153])
2025-05-28 19:38:14,724 | INFO | aether2 | Model output shape: torch.Size([1, 797, 153])
2025-05-28 19:38:15,058 | INFO | aether2 | Model output shape: torch.Size([1, 798, 153])
2025-05-28 19:38:15,392 | INFO | aether2 | Model output shape: torch.Size([1, 799, 153])
2025-05-28 19:38:15,705 | INFO | aether2 | Model output shape: torch.Size([1, 800, 153])
2025-05-28 19:38:16,038 | INFO | aether2 | Model output shape: torch.Size([1, 801, 153])
2025-05-28 19:38:16,368 | INFO | aether2 | Model output shape: torch.Size([1, 802, 153])
2025-05-28 19:38:16,719 | INFO | aether2 | Model output shape: torch.Size([1, 803, 153])
2025-05-28 19:38:17,031 | INFO | aether2 | Model output shape: torch.Size([1, 804, 153])
2025-05-28 19:38:17,378 | INFO | aether2 | Model output shape: torch.Size([1, 805, 153])
2025-05-28 19:38:17,714 | INFO | aether2 | Model output shape: torch.Size([1, 806, 153])
2025-05-28 19:38:18,065 | INFO | aether2 | Model output shape: torch.Size([1, 807, 153])
2025-05-28 19:38:18,398 | INFO | aether2 | Model output shape: torch.Size([1, 808, 153])
2025-05-28 19:38:18,746 | INFO | aether2 | Model output shape: torch.Size([1, 809, 153])
2025-05-28 19:38:19,089 | INFO | aether2 | Model output shape: torch.Size([1, 810, 153])
2025-05-28 19:38:19,432 | INFO | aether2 | Model output shape: torch.Size([1, 811, 153])
2025-05-28 19:38:19,769 | INFO | aether2 | Model output shape: torch.Size([1, 812, 153])
2025-05-28 19:38:20,111 | INFO | aether2 | Model output shape: torch.Size([1, 813, 153])
2025-05-28 19:38:20,447 | INFO | aether2 | Model output shape: torch.Size([1, 814, 153])
2025-05-28 19:38:20,846 | INFO | aether2 | Model output shape: torch.Size([1, 815, 153])
2025-05-28 19:38:21,213 | INFO | aether2 | Model output shape: torch.Size([1, 816, 153])
2025-05-28 19:38:21,637 | INFO | aether2 | Model output shape: torch.Size([1, 817, 153])
2025-05-28 19:38:22,026 | INFO | aether2 | Model output shape: torch.Size([1, 818, 153])
2025-05-28 19:38:22,466 | INFO | aether2 | Model output shape: torch.Size([1, 819, 153])
2025-05-28 19:38:22,839 | INFO | aether2 | Model output shape: torch.Size([1, 820, 153])
2025-05-28 19:38:23,250 | INFO | aether2 | Model output shape: torch.Size([1, 821, 153])
2025-05-28 19:38:23,645 | INFO | aether2 | Model output shape: torch.Size([1, 822, 153])
2025-05-28 19:38:24,032 | INFO | aether2 | Model output shape: torch.Size([1, 823, 153])
2025-05-28 19:38:24,377 | INFO | aether2 | Model output shape: torch.Size([1, 824, 153])
2025-05-28 19:38:24,764 | INFO | aether2 | Model output shape: torch.Size([1, 825, 153])
2025-05-28 19:38:25,124 | INFO | aether2 | Model output shape: torch.Size([1, 826, 153])
2025-05-28 19:38:25,507 | INFO | aether2 | Model output shape: torch.Size([1, 827, 153])
2025-05-28 19:38:25,890 | INFO | aether2 | Model output shape: torch.Size([1, 828, 153])
2025-05-28 19:38:26,265 | INFO | aether2 | Model output shape: torch.Size([1, 829, 153])
2025-05-28 19:38:26,654 | INFO | aether2 | Model output shape: torch.Size([1, 830, 153])
2025-05-28 19:38:27,037 | INFO | aether2 | Model output shape: torch.Size([1, 831, 153])
2025-05-28 19:38:27,374 | INFO | aether2 | Model output shape: torch.Size([1, 832, 153])
2025-05-28 19:38:27,742 | INFO | aether2 | Model output shape: torch.Size([1, 833, 153])
2025-05-28 19:38:28,120 | INFO | aether2 | Model output shape: torch.Size([1, 834, 153])
2025-05-28 19:38:28,517 | INFO | aether2 | Model output shape: torch.Size([1, 835, 153])
2025-05-28 19:38:28,863 | INFO | aether2 | Model output shape: torch.Size([1, 836, 153])
2025-05-28 19:38:29,235 | INFO | aether2 | Model output shape: torch.Size([1, 837, 153])
2025-05-28 19:38:29,617 | INFO | aether2 | Model output shape: torch.Size([1, 838, 153])
2025-05-28 19:38:29,995 | INFO | aether2 | Model output shape: torch.Size([1, 839, 153])
2025-05-28 19:38:30,337 | INFO | aether2 | Model output shape: torch.Size([1, 840, 153])
2025-05-28 19:38:30,716 | INFO | aether2 | Model output shape: torch.Size([1, 841, 153])
2025-05-28 19:38:31,116 | INFO | aether2 | Model output shape: torch.Size([1, 842, 153])
2025-05-28 19:38:31,526 | INFO | aether2 | Model output shape: torch.Size([1, 843, 153])
2025-05-28 19:38:32,062 | INFO | aether2 | Model output shape: torch.Size([1, 844, 153])
2025-05-28 19:38:32,774 | INFO | aether2 | Model output shape: torch.Size([1, 845, 153])
2025-05-28 19:38:33,235 | INFO | aether2 | Model output shape: torch.Size([1, 846, 153])
2025-05-28 19:38:33,643 | INFO | aether2 | Model output shape: torch.Size([1, 847, 153])
2025-05-28 19:38:34,047 | INFO | aether2 | Model output shape: torch.Size([1, 848, 153])
2025-05-28 19:38:34,478 | INFO | aether2 | Model output shape: torch.Size([1, 849, 153])
2025-05-28 19:38:34,899 | INFO | aether2 | Model output shape: torch.Size([1, 850, 153])
2025-05-28 19:38:35,306 | INFO | aether2 | Model output shape: torch.Size([1, 851, 153])
2025-05-28 19:38:35,680 | INFO | aether2 | Model output shape: torch.Size([1, 852, 153])
2025-05-28 19:38:36,059 | INFO | aether2 | Model output shape: torch.Size([1, 853, 153])
2025-05-28 19:38:36,477 | INFO | aether2 | Model output shape: torch.Size([1, 854, 153])
2025-05-28 19:38:36,887 | INFO | aether2 | Model output shape: torch.Size([1, 855, 153])
2025-05-28 19:38:37,285 | INFO | aether2 | Model output shape: torch.Size([1, 856, 153])
2025-05-28 19:38:37,692 | INFO | aether2 | Model output shape: torch.Size([1, 857, 153])
2025-05-28 19:38:38,082 | INFO | aether2 | Model output shape: torch.Size([1, 858, 153])
2025-05-28 19:38:38,490 | INFO | aether2 | Model output shape: torch.Size([1, 859, 153])
2025-05-28 19:38:38,875 | INFO | aether2 | Model output shape: torch.Size([1, 860, 153])
2025-05-28 19:38:39,266 | INFO | aether2 | Model output shape: torch.Size([1, 861, 153])
2025-05-28 19:38:39,651 | INFO | aether2 | Model output shape: torch.Size([1, 862, 153])
2025-05-28 19:38:40,050 | INFO | aether2 | Model output shape: torch.Size([1, 863, 153])
2025-05-28 19:38:40,414 | INFO | aether2 | Model output shape: torch.Size([1, 864, 153])
2025-05-28 19:38:40,810 | INFO | aether2 | Model output shape: torch.Size([1, 865, 153])
2025-05-28 19:38:41,209 | INFO | aether2 | Model output shape: torch.Size([1, 866, 153])
2025-05-28 19:38:41,602 | INFO | aether2 | Model output shape: torch.Size([1, 867, 153])
2025-05-28 19:38:41,978 | INFO | aether2 | Model output shape: torch.Size([1, 868, 153])
2025-05-28 19:38:42,372 | INFO | aether2 | Model output shape: torch.Size([1, 869, 153])
2025-05-28 19:38:42,774 | INFO | aether2 | Model output shape: torch.Size([1, 870, 153])
2025-05-28 19:38:43,184 | INFO | aether2 | Model output shape: torch.Size([1, 871, 153])
2025-05-28 19:38:43,563 | INFO | aether2 | Model output shape: torch.Size([1, 872, 153])
2025-05-28 19:38:43,963 | INFO | aether2 | Model output shape: torch.Size([1, 873, 153])
2025-05-28 19:38:44,367 | INFO | aether2 | Model output shape: torch.Size([1, 874, 153])
2025-05-28 19:38:44,773 | INFO | aether2 | Model output shape: torch.Size([1, 875, 153])
2025-05-28 19:38:45,171 | INFO | aether2 | Model output shape: torch.Size([1, 876, 153])
2025-05-28 19:38:45,586 | INFO | aether2 | Model output shape: torch.Size([1, 877, 153])
2025-05-28 19:38:45,995 | INFO | aether2 | Model output shape: torch.Size([1, 878, 153])
2025-05-28 19:38:46,396 | INFO | aether2 | Model output shape: torch.Size([1, 879, 153])
2025-05-28 19:38:46,766 | INFO | aether2 | Model output shape: torch.Size([1, 880, 153])
2025-05-28 19:38:47,168 | INFO | aether2 | Model output shape: torch.Size([1, 881, 153])
2025-05-28 19:38:47,580 | INFO | aether2 | Model output shape: torch.Size([1, 882, 153])
2025-05-28 19:38:47,987 | INFO | aether2 | Model output shape: torch.Size([1, 883, 153])
2025-05-28 19:38:48,392 | INFO | aether2 | Model output shape: torch.Size([1, 884, 153])
2025-05-28 19:38:48,806 | INFO | aether2 | Model output shape: torch.Size([1, 885, 153])
2025-05-28 19:38:49,207 | INFO | aether2 | Model output shape: torch.Size([1, 886, 153])
2025-05-28 19:38:49,626 | INFO | aether2 | Model output shape: torch.Size([1, 887, 153])
2025-05-28 19:38:50,476 | INFO | aether2 | Model output shape: torch.Size([1, 888, 153])
2025-05-28 19:38:51,064 | INFO | aether2 | Model output shape: torch.Size([1, 889, 153])
2025-05-28 19:38:51,503 | INFO | aether2 | Model output shape: torch.Size([1, 890, 153])
2025-05-28 19:38:51,957 | INFO | aether2 | Model output shape: torch.Size([1, 891, 153])
2025-05-28 19:38:52,384 | INFO | aether2 | Model output shape: torch.Size([1, 892, 153])
2025-05-28 19:38:52,820 | INFO | aether2 | Model output shape: torch.Size([1, 893, 153])
2025-05-28 19:38:53,242 | INFO | aether2 | Model output shape: torch.Size([1, 894, 153])
2025-05-28 19:38:53,681 | INFO | aether2 | Model output shape: torch.Size([1, 895, 153])
2025-05-28 19:38:54,085 | INFO | aether2 | Model output shape: torch.Size([1, 896, 153])
2025-05-28 19:38:54,604 | INFO | aether2 | Model output shape: torch.Size([1, 897, 153])
2025-05-28 19:38:55,035 | INFO | aether2 | Model output shape: torch.Size([1, 898, 153])
2025-05-28 19:38:55,463 | INFO | aether2 | Model output shape: torch.Size([1, 899, 153])
2025-05-28 19:38:55,889 | INFO | aether2 | Model output shape: torch.Size([1, 900, 153])
2025-05-28 19:38:56,329 | INFO | aether2 | Model output shape: torch.Size([1, 901, 153])
2025-05-28 19:38:56,790 | INFO | aether2 | Model output shape: torch.Size([1, 902, 153])
2025-05-28 19:38:57,222 | INFO | aether2 | Model output shape: torch.Size([1, 903, 153])
2025-05-28 19:38:57,617 | INFO | aether2 | Model output shape: torch.Size([1, 904, 153])
2025-05-28 19:38:58,052 | INFO | aether2 | Model output shape: torch.Size([1, 905, 153])
2025-05-28 19:38:58,507 | INFO | aether2 | Model output shape: torch.Size([1, 906, 153])
2025-05-28 19:38:59,347 | INFO | aether2 | Model output shape: torch.Size([1, 907, 153])
2025-05-28 19:38:59,794 | INFO | aether2 | Model output shape: torch.Size([1, 908, 153])
2025-05-28 19:39:00,223 | INFO | aether2 | Model output shape: torch.Size([1, 909, 153])
2025-05-28 19:39:00,659 | INFO | aether2 | Model output shape: torch.Size([1, 910, 153])
2025-05-28 19:39:01,090 | INFO | aether2 | Model output shape: torch.Size([1, 911, 153])
2025-05-28 19:39:01,485 | INFO | aether2 | Model output shape: torch.Size([1, 912, 153])
2025-05-28 19:39:01,936 | INFO | aether2 | Model output shape: torch.Size([1, 913, 153])
2025-05-28 19:39:02,364 | INFO | aether2 | Model output shape: torch.Size([1, 914, 153])
2025-05-28 19:39:02,801 | INFO | aether2 | Model output shape: torch.Size([1, 915, 153])
2025-05-28 19:39:03,224 | INFO | aether2 | Model output shape: torch.Size([1, 916, 153])
2025-05-28 19:39:03,664 | INFO | aether2 | Model output shape: torch.Size([1, 917, 153])
2025-05-28 19:39:04,100 | INFO | aether2 | Model output shape: torch.Size([1, 918, 153])
2025-05-28 19:39:04,530 | INFO | aether2 | Model output shape: torch.Size([1, 919, 153])
2025-05-28 19:39:04,919 | INFO | aether2 | Model output shape: torch.Size([1, 920, 153])
2025-05-28 19:39:05,342 | INFO | aether2 | Model output shape: torch.Size([1, 921, 153])
2025-05-28 19:39:05,779 | INFO | aether2 | Model output shape: torch.Size([1, 922, 153])
2025-05-28 19:39:06,210 | INFO | aether2 | Model output shape: torch.Size([1, 923, 153])
2025-05-28 19:39:06,634 | INFO | aether2 | Model output shape: torch.Size([1, 924, 153])
2025-05-28 19:39:07,078 | INFO | aether2 | Model output shape: torch.Size([1, 925, 153])
2025-05-28 19:39:07,535 | INFO | aether2 | Model output shape: torch.Size([1, 926, 153])
2025-05-28 19:39:07,993 | INFO | aether2 | Model output shape: torch.Size([1, 927, 153])
2025-05-28 19:39:08,421 | INFO | aether2 | Model output shape: torch.Size([1, 928, 153])
2025-05-28 19:39:08,859 | INFO | aether2 | Model output shape: torch.Size([1, 929, 153])
2025-05-28 19:39:09,298 | INFO | aether2 | Model output shape: torch.Size([1, 930, 153])
2025-05-28 19:39:09,732 | INFO | aether2 | Model output shape: torch.Size([1, 931, 153])
2025-05-28 19:39:10,150 | INFO | aether2 | Model output shape: torch.Size([1, 932, 153])
2025-05-28 19:39:10,583 | INFO | aether2 | Model output shape: torch.Size([1, 933, 153])
2025-05-28 19:39:11,037 | INFO | aether2 | Model output shape: torch.Size([1, 934, 153])
2025-05-28 19:39:11,484 | INFO | aether2 | Model output shape: torch.Size([1, 935, 153])
2025-05-28 19:39:11,900 | INFO | aether2 | Model output shape: torch.Size([1, 936, 153])
2025-05-28 19:39:12,358 | INFO | aether2 | Model output shape: torch.Size([1, 937, 153])
2025-05-28 19:39:12,816 | INFO | aether2 | Model output shape: torch.Size([1, 938, 153])
2025-05-28 19:39:13,318 | INFO | aether2 | Model output shape: torch.Size([1, 939, 153])
2025-05-28 19:39:13,794 | INFO | aether2 | Model output shape: torch.Size([1, 940, 153])
2025-05-28 19:39:14,270 | INFO | aether2 | Model output shape: torch.Size([1, 941, 153])
2025-05-28 19:39:14,739 | INFO | aether2 | Model output shape: torch.Size([1, 942, 153])
2025-05-28 19:39:15,223 | INFO | aether2 | Model output shape: torch.Size([1, 943, 153])
2025-05-28 19:39:15,669 | INFO | aether2 | Model output shape: torch.Size([1, 944, 153])
2025-05-28 19:39:16,148 | INFO | aether2 | Model output shape: torch.Size([1, 945, 153])
2025-05-28 19:39:16,638 | INFO | aether2 | Model output shape: torch.Size([1, 946, 153])
2025-05-28 19:39:17,140 | INFO | aether2 | Model output shape: torch.Size([1, 947, 153])
2025-05-28 19:39:17,608 | INFO | aether2 | Model output shape: torch.Size([1, 948, 153])
2025-05-28 19:39:18,099 | INFO | aether2 | Model output shape: torch.Size([1, 949, 153])
2025-05-28 19:39:18,607 | INFO | aether2 | Model output shape: torch.Size([1, 950, 153])
2025-05-28 19:39:19,098 | INFO | aether2 | Model output shape: torch.Size([1, 951, 153])
2025-05-28 19:39:19,543 | INFO | aether2 | Model output shape: torch.Size([1, 952, 153])
2025-05-28 19:39:20,120 | INFO | aether2 | Model output shape: torch.Size([1, 953, 153])
2025-05-28 19:39:20,824 | INFO | aether2 | Model output shape: torch.Size([1, 954, 153])
2025-05-28 19:39:21,384 | INFO | aether2 | Model output shape: torch.Size([1, 955, 153])
2025-05-28 19:39:22,008 | INFO | aether2 | Model output shape: torch.Size([1, 956, 153])
2025-05-28 19:39:22,571 | INFO | aether2 | Model output shape: torch.Size([1, 957, 153])
2025-05-28 19:39:23,091 | INFO | aether2 | Model output shape: torch.Size([1, 958, 153])
2025-05-28 19:39:23,640 | INFO | aether2 | Model output shape: torch.Size([1, 959, 153])
2025-05-28 19:39:24,110 | INFO | aether2 | Model output shape: torch.Size([1, 960, 153])
2025-05-28 19:39:24,651 | INFO | aether2 | Model output shape: torch.Size([1, 961, 153])
2025-05-28 19:39:25,140 | INFO | aether2 | Model output shape: torch.Size([1, 962, 153])
2025-05-28 19:39:25,632 | INFO | aether2 | Model output shape: torch.Size([1, 963, 153])
2025-05-28 19:39:26,103 | INFO | aether2 | Model output shape: torch.Size([1, 964, 153])
2025-05-28 19:39:26,606 | INFO | aether2 | Model output shape: torch.Size([1, 965, 153])
2025-05-28 19:39:27,123 | INFO | aether2 | Model output shape: torch.Size([1, 966, 153])
2025-05-28 19:39:27,621 | INFO | aether2 | Model output shape: torch.Size([1, 967, 153])
2025-05-28 19:39:28,113 | INFO | aether2 | Model output shape: torch.Size([1, 968, 153])
2025-05-28 19:39:28,704 | INFO | aether2 | Model output shape: torch.Size([1, 969, 153])
2025-05-28 19:39:29,196 | INFO | aether2 | Model output shape: torch.Size([1, 970, 153])
2025-05-28 19:39:29,729 | INFO | aether2 | Model output shape: torch.Size([1, 971, 153])
2025-05-28 19:39:30,220 | INFO | aether2 | Model output shape: torch.Size([1, 972, 153])
2025-05-28 19:39:30,756 | INFO | aether2 | Model output shape: torch.Size([1, 973, 153])
2025-05-28 19:39:31,250 | INFO | aether2 | Model output shape: torch.Size([1, 974, 153])
2025-05-28 19:39:31,778 | INFO | aether2 | Model output shape: torch.Size([1, 975, 153])
2025-05-28 19:39:32,251 | INFO | aether2 | Model output shape: torch.Size([1, 976, 153])
2025-05-28 19:39:32,754 | INFO | aether2 | Model output shape: torch.Size([1, 977, 153])
2025-05-28 19:39:33,284 | INFO | aether2 | Model output shape: torch.Size([1, 978, 153])
2025-05-28 19:39:33,792 | INFO | aether2 | Model output shape: torch.Size([1, 979, 153])
2025-05-28 19:39:34,266 | INFO | aether2 | Model output shape: torch.Size([1, 980, 153])
2025-05-28 19:39:34,786 | INFO | aether2 | Model output shape: torch.Size([1, 981, 153])
2025-05-28 19:39:35,336 | INFO | aether2 | Model output shape: torch.Size([1, 982, 153])
2025-05-28 19:39:35,897 | INFO | aether2 | Model output shape: torch.Size([1, 983, 153])
2025-05-28 19:39:36,367 | INFO | aether2 | Model output shape: torch.Size([1, 984, 153])
2025-05-28 19:39:36,877 | INFO | aether2 | Model output shape: torch.Size([1, 985, 153])
2025-05-28 19:39:37,396 | INFO | aether2 | Model output shape: torch.Size([1, 986, 153])
2025-05-28 19:39:37,925 | INFO | aether2 | Model output shape: torch.Size([1, 987, 153])
2025-05-28 19:39:38,496 | INFO | aether2 | Model output shape: torch.Size([1, 988, 153])
2025-05-28 19:39:39,092 | INFO | aether2 | Model output shape: torch.Size([1, 989, 153])
2025-05-28 19:39:39,627 | INFO | aether2 | Model output shape: torch.Size([1, 990, 153])
2025-05-28 19:39:40,169 | INFO | aether2 | Model output shape: torch.Size([1, 991, 153])
2025-05-28 19:39:40,655 | INFO | aether2 | Model output shape: torch.Size([1, 992, 153])
2025-05-28 19:39:41,196 | INFO | aether2 | Model output shape: torch.Size([1, 993, 153])
2025-05-28 19:39:41,736 | INFO | aether2 | Model output shape: torch.Size([1, 994, 153])
2025-05-28 19:39:42,309 | INFO | aether2 | Model output shape: torch.Size([1, 995, 153])
2025-05-28 19:39:42,841 | INFO | aether2 | Model output shape: torch.Size([1, 996, 153])
2025-05-28 19:39:43,408 | INFO | aether2 | Model output shape: torch.Size([1, 997, 153])
2025-05-28 19:39:43,957 | INFO | aether2 | Model output shape: torch.Size([1, 998, 153])
2025-05-28 19:39:44,503 | INFO | aether2 | Model output shape: torch.Size([1, 999, 153])
2025-05-28 19:39:44,980 | INFO | aether2 | Model output shape: torch.Size([1, 1000, 153])
2025-05-28 19:39:45,495 | INFO | aether2 | Model output shape: torch.Size([1, 1001, 153])
2025-05-28 19:39:46,053 | INFO | aether2 | Model output shape: torch.Size([1, 1002, 153])
2025-05-28 19:39:46,575 | INFO | aether2 | Model output shape: torch.Size([1, 1003, 153])
2025-05-28 19:39:47,086 | INFO | aether2 | Model output shape: torch.Size([1, 1004, 153])
2025-05-28 19:39:47,606 | INFO | aether2 | Model output shape: torch.Size([1, 1005, 153])
2025-05-28 19:39:48,125 | INFO | aether2 | Model output shape: torch.Size([1, 1006, 153])
2025-05-28 19:39:48,672 | INFO | aether2 | Model output shape: torch.Size([1, 1007, 153])
2025-05-28 19:39:49,165 | INFO | aether2 | Model output shape: torch.Size([1, 1008, 153])
2025-05-28 19:39:49,744 | INFO | aether2 | Model output shape: torch.Size([1, 1009, 153])
2025-05-28 19:39:50,353 | INFO | aether2 | Model output shape: torch.Size([1, 1010, 153])
2025-05-28 19:39:50,897 | INFO | aether2 | Model output shape: torch.Size([1, 1011, 153])
2025-05-28 19:39:51,431 | INFO | aether2 | Model output shape: torch.Size([1, 1012, 153])
2025-05-28 19:39:51,959 | INFO | aether2 | Model output shape: torch.Size([1, 1013, 153])
2025-05-28 19:39:52,500 | INFO | aether2 | Model output shape: torch.Size([1, 1014, 153])
2025-05-28 19:39:53,045 | INFO | aether2 | Model output shape: torch.Size([1, 1015, 153])
2025-05-28 19:39:53,540 | INFO | aether2 | Model output shape: torch.Size([1, 1016, 153])
2025-05-28 19:39:54,085 | INFO | aether2 | Model output shape: torch.Size([1, 1017, 153])
2025-05-28 19:39:54,600 | INFO | aether2 | Model output shape: torch.Size([1, 1018, 153])
2025-05-28 19:39:55,138 | INFO | aether2 | Model output shape: torch.Size([1, 1019, 153])
2025-05-28 19:39:55,683 | INFO | aether2 | Model output shape: torch.Size([1, 1020, 153])
2025-05-28 19:39:56,281 | INFO | aether2 | Model output shape: torch.Size([1, 1021, 153])
2025-05-28 19:39:56,835 | INFO | aether2 | Model output shape: torch.Size([1, 1022, 153])
2025-05-28 19:39:57,383 | INFO | aether2 | Model output shape: torch.Size([1, 1023, 153])
2025-05-28 19:39:57,926 | INFO | aether2 | Model output shape: torch.Size([1, 1024, 153])
2025-05-28 19:39:58,498 | INFO | aether2 | Model output shape: torch.Size([1, 1025, 153])
2025-05-28 19:39:59,045 | INFO | aether2 | Model output shape: torch.Size([1, 1026, 153])
2025-05-28 19:39:59,603 | INFO | aether2 | Model output shape: torch.Size([1, 1027, 153])
2025-05-28 19:40:00,136 | INFO | aether2 | Model output shape: torch.Size([1, 1028, 153])
2025-05-28 19:40:00,673 | INFO | aether2 | Model output shape: torch.Size([1, 1029, 153])
2025-05-28 19:40:01,221 | INFO | aether2 | Model output shape: torch.Size([1, 1030, 153])
2025-05-28 19:40:01,751 | INFO | aether2 | Model output shape: torch.Size([1, 1031, 153])
2025-05-28 19:40:02,250 | INFO | aether2 | Model output shape: torch.Size([1, 1032, 153])
2025-05-28 19:40:02,794 | INFO | aether2 | Model output shape: torch.Size([1, 1033, 153])
2025-05-28 19:40:03,332 | INFO | aether2 | Model output shape: torch.Size([1, 1034, 153])
2025-05-28 19:40:03,876 | INFO | aether2 | Model output shape: torch.Size([1, 1035, 153])
2025-05-28 19:40:04,412 | INFO | aether2 | Model output shape: torch.Size([1, 1036, 153])
2025-05-28 19:40:04,949 | INFO | aether2 | Model output shape: torch.Size([1, 1037, 153])
2025-05-28 19:40:05,501 | INFO | aether2 | Model output shape: torch.Size([1, 1038, 153])
2025-05-28 19:40:06,048 | INFO | aether2 | Model output shape: torch.Size([1, 1039, 153])
2025-05-28 19:40:06,539 | INFO | aether2 | Model output shape: torch.Size([1, 1040, 153])
2025-05-28 19:40:07,083 | INFO | aether2 | Model output shape: torch.Size([1, 1041, 153])
2025-05-28 19:40:07,628 | INFO | aether2 | Model output shape: torch.Size([1, 1042, 153])
2025-05-28 19:40:08,192 | INFO | aether2 | Model output shape: torch.Size([1, 1043, 153])
2025-05-28 19:40:08,736 | INFO | aether2 | Model output shape: torch.Size([1, 1044, 153])
2025-05-28 19:40:09,276 | INFO | aether2 | Model output shape: torch.Size([1, 1045, 153])
2025-05-28 19:40:09,839 | INFO | aether2 | Model output shape: torch.Size([1, 1046, 153])
2025-05-28 19:40:10,393 | INFO | aether2 | Model output shape: torch.Size([1, 1047, 153])
2025-05-28 19:40:10,905 | INFO | aether2 | Model output shape: torch.Size([1, 1048, 153])
2025-05-28 19:40:11,455 | INFO | aether2 | Model output shape: torch.Size([1, 1049, 153])
2025-05-28 19:40:12,005 | INFO | aether2 | Model output shape: torch.Size([1, 1050, 153])
2025-05-28 19:40:12,551 | INFO | aether2 | Model output shape: torch.Size([1, 1051, 153])
2025-05-28 19:40:13,102 | INFO | aether2 | Model output shape: torch.Size([1, 1052, 153])
2025-05-28 19:40:13,678 | INFO | aether2 | Model output shape: torch.Size([1, 1053, 153])
2025-05-28 19:40:14,234 | INFO | aether2 | Model output shape: torch.Size([1, 1054, 153])
2025-05-28 19:40:14,808 | INFO | aether2 | Model output shape: torch.Size([1, 1055, 153])
2025-05-28 19:40:15,318 | INFO | aether2 | Model output shape: torch.Size([1, 1056, 153])
2025-05-28 19:40:15,887 | INFO | aether2 | Model output shape: torch.Size([1, 1057, 153])
2025-05-28 19:40:16,445 | INFO | aether2 | Model output shape: torch.Size([1, 1058, 153])
2025-05-28 19:40:17,011 | INFO | aether2 | Model output shape: torch.Size([1, 1059, 153])
2025-05-28 19:40:17,545 | INFO | aether2 | Model output shape: torch.Size([1, 1060, 153])
2025-05-28 19:40:18,115 | INFO | aether2 | Model output shape: torch.Size([1, 1061, 153])
2025-05-28 19:40:18,692 | INFO | aether2 | Model output shape: torch.Size([1, 1062, 153])
2025-05-28 19:40:19,255 | INFO | aether2 | Model output shape: torch.Size([1, 1063, 153])
2025-05-28 19:40:19,786 | INFO | aether2 | Model output shape: torch.Size([1, 1064, 153])
2025-05-28 19:40:20,363 | INFO | aether2 | Model output shape: torch.Size([1, 1065, 153])
2025-05-28 19:40:20,936 | INFO | aether2 | Model output shape: torch.Size([1, 1066, 153])
2025-05-28 19:40:21,510 | INFO | aether2 | Model output shape: torch.Size([1, 1067, 153])
2025-05-28 19:40:22,077 | INFO | aether2 | Model output shape: torch.Size([1, 1068, 153])
2025-05-28 19:40:22,649 | INFO | aether2 | Model output shape: torch.Size([1, 1069, 153])
2025-05-28 19:40:23,215 | INFO | aether2 | Model output shape: torch.Size([1, 1070, 153])
2025-05-28 19:40:23,816 | INFO | aether2 | Model output shape: torch.Size([1, 1071, 153])
2025-05-28 19:40:24,339 | INFO | aether2 | Model output shape: torch.Size([1, 1072, 153])
2025-05-28 19:40:24,914 | INFO | aether2 | Model output shape: torch.Size([1, 1073, 153])
2025-05-28 19:40:25,493 | INFO | aether2 | Model output shape: torch.Size([1, 1074, 153])
2025-05-28 19:40:26,070 | INFO | aether2 | Model output shape: torch.Size([1, 1075, 153])
2025-05-28 19:40:26,624 | INFO | aether2 | Model output shape: torch.Size([1, 1076, 153])
2025-05-28 19:40:27,234 | INFO | aether2 | Model output shape: torch.Size([1, 1077, 153])
2025-05-28 19:40:27,889 | INFO | aether2 | Model output shape: torch.Size([1, 1078, 153])
2025-05-28 19:40:28,598 | INFO | aether2 | Model output shape: torch.Size([1, 1079, 153])
2025-05-28 19:40:29,172 | INFO | aether2 | Model output shape: torch.Size([1, 1080, 153])
2025-05-28 19:40:29,778 | INFO | aether2 | Model output shape: torch.Size([1, 1081, 153])
2025-05-28 19:40:30,406 | INFO | aether2 | Model output shape: torch.Size([1, 1082, 153])
2025-05-28 19:40:31,018 | INFO | aether2 | Model output shape: torch.Size([1, 1083, 153])
2025-05-28 19:40:31,636 | INFO | aether2 | Model output shape: torch.Size([1, 1084, 153])
2025-05-28 19:40:32,271 | INFO | aether2 | Model output shape: torch.Size([1, 1085, 153])
2025-05-28 19:40:32,890 | INFO | aether2 | Model output shape: torch.Size([1, 1086, 153])
2025-05-28 19:40:33,533 | INFO | aether2 | Model output shape: torch.Size([1, 1087, 153])
2025-05-28 19:40:34,099 | INFO | aether2 | Model output shape: torch.Size([1, 1088, 153])
2025-05-28 19:40:34,731 | INFO | aether2 | Model output shape: torch.Size([1, 1089, 153])
2025-05-28 19:40:35,354 | INFO | aether2 | Model output shape: torch.Size([1, 1090, 153])
2025-05-28 19:40:35,989 | INFO | aether2 | Model output shape: torch.Size([1, 1091, 153])
2025-05-28 19:40:36,580 | INFO | aether2 | Model output shape: torch.Size([1, 1092, 153])
2025-05-28 19:40:37,196 | INFO | aether2 | Model output shape: torch.Size([1, 1093, 153])
2025-05-28 19:40:37,801 | INFO | aether2 | Model output shape: torch.Size([1, 1094, 153])
2025-05-28 19:40:38,452 | INFO | aether2 | Model output shape: torch.Size([1, 1095, 153])
2025-05-28 19:40:39,018 | INFO | aether2 | Model output shape: torch.Size([1, 1096, 153])
2025-05-28 19:40:39,645 | INFO | aether2 | Model output shape: torch.Size([1, 1097, 153])
2025-05-28 19:40:40,270 | INFO | aether2 | Model output shape: torch.Size([1, 1098, 153])
2025-05-28 19:40:40,904 | INFO | aether2 | Model output shape: torch.Size([1, 1099, 153])
2025-05-28 19:40:41,505 | INFO | aether2 | Model output shape: torch.Size([1, 1100, 153])
2025-05-28 19:40:42,140 | INFO | aether2 | Model output shape: torch.Size([1, 1101, 153])
2025-05-28 19:40:42,752 | INFO | aether2 | Model output shape: torch.Size([1, 1102, 153])
2025-05-28 19:40:43,384 | INFO | aether2 | Model output shape: torch.Size([1, 1103, 153])
2025-05-28 19:40:43,958 | INFO | aether2 | Model output shape: torch.Size([1, 1104, 153])
2025-05-28 19:40:44,585 | INFO | aether2 | Model output shape: torch.Size([1, 1105, 153])
2025-05-28 19:40:45,216 | INFO | aether2 | Model output shape: torch.Size([1, 1106, 153])
2025-05-28 19:40:45,841 | INFO | aether2 | Model output shape: torch.Size([1, 1107, 153])
2025-05-28 19:40:46,428 | INFO | aether2 | Model output shape: torch.Size([1, 1108, 153])
2025-05-28 19:40:47,058 | INFO | aether2 | Model output shape: torch.Size([1, 1109, 153])
2025-05-28 19:40:47,686 | INFO | aether2 | Model output shape: torch.Size([1, 1110, 153])
2025-05-28 19:40:48,320 | INFO | aether2 | Model output shape: torch.Size([1, 1111, 153])
2025-05-28 19:40:48,907 | INFO | aether2 | Model output shape: torch.Size([1, 1112, 153])
2025-05-28 19:40:49,548 | INFO | aether2 | Model output shape: torch.Size([1, 1113, 153])
2025-05-28 19:40:50,188 | INFO | aether2 | Model output shape: torch.Size([1, 1114, 153])
2025-05-28 19:40:50,854 | INFO | aether2 | Model output shape: torch.Size([1, 1115, 153])
2025-05-28 19:40:51,485 | INFO | aether2 | Model output shape: torch.Size([1, 1116, 153])
2025-05-28 19:40:52,129 | INFO | aether2 | Model output shape: torch.Size([1, 1117, 153])
2025-05-28 19:40:52,756 | INFO | aether2 | Model output shape: torch.Size([1, 1118, 153])
2025-05-28 19:40:53,386 | INFO | aether2 | Model output shape: torch.Size([1, 1119, 153])
2025-05-28 19:40:53,961 | INFO | aether2 | Model output shape: torch.Size([1, 1120, 153])
2025-05-28 19:40:54,616 | INFO | aether2 | Model output shape: torch.Size([1, 1121, 153])
2025-05-28 19:40:55,314 | INFO | aether2 | Model output shape: torch.Size([1, 1122, 153])
2025-05-28 19:40:55,962 | INFO | aether2 | Model output shape: torch.Size([1, 1123, 153])
2025-05-28 19:40:56,559 | INFO | aether2 | Model output shape: torch.Size([1, 1124, 153])
2025-05-28 19:40:57,226 | INFO | aether2 | Model output shape: torch.Size([1, 1125, 153])
2025-05-28 19:40:57,908 | INFO | aether2 | Model output shape: torch.Size([1, 1126, 153])
2025-05-28 19:40:58,599 | INFO | aether2 | Model output shape: torch.Size([1, 1127, 153])
2025-05-28 19:40:59,181 | INFO | aether2 | Model output shape: torch.Size([1, 1128, 153])
2025-05-28 19:40:59,844 | INFO | aether2 | Model output shape: torch.Size([1, 1129, 153])
2025-05-28 19:41:00,494 | INFO | aether2 | Model output shape: torch.Size([1, 1130, 153])
2025-05-28 19:41:01,187 | INFO | aether2 | Model output shape: torch.Size([1, 1131, 153])
2025-05-28 19:41:01,820 | INFO | aether2 | Model output shape: torch.Size([1, 1132, 153])
2025-05-28 19:41:02,469 | INFO | aether2 | Model output shape: torch.Size([1, 1133, 153])
2025-05-28 19:41:03,104 | INFO | aether2 | Model output shape: torch.Size([1, 1134, 153])
2025-05-28 19:41:03,881 | INFO | aether2 | Model output shape: torch.Size([1, 1135, 153])
2025-05-28 19:41:04,486 | INFO | aether2 | Model output shape: torch.Size([1, 1136, 153])
2025-05-28 19:41:05,139 | INFO | aether2 | Model output shape: torch.Size([1, 1137, 153])
2025-05-28 19:41:05,799 | INFO | aether2 | Model output shape: torch.Size([1, 1138, 153])
2025-05-28 19:41:06,451 | INFO | aether2 | Model output shape: torch.Size([1, 1139, 153])
2025-05-28 19:41:07,123 | INFO | aether2 | Model output shape: torch.Size([1, 1140, 153])
2025-05-28 19:41:07,784 | INFO | aether2 | Model output shape: torch.Size([1, 1141, 153])
2025-05-28 19:41:08,456 | INFO | aether2 | Model output shape: torch.Size([1, 1142, 153])
2025-05-28 19:41:09,150 | INFO | aether2 | Model output shape: torch.Size([1, 1143, 153])
2025-05-28 19:41:09,911 | INFO | aether2 | Model output shape: torch.Size([1, 1144, 153])
2025-05-28 19:41:10,839 | INFO | aether2 | Model output shape: torch.Size([1, 1145, 153])
2025-05-28 19:41:11,561 | INFO | aether2 | Model output shape: torch.Size([1, 1146, 153])
2025-05-28 19:41:12,354 | INFO | aether2 | Model output shape: torch.Size([1, 1147, 153])
2025-05-28 19:41:13,061 | INFO | aether2 | Model output shape: torch.Size([1, 1148, 153])
2025-05-28 19:41:13,817 | INFO | aether2 | Model output shape: torch.Size([1, 1149, 153])
2025-05-28 19:41:14,512 | INFO | aether2 | Model output shape: torch.Size([1, 1150, 153])
2025-05-28 19:41:15,288 | INFO | aether2 | Model output shape: torch.Size([1, 1151, 153])
2025-05-28 19:41:15,903 | INFO | aether2 | Model output shape: torch.Size([1, 1152, 153])
2025-05-28 19:41:16,616 | INFO | aether2 | Model output shape: torch.Size([1, 1153, 153])
2025-05-28 19:41:17,298 | INFO | aether2 | Model output shape: torch.Size([1, 1154, 153])
2025-05-28 19:41:17,972 | INFO | aether2 | Model output shape: torch.Size([1, 1155, 153])
2025-05-28 19:41:18,659 | INFO | aether2 | Model output shape: torch.Size([1, 1156, 153])
2025-05-28 19:41:19,335 | INFO | aether2 | Model output shape: torch.Size([1, 1157, 153])
2025-05-28 19:41:20,014 | INFO | aether2 | Model output shape: torch.Size([1, 1158, 153])
2025-05-28 19:41:20,731 | INFO | aether2 | Model output shape: torch.Size([1, 1159, 153])
2025-05-28 19:41:21,345 | INFO | aether2 | Model output shape: torch.Size([1, 1160, 153])
2025-05-28 19:41:22,066 | INFO | aether2 | Model output shape: torch.Size([1, 1161, 153])
2025-05-28 19:41:22,778 | INFO | aether2 | Model output shape: torch.Size([1, 1162, 153])
2025-05-28 19:41:23,585 | INFO | aether2 | Model output shape: torch.Size([1, 1163, 153])
2025-05-28 19:41:24,270 | INFO | aether2 | Model output shape: torch.Size([1, 1164, 153])
2025-05-28 19:41:24,938 | INFO | aether2 | Model output shape: torch.Size([1, 1165, 153])
2025-05-28 19:41:25,619 | INFO | aether2 | Model output shape: torch.Size([1, 1166, 153])
2025-05-28 19:41:42,372 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:41:42,476 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:41:42,477 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:41:42,477 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:41:42,586 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:41:42,593 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:41:42,593 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:41:59,514 | INFO | aether2 | GOAL: hi
2025-05-28 19:41:59,518 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:41:59,536 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:41:59,551 | INFO | aether2 | Model output shape: torch.Size([1, 4, 153])
2025-05-28 19:41:59,567 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:41:59,590 | INFO | aether2 | Model output shape: torch.Size([1, 6, 153])
2025-05-28 19:41:59,605 | INFO | aether2 | Model output shape: torch.Size([1, 7, 153])
2025-05-28 19:41:59,623 | INFO | aether2 | Model output shape: torch.Size([1, 8, 153])
2025-05-28 19:41:59,639 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 19:41:59,657 | INFO | aether2 | Model output shape: torch.Size([1, 10, 153])
2025-05-28 19:41:59,674 | INFO | aether2 | Model output shape: torch.Size([1, 11, 153])
2025-05-28 19:41:59,693 | INFO | aether2 | Model output shape: torch.Size([1, 12, 153])
2025-05-28 19:41:59,711 | INFO | aether2 | Model output shape: torch.Size([1, 13, 153])
2025-05-28 19:41:59,728 | INFO | aether2 | Model output shape: torch.Size([1, 14, 153])
2025-05-28 19:41:59,745 | INFO | aether2 | Model output shape: torch.Size([1, 15, 153])
2025-05-28 19:41:59,761 | INFO | aether2 | Model output shape: torch.Size([1, 16, 153])
2025-05-28 19:41:59,778 | INFO | aether2 | Model output shape: torch.Size([1, 17, 153])
2025-05-28 19:41:59,797 | INFO | aether2 | Model output shape: torch.Size([1, 18, 153])
2025-05-28 19:41:59,816 | INFO | aether2 | Model output shape: torch.Size([1, 19, 153])
2025-05-28 19:41:59,834 | INFO | aether2 | Model output shape: torch.Size([1, 20, 153])
2025-05-28 19:41:59,855 | INFO | aether2 | Model output shape: torch.Size([1, 21, 153])
2025-05-28 19:41:59,873 | INFO | aether2 | Model output shape: torch.Size([1, 22, 153])
2025-05-28 19:41:59,892 | INFO | aether2 | Model output shape: torch.Size([1, 23, 153])
2025-05-28 19:41:59,910 | INFO | aether2 | Model output shape: torch.Size([1, 24, 153])
2025-05-28 19:41:59,930 | INFO | aether2 | Model output shape: torch.Size([1, 25, 153])
2025-05-28 19:41:59,948 | INFO | aether2 | Model output shape: torch.Size([1, 26, 153])
2025-05-28 19:41:59,967 | INFO | aether2 | Model output shape: torch.Size([1, 27, 153])
2025-05-28 19:41:59,986 | INFO | aether2 | Model output shape: torch.Size([1, 28, 153])
2025-05-28 19:42:00,004 | INFO | aether2 | Model output shape: torch.Size([1, 29, 153])
2025-05-28 19:42:00,027 | INFO | aether2 | Model output shape: torch.Size([1, 30, 153])
2025-05-28 19:42:00,047 | INFO | aether2 | Model output shape: torch.Size([1, 31, 153])
2025-05-28 19:42:00,068 | INFO | aether2 | Model output shape: torch.Size([1, 32, 153])
2025-05-28 19:42:00,087 | INFO | aether2 | Model output shape: torch.Size([1, 33, 153])
2025-05-28 19:42:00,106 | INFO | aether2 | Model output shape: torch.Size([1, 34, 153])
2025-05-28 19:42:00,128 | INFO | aether2 | Model output shape: torch.Size([1, 35, 153])
2025-05-28 19:42:00,148 | INFO | aether2 | Model output shape: torch.Size([1, 36, 153])
2025-05-28 19:42:00,168 | INFO | aether2 | Model output shape: torch.Size([1, 37, 153])
2025-05-28 19:42:00,189 | INFO | aether2 | Model output shape: torch.Size([1, 38, 153])
2025-05-28 19:42:00,213 | INFO | aether2 | Model output shape: torch.Size([1, 39, 153])
2025-05-28 19:42:00,235 | INFO | aether2 | Model output shape: torch.Size([1, 40, 153])
2025-05-28 19:42:00,253 | INFO | aether2 | Model output shape: torch.Size([1, 41, 153])
2025-05-28 19:42:00,273 | INFO | aether2 | Model output shape: torch.Size([1, 42, 153])
2025-05-28 19:42:00,297 | INFO | aether2 | Model output shape: torch.Size([1, 43, 153])
2025-05-28 19:42:00,319 | INFO | aether2 | Model output shape: torch.Size([1, 44, 153])
2025-05-28 19:42:00,342 | INFO | aether2 | Model output shape: torch.Size([1, 45, 153])
2025-05-28 19:42:00,363 | INFO | aether2 | Model output shape: torch.Size([1, 46, 153])
2025-05-28 19:42:00,385 | INFO | aether2 | Model output shape: torch.Size([1, 47, 153])
2025-05-28 19:42:00,407 | INFO | aether2 | Model output shape: torch.Size([1, 48, 153])
2025-05-28 19:42:00,427 | INFO | aether2 | Model output shape: torch.Size([1, 49, 153])
2025-05-28 19:42:00,451 | INFO | aether2 | Model output shape: torch.Size([1, 50, 153])
2025-05-28 19:42:00,475 | INFO | aether2 | Model output shape: torch.Size([1, 51, 153])
2025-05-28 19:42:00,497 | INFO | aether2 | Model output shape: torch.Size([1, 52, 153])
2025-05-28 19:42:00,519 | INFO | aether2 | Model output shape: torch.Size([1, 53, 153])
2025-05-28 19:42:00,543 | INFO | aether2 | Model output shape: torch.Size([1, 54, 153])
2025-05-28 19:42:00,566 | INFO | aether2 | Model output shape: torch.Size([1, 55, 153])
2025-05-28 19:42:00,591 | INFO | aether2 | Model output shape: torch.Size([1, 56, 153])
2025-05-28 19:42:00,614 | INFO | aether2 | Model output shape: torch.Size([1, 57, 153])
2025-05-28 19:42:00,639 | INFO | aether2 | Model output shape: torch.Size([1, 58, 153])
2025-05-28 19:42:00,660 | INFO | aether2 | Model output shape: torch.Size([1, 59, 153])
2025-05-28 19:42:00,682 | INFO | aether2 | Model output shape: torch.Size([1, 60, 153])
2025-05-28 19:42:00,706 | INFO | aether2 | Model output shape: torch.Size([1, 61, 153])
2025-05-28 19:42:00,732 | INFO | aether2 | Model output shape: torch.Size([1, 62, 153])
2025-05-28 19:42:00,754 | INFO | aether2 | Model output shape: torch.Size([1, 63, 153])
2025-05-28 19:42:00,778 | INFO | aether2 | Model output shape: torch.Size([1, 64, 153])
2025-05-28 19:42:00,802 | INFO | aether2 | Model output shape: torch.Size([1, 65, 153])
2025-05-28 19:42:00,824 | INFO | aether2 | Model output shape: torch.Size([1, 66, 153])
2025-05-28 19:42:00,846 | INFO | aether2 | Model output shape: torch.Size([1, 67, 153])
2025-05-28 19:42:00,870 | INFO | aether2 | Model output shape: torch.Size([1, 68, 153])
2025-05-28 19:42:00,896 | INFO | aether2 | Model output shape: torch.Size([1, 69, 153])
2025-05-28 19:42:00,920 | INFO | aether2 | Model output shape: torch.Size([1, 70, 153])
2025-05-28 19:42:00,951 | INFO | aether2 | Model output shape: torch.Size([1, 71, 153])
2025-05-28 19:42:00,975 | INFO | aether2 | Model output shape: torch.Size([1, 72, 153])
2025-05-28 19:42:01,002 | INFO | aether2 | Model output shape: torch.Size([1, 73, 153])
2025-05-28 19:42:01,026 | INFO | aether2 | Model output shape: torch.Size([1, 74, 153])
2025-05-28 19:42:01,050 | INFO | aether2 | Model output shape: torch.Size([1, 75, 153])
2025-05-28 19:42:01,078 | INFO | aether2 | Model output shape: torch.Size([1, 76, 153])
2025-05-28 19:42:01,104 | INFO | aether2 | Model output shape: torch.Size([1, 77, 153])
2025-05-28 19:42:01,130 | INFO | aether2 | Model output shape: torch.Size([1, 78, 153])
2025-05-28 19:42:01,156 | INFO | aether2 | Model output shape: torch.Size([1, 79, 153])
2025-05-28 19:42:01,181 | INFO | aether2 | Model output shape: torch.Size([1, 80, 153])
2025-05-28 19:42:01,208 | INFO | aether2 | Model output shape: torch.Size([1, 81, 153])
2025-05-28 19:42:01,234 | INFO | aether2 | Model output shape: torch.Size([1, 82, 153])
2025-05-28 19:42:01,268 | INFO | aether2 | Model output shape: torch.Size([1, 83, 153])
2025-05-28 19:42:01,296 | INFO | aether2 | Model output shape: torch.Size([1, 84, 153])
2025-05-28 19:42:01,325 | INFO | aether2 | Model output shape: torch.Size([1, 85, 153])
2025-05-28 19:42:01,355 | INFO | aether2 | Model output shape: torch.Size([1, 86, 153])
2025-05-28 19:42:01,383 | INFO | aether2 | Model output shape: torch.Size([1, 87, 153])
2025-05-28 19:42:01,413 | INFO | aether2 | Model output shape: torch.Size([1, 88, 153])
2025-05-28 19:42:01,442 | INFO | aether2 | Model output shape: torch.Size([1, 89, 153])
2025-05-28 19:42:01,468 | INFO | aether2 | Model output shape: torch.Size([1, 90, 153])
2025-05-28 19:42:01,497 | INFO | aether2 | Model output shape: torch.Size([1, 91, 153])
2025-05-28 19:42:01,526 | INFO | aether2 | Model output shape: torch.Size([1, 92, 153])
2025-05-28 19:42:01,558 | INFO | aether2 | Model output shape: torch.Size([1, 93, 153])
2025-05-28 19:42:01,589 | INFO | aether2 | Model output shape: torch.Size([1, 94, 153])
2025-05-28 19:42:01,617 | INFO | aether2 | Model output shape: torch.Size([1, 95, 153])
2025-05-28 19:42:01,649 | INFO | aether2 | Model output shape: torch.Size([1, 96, 153])
2025-05-28 19:42:01,675 | INFO | aether2 | Model output shape: torch.Size([1, 97, 153])
2025-05-28 19:42:01,701 | INFO | aether2 | Model output shape: torch.Size([1, 98, 153])
2025-05-28 19:42:01,730 | INFO | aether2 | Model output shape: torch.Size([1, 99, 153])
2025-05-28 19:42:01,762 | INFO | aether2 | Model output shape: torch.Size([1, 100, 153])
2025-05-28 19:42:01,794 | INFO | aether2 | Model output shape: torch.Size([1, 101, 153])
2025-05-28 19:42:01,825 | INFO | aether2 | Model output shape: torch.Size([1, 102, 153])
2025-05-28 19:42:01,858 | INFO | aether2 | Model output shape: torch.Size([1, 103, 153])
2025-05-28 19:42:01,887 | INFO | aether2 | Model output shape: torch.Size([1, 104, 153])
2025-05-28 19:42:01,919 | INFO | aether2 | Model output shape: torch.Size([1, 105, 153])
2025-05-28 19:42:01,948 | INFO | aether2 | Model output shape: torch.Size([1, 106, 153])
2025-05-28 19:42:01,980 | INFO | aether2 | Model output shape: torch.Size([1, 107, 153])
2025-05-28 19:42:02,012 | INFO | aether2 | Model output shape: torch.Size([1, 108, 153])
2025-05-28 19:42:02,045 | INFO | aether2 | Model output shape: torch.Size([1, 109, 153])
2025-05-28 19:42:02,075 | INFO | aether2 | Model output shape: torch.Size([1, 110, 153])
2025-05-28 19:42:02,107 | INFO | aether2 | Model output shape: torch.Size([1, 111, 153])
2025-05-28 19:42:02,141 | INFO | aether2 | Model output shape: torch.Size([1, 112, 153])
2025-05-28 19:42:02,173 | INFO | aether2 | Model output shape: torch.Size([1, 113, 153])
2025-05-28 19:42:02,204 | INFO | aether2 | Model output shape: torch.Size([1, 114, 153])
2025-05-28 19:42:02,233 | INFO | aether2 | Model output shape: torch.Size([1, 115, 153])
2025-05-28 19:42:02,265 | INFO | aether2 | Model output shape: torch.Size([1, 116, 153])
2025-05-28 19:42:02,296 | INFO | aether2 | Model output shape: torch.Size([1, 117, 153])
2025-05-28 19:42:02,326 | INFO | aether2 | Model output shape: torch.Size([1, 118, 153])
2025-05-28 19:42:02,357 | INFO | aether2 | Model output shape: torch.Size([1, 119, 153])
2025-05-28 19:42:02,395 | INFO | aether2 | Model output shape: torch.Size([1, 120, 153])
2025-05-28 19:42:02,428 | INFO | aether2 | Model output shape: torch.Size([1, 121, 153])
2025-05-28 19:42:02,462 | INFO | aether2 | Model output shape: torch.Size([1, 122, 153])
2025-05-28 19:42:02,495 | INFO | aether2 | Model output shape: torch.Size([1, 123, 153])
2025-05-28 19:42:02,529 | INFO | aether2 | Model output shape: torch.Size([1, 124, 153])
2025-05-28 19:42:02,563 | INFO | aether2 | Model output shape: torch.Size([1, 125, 153])
2025-05-28 19:42:02,601 | INFO | aether2 | Model output shape: torch.Size([1, 126, 153])
2025-05-28 19:42:02,635 | INFO | aether2 | Model output shape: torch.Size([1, 127, 153])
2025-05-28 19:42:02,669 | INFO | aether2 | Model output shape: torch.Size([1, 128, 153])
2025-05-28 19:42:02,703 | INFO | aether2 | Model output shape: torch.Size([1, 129, 153])
2025-05-28 19:42:02,736 | INFO | aether2 | Model output shape: torch.Size([1, 130, 153])
2025-05-28 19:42:02,771 | INFO | aether2 | Model output shape: torch.Size([1, 131, 153])
2025-05-28 19:42:02,804 | INFO | aether2 | Model output shape: torch.Size([1, 132, 153])
2025-05-28 19:42:02,843 | INFO | aether2 | Model output shape: torch.Size([1, 133, 153])
2025-05-28 19:42:02,878 | INFO | aether2 | Model output shape: torch.Size([1, 134, 153])
2025-05-28 19:42:02,915 | INFO | aether2 | Model output shape: torch.Size([1, 135, 153])
2025-05-28 19:42:02,951 | INFO | aether2 | Model output shape: torch.Size([1, 136, 153])
2025-05-28 19:42:02,986 | INFO | aether2 | Model output shape: torch.Size([1, 137, 153])
2025-05-28 19:42:03,023 | INFO | aether2 | Model output shape: torch.Size([1, 138, 153])
2025-05-28 19:42:03,061 | INFO | aether2 | Model output shape: torch.Size([1, 139, 153])
2025-05-28 19:42:03,097 | INFO | aether2 | Model output shape: torch.Size([1, 140, 153])
2025-05-28 19:42:03,135 | INFO | aether2 | Model output shape: torch.Size([1, 141, 153])
2025-05-28 19:42:03,172 | INFO | aether2 | Model output shape: torch.Size([1, 142, 153])
2025-05-28 19:42:03,210 | INFO | aether2 | Model output shape: torch.Size([1, 143, 153])
2025-05-28 19:42:03,249 | INFO | aether2 | Model output shape: torch.Size([1, 144, 153])
2025-05-28 19:42:03,284 | INFO | aether2 | Model output shape: torch.Size([1, 145, 153])
2025-05-28 19:42:03,321 | INFO | aether2 | Model output shape: torch.Size([1, 146, 153])
2025-05-28 19:42:03,361 | INFO | aether2 | Model output shape: torch.Size([1, 147, 153])
2025-05-28 19:42:03,401 | INFO | aether2 | Model output shape: torch.Size([1, 148, 153])
2025-05-28 19:42:03,443 | INFO | aether2 | Model output shape: torch.Size([1, 149, 153])
2025-05-28 19:42:03,488 | INFO | aether2 | Model output shape: torch.Size([1, 150, 153])
2025-05-28 19:42:03,530 | INFO | aether2 | Model output shape: torch.Size([1, 151, 153])
2025-05-28 19:42:03,575 | INFO | aether2 | Model output shape: torch.Size([1, 152, 153])
2025-05-28 19:42:03,615 | INFO | aether2 | Model output shape: torch.Size([1, 153, 153])
2025-05-28 19:42:03,653 | INFO | aether2 | Model output shape: torch.Size([1, 154, 153])
2025-05-28 19:42:03,694 | INFO | aether2 | Model output shape: torch.Size([1, 155, 153])
2025-05-28 19:42:03,735 | INFO | aether2 | Model output shape: torch.Size([1, 156, 153])
2025-05-28 19:42:03,775 | INFO | aether2 | Model output shape: torch.Size([1, 157, 153])
2025-05-28 19:42:03,814 | INFO | aether2 | Model output shape: torch.Size([1, 158, 153])
2025-05-28 19:42:03,854 | INFO | aether2 | Model output shape: torch.Size([1, 159, 153])
2025-05-28 19:42:03,895 | INFO | aether2 | Model output shape: torch.Size([1, 160, 153])
2025-05-28 19:42:03,937 | INFO | aether2 | Model output shape: torch.Size([1, 161, 153])
2025-05-28 19:42:03,981 | INFO | aether2 | Model output shape: torch.Size([1, 162, 153])
2025-05-28 19:42:04,021 | INFO | aether2 | Model output shape: torch.Size([1, 163, 153])
2025-05-28 19:42:04,062 | INFO | aether2 | Model output shape: torch.Size([1, 164, 153])
2025-05-28 19:42:04,102 | INFO | aether2 | Model output shape: torch.Size([1, 165, 153])
2025-05-28 19:42:04,143 | INFO | aether2 | Model output shape: torch.Size([1, 166, 153])
2025-05-28 19:42:04,187 | INFO | aether2 | Model output shape: torch.Size([1, 167, 153])
2025-05-28 19:42:04,231 | INFO | aether2 | Model output shape: torch.Size([1, 168, 153])
2025-05-28 19:42:04,273 | INFO | aether2 | Model output shape: torch.Size([1, 169, 153])
2025-05-28 19:42:04,315 | INFO | aether2 | Model output shape: torch.Size([1, 170, 153])
2025-05-28 19:42:04,357 | INFO | aether2 | Model output shape: torch.Size([1, 171, 153])
2025-05-28 19:42:04,401 | INFO | aether2 | Model output shape: torch.Size([1, 172, 153])
2025-05-28 19:42:04,443 | INFO | aether2 | Model output shape: torch.Size([1, 173, 153])
2025-05-28 19:42:04,486 | INFO | aether2 | Model output shape: torch.Size([1, 174, 153])
2025-05-28 19:42:04,530 | INFO | aether2 | Model output shape: torch.Size([1, 175, 153])
2025-05-28 19:42:04,574 | INFO | aether2 | Model output shape: torch.Size([1, 176, 153])
2025-05-28 19:42:04,619 | INFO | aether2 | Model output shape: torch.Size([1, 177, 153])
2025-05-28 19:42:04,664 | INFO | aether2 | Model output shape: torch.Size([1, 178, 153])
2025-05-28 19:42:04,709 | INFO | aether2 | Model output shape: torch.Size([1, 179, 153])
2025-05-28 19:42:04,754 | INFO | aether2 | Model output shape: torch.Size([1, 180, 153])
2025-05-28 19:42:04,798 | INFO | aether2 | Model output shape: torch.Size([1, 181, 153])
2025-05-28 19:42:04,838 | INFO | aether2 | Model output shape: torch.Size([1, 182, 153])
2025-05-28 19:42:04,885 | INFO | aether2 | Model output shape: torch.Size([1, 183, 153])
2025-05-28 19:42:04,929 | INFO | aether2 | Model output shape: torch.Size([1, 184, 153])
2025-05-28 19:42:04,975 | INFO | aether2 | Model output shape: torch.Size([1, 185, 153])
2025-05-28 19:42:05,018 | INFO | aether2 | Model output shape: torch.Size([1, 186, 153])
2025-05-28 19:42:05,064 | INFO | aether2 | Model output shape: torch.Size([1, 187, 153])
2025-05-28 19:42:05,111 | INFO | aether2 | Model output shape: torch.Size([1, 188, 153])
2025-05-28 19:42:05,156 | INFO | aether2 | Model output shape: torch.Size([1, 189, 153])
2025-05-28 19:42:05,203 | INFO | aether2 | Model output shape: torch.Size([1, 190, 153])
2025-05-28 19:42:05,251 | INFO | aether2 | Model output shape: torch.Size([1, 191, 153])
2025-05-28 19:42:05,299 | INFO | aether2 | Model output shape: torch.Size([1, 192, 153])
2025-05-28 19:42:05,346 | INFO | aether2 | Model output shape: torch.Size([1, 193, 153])
2025-05-28 19:42:05,395 | INFO | aether2 | Model output shape: torch.Size([1, 194, 153])
2025-05-28 19:42:05,444 | INFO | aether2 | Model output shape: torch.Size([1, 195, 153])
2025-05-28 19:42:05,492 | INFO | aether2 | Model output shape: torch.Size([1, 196, 153])
2025-05-28 19:42:05,543 | INFO | aether2 | Model output shape: torch.Size([1, 197, 153])
2025-05-28 19:42:05,591 | INFO | aether2 | Model output shape: torch.Size([1, 198, 153])
2025-05-28 19:42:05,640 | INFO | aether2 | Model output shape: torch.Size([1, 199, 153])
2025-05-28 19:42:05,690 | INFO | aether2 | Model output shape: torch.Size([1, 200, 153])
2025-05-28 19:42:05,738 | INFO | aether2 | Model output shape: torch.Size([1, 201, 153])
2025-05-28 19:42:05,788 | INFO | aether2 | Model output shape: torch.Size([1, 202, 153])
2025-05-28 19:42:05,838 | INFO | aether2 | Model output shape: torch.Size([1, 203, 153])
2025-05-28 19:42:05,887 | INFO | aether2 | Model output shape: torch.Size([1, 204, 153])
2025-05-28 19:42:05,933 | INFO | aether2 | Model output shape: torch.Size([1, 205, 153])
2025-05-28 19:42:05,983 | INFO | aether2 | Model output shape: torch.Size([1, 206, 153])
2025-05-28 19:42:06,034 | INFO | aether2 | Model output shape: torch.Size([1, 207, 153])
2025-05-28 19:42:06,085 | INFO | aether2 | Model output shape: torch.Size([1, 208, 153])
2025-05-28 19:42:06,135 | INFO | aether2 | Model output shape: torch.Size([1, 209, 153])
2025-05-28 19:42:06,244 | INFO | aether2 | Model output shape: torch.Size([1, 210, 153])
2025-05-28 19:42:06,297 | INFO | aether2 | Model output shape: torch.Size([1, 211, 153])
2025-05-28 19:42:06,347 | INFO | aether2 | Model output shape: torch.Size([1, 212, 153])
2025-05-28 19:42:06,400 | INFO | aether2 | Model output shape: torch.Size([1, 213, 153])
2025-05-28 19:42:06,453 | INFO | aether2 | Model output shape: torch.Size([1, 214, 153])
2025-05-28 19:42:06,506 | INFO | aether2 | Model output shape: torch.Size([1, 215, 153])
2025-05-28 19:42:06,560 | INFO | aether2 | Model output shape: torch.Size([1, 216, 153])
2025-05-28 19:42:06,614 | INFO | aether2 | Model output shape: torch.Size([1, 217, 153])
2025-05-28 19:42:06,670 | INFO | aether2 | Model output shape: torch.Size([1, 218, 153])
2025-05-28 19:42:06,722 | INFO | aether2 | Model output shape: torch.Size([1, 219, 153])
2025-05-28 19:42:06,775 | INFO | aether2 | Model output shape: torch.Size([1, 220, 153])
2025-05-28 19:42:06,826 | INFO | aether2 | Model output shape: torch.Size([1, 221, 153])
2025-05-28 19:42:06,883 | INFO | aether2 | Model output shape: torch.Size([1, 222, 153])
2025-05-28 19:42:06,935 | INFO | aether2 | Model output shape: torch.Size([1, 223, 153])
2025-05-28 19:42:06,992 | INFO | aether2 | Model output shape: torch.Size([1, 224, 153])
2025-05-28 19:42:07,047 | INFO | aether2 | Model output shape: torch.Size([1, 225, 153])
2025-05-28 19:42:07,103 | INFO | aether2 | Model output shape: torch.Size([1, 226, 153])
2025-05-28 19:42:07,158 | INFO | aether2 | Model output shape: torch.Size([1, 227, 153])
2025-05-28 19:42:07,215 | INFO | aether2 | Model output shape: torch.Size([1, 228, 153])
2025-05-28 19:42:07,271 | INFO | aether2 | Model output shape: torch.Size([1, 229, 153])
2025-05-28 19:42:07,327 | INFO | aether2 | Model output shape: torch.Size([1, 230, 153])
2025-05-28 19:42:07,386 | INFO | aether2 | Model output shape: torch.Size([1, 231, 153])
2025-05-28 19:42:07,441 | INFO | aether2 | Model output shape: torch.Size([1, 232, 153])
2025-05-28 19:42:07,497 | INFO | aether2 | Model output shape: torch.Size([1, 233, 153])
2025-05-28 19:42:07,554 | INFO | aether2 | Model output shape: torch.Size([1, 234, 153])
2025-05-28 19:42:07,607 | INFO | aether2 | Model output shape: torch.Size([1, 235, 153])
2025-05-28 19:42:07,660 | INFO | aether2 | Model output shape: torch.Size([1, 236, 153])
2025-05-28 19:42:07,715 | INFO | aether2 | Model output shape: torch.Size([1, 237, 153])
2025-05-28 19:42:07,768 | INFO | aether2 | Model output shape: torch.Size([1, 238, 153])
2025-05-28 19:42:07,824 | INFO | aether2 | Model output shape: torch.Size([1, 239, 153])
2025-05-28 19:42:07,879 | INFO | aether2 | Model output shape: torch.Size([1, 240, 153])
2025-05-28 19:42:07,935 | INFO | aether2 | Model output shape: torch.Size([1, 241, 153])
2025-05-28 19:42:07,988 | INFO | aether2 | Model output shape: torch.Size([1, 242, 153])
2025-05-28 19:42:08,044 | INFO | aether2 | Model output shape: torch.Size([1, 243, 153])
2025-05-28 19:42:08,097 | INFO | aether2 | Model output shape: torch.Size([1, 244, 153])
2025-05-28 19:42:08,154 | INFO | aether2 | Model output shape: torch.Size([1, 245, 153])
2025-05-28 19:42:08,211 | INFO | aether2 | Model output shape: torch.Size([1, 246, 153])
2025-05-28 19:42:08,266 | INFO | aether2 | Model output shape: torch.Size([1, 247, 153])
2025-05-28 19:42:08,319 | INFO | aether2 | Model output shape: torch.Size([1, 248, 153])
2025-05-28 19:42:08,369 | INFO | aether2 | Model output shape: torch.Size([1, 249, 153])
2025-05-28 19:42:08,427 | INFO | aether2 | Model output shape: torch.Size([1, 250, 153])
2025-05-28 19:42:08,491 | INFO | aether2 | Model output shape: torch.Size([1, 251, 153])
2025-05-28 19:42:08,555 | INFO | aether2 | Model output shape: torch.Size([1, 252, 153])
2025-05-28 19:42:08,614 | INFO | aether2 | Model output shape: torch.Size([1, 253, 153])
2025-05-28 19:42:08,677 | INFO | aether2 | Model output shape: torch.Size([1, 254, 153])
2025-05-28 19:42:08,737 | INFO | aether2 | Model output shape: torch.Size([1, 255, 153])
2025-05-28 19:42:08,794 | INFO | aether2 | Model output shape: torch.Size([1, 256, 153])
2025-05-28 19:42:08,851 | INFO | aether2 | Model output shape: torch.Size([1, 257, 153])
2025-05-28 19:42:08,910 | INFO | aether2 | Model output shape: torch.Size([1, 258, 153])
2025-05-28 19:42:08,969 | INFO | aether2 | Model output shape: torch.Size([1, 259, 153])
2025-05-28 19:42:09,029 | INFO | aether2 | Model output shape: torch.Size([1, 260, 153])
2025-05-28 19:42:09,090 | INFO | aether2 | Model output shape: torch.Size([1, 261, 153])
2025-05-28 19:42:09,150 | INFO | aether2 | Model output shape: torch.Size([1, 262, 153])
2025-05-28 19:42:09,209 | INFO | aether2 | Model output shape: torch.Size([1, 263, 153])
2025-05-28 19:42:09,268 | INFO | aether2 | Model output shape: torch.Size([1, 264, 153])
2025-05-28 19:42:09,328 | INFO | aether2 | Model output shape: torch.Size([1, 265, 153])
2025-05-28 19:42:09,389 | INFO | aether2 | Model output shape: torch.Size([1, 266, 153])
2025-05-28 19:42:09,448 | INFO | aether2 | Model output shape: torch.Size([1, 267, 153])
2025-05-28 19:42:09,510 | INFO | aether2 | Model output shape: torch.Size([1, 268, 153])
2025-05-28 19:42:09,571 | INFO | aether2 | Model output shape: torch.Size([1, 269, 153])
2025-05-28 19:42:09,632 | INFO | aether2 | Model output shape: torch.Size([1, 270, 153])
2025-05-28 19:42:09,693 | INFO | aether2 | Model output shape: torch.Size([1, 271, 153])
2025-05-28 19:42:09,757 | INFO | aether2 | Model output shape: torch.Size([1, 272, 153])
2025-05-28 19:42:09,818 | INFO | aether2 | Model output shape: torch.Size([1, 273, 153])
2025-05-28 19:42:09,881 | INFO | aether2 | Model output shape: torch.Size([1, 274, 153])
2025-05-28 19:42:09,946 | INFO | aether2 | Model output shape: torch.Size([1, 275, 153])
2025-05-28 19:42:10,009 | INFO | aether2 | Model output shape: torch.Size([1, 276, 153])
2025-05-28 19:42:10,080 | INFO | aether2 | Model output shape: torch.Size([1, 277, 153])
2025-05-28 19:42:10,144 | INFO | aether2 | Model output shape: torch.Size([1, 278, 153])
2025-05-28 19:42:10,205 | INFO | aether2 | Model output shape: torch.Size([1, 279, 153])
2025-05-28 19:42:10,269 | INFO | aether2 | Model output shape: torch.Size([1, 280, 153])
2025-05-28 19:42:10,335 | INFO | aether2 | Model output shape: torch.Size([1, 281, 153])
2025-05-28 19:42:10,401 | INFO | aether2 | Model output shape: torch.Size([1, 282, 153])
2025-05-28 19:42:10,465 | INFO | aether2 | Model output shape: torch.Size([1, 283, 153])
2025-05-28 19:42:10,530 | INFO | aether2 | Model output shape: torch.Size([1, 284, 153])
2025-05-28 19:42:10,595 | INFO | aether2 | Model output shape: torch.Size([1, 285, 153])
2025-05-28 19:42:10,659 | INFO | aether2 | Model output shape: torch.Size([1, 286, 153])
2025-05-28 19:42:10,725 | INFO | aether2 | Model output shape: torch.Size([1, 287, 153])
2025-05-28 19:42:10,792 | INFO | aether2 | Model output shape: torch.Size([1, 288, 153])
2025-05-28 19:42:10,861 | INFO | aether2 | Model output shape: torch.Size([1, 289, 153])
2025-05-28 19:42:10,927 | INFO | aether2 | Model output shape: torch.Size([1, 290, 153])
2025-05-28 19:42:10,993 | INFO | aether2 | Model output shape: torch.Size([1, 291, 153])
2025-05-28 19:42:11,059 | INFO | aether2 | Model output shape: torch.Size([1, 292, 153])
2025-05-28 19:42:11,127 | INFO | aether2 | Model output shape: torch.Size([1, 293, 153])
2025-05-28 19:42:11,194 | INFO | aether2 | Model output shape: torch.Size([1, 294, 153])
2025-05-28 19:42:11,264 | INFO | aether2 | Model output shape: torch.Size([1, 295, 153])
2025-05-28 19:42:11,333 | INFO | aether2 | Model output shape: torch.Size([1, 296, 153])
2025-05-28 19:42:11,401 | INFO | aether2 | Model output shape: torch.Size([1, 297, 153])
2025-05-28 19:42:11,475 | INFO | aether2 | Model output shape: torch.Size([1, 298, 153])
2025-05-28 19:42:11,548 | INFO | aether2 | Model output shape: torch.Size([1, 299, 153])
2025-05-28 19:42:11,622 | INFO | aether2 | Model output shape: torch.Size([1, 300, 153])
2025-05-28 19:42:11,691 | INFO | aether2 | Model output shape: torch.Size([1, 301, 153])
2025-05-28 19:42:11,759 | INFO | aether2 | Model output shape: torch.Size([1, 302, 153])
2025-05-28 19:42:11,826 | INFO | aether2 | Model output shape: torch.Size([1, 303, 153])
2025-05-28 19:42:11,898 | INFO | aether2 | Model output shape: torch.Size([1, 304, 153])
2025-05-28 19:42:11,968 | INFO | aether2 | Model output shape: torch.Size([1, 305, 153])
2025-05-28 19:42:12,041 | INFO | aether2 | Model output shape: torch.Size([1, 306, 153])
2025-05-28 19:42:12,113 | INFO | aether2 | Model output shape: torch.Size([1, 307, 153])
2025-05-28 19:42:12,185 | INFO | aether2 | Model output shape: torch.Size([1, 308, 153])
2025-05-28 19:42:12,258 | INFO | aether2 | Model output shape: torch.Size([1, 309, 153])
2025-05-28 19:42:12,332 | INFO | aether2 | Model output shape: torch.Size([1, 310, 153])
2025-05-28 19:42:12,406 | INFO | aether2 | Model output shape: torch.Size([1, 311, 153])
2025-05-28 19:42:12,476 | INFO | aether2 | Model output shape: torch.Size([1, 312, 153])
2025-05-28 19:42:12,551 | INFO | aether2 | Model output shape: torch.Size([1, 313, 153])
2025-05-28 19:42:12,629 | INFO | aether2 | Model output shape: torch.Size([1, 314, 153])
2025-05-28 19:42:12,701 | INFO | aether2 | Model output shape: torch.Size([1, 315, 153])
2025-05-28 19:42:12,774 | INFO | aether2 | Model output shape: torch.Size([1, 316, 153])
2025-05-28 19:42:12,849 | INFO | aether2 | Model output shape: torch.Size([1, 317, 153])
2025-05-28 19:42:12,921 | INFO | aether2 | Model output shape: torch.Size([1, 318, 153])
2025-05-28 19:42:12,994 | INFO | aether2 | Model output shape: torch.Size([1, 319, 153])
2025-05-28 19:42:13,068 | INFO | aether2 | Model output shape: torch.Size([1, 320, 153])
2025-05-28 19:42:13,144 | INFO | aether2 | Model output shape: torch.Size([1, 321, 153])
2025-05-28 19:42:13,219 | INFO | aether2 | Model output shape: torch.Size([1, 322, 153])
2025-05-28 19:42:13,295 | INFO | aether2 | Model output shape: torch.Size([1, 323, 153])
2025-05-28 19:42:13,369 | INFO | aether2 | Model output shape: torch.Size([1, 324, 153])
2025-05-28 19:42:13,451 | INFO | aether2 | Model output shape: torch.Size([1, 325, 153])
2025-05-28 19:42:13,533 | INFO | aether2 | Model output shape: torch.Size([1, 326, 153])
2025-05-28 19:42:13,619 | INFO | aether2 | Model output shape: torch.Size([1, 327, 153])
2025-05-28 19:42:13,698 | INFO | aether2 | Model output shape: torch.Size([1, 328, 153])
2025-05-28 19:42:13,776 | INFO | aether2 | Model output shape: torch.Size([1, 329, 153])
2025-05-28 19:42:13,851 | INFO | aether2 | Model output shape: torch.Size([1, 330, 153])
2025-05-28 19:42:13,925 | INFO | aether2 | Model output shape: torch.Size([1, 331, 153])
2025-05-28 19:42:14,001 | INFO | aether2 | Model output shape: torch.Size([1, 332, 153])
2025-05-28 19:42:14,081 | INFO | aether2 | Model output shape: torch.Size([1, 333, 153])
2025-05-28 19:42:14,159 | INFO | aether2 | Model output shape: torch.Size([1, 334, 153])
2025-05-28 19:42:14,238 | INFO | aether2 | Model output shape: torch.Size([1, 335, 153])
2025-05-28 19:42:14,317 | INFO | aether2 | Model output shape: torch.Size([1, 336, 153])
2025-05-28 19:42:14,400 | INFO | aether2 | Model output shape: torch.Size([1, 337, 153])
2025-05-28 19:42:14,481 | INFO | aether2 | Model output shape: torch.Size([1, 338, 153])
2025-05-28 19:42:14,563 | INFO | aether2 | Model output shape: torch.Size([1, 339, 153])
2025-05-28 19:42:14,646 | INFO | aether2 | Model output shape: torch.Size([1, 340, 153])
2025-05-28 19:42:14,726 | INFO | aether2 | Model output shape: torch.Size([1, 341, 153])
2025-05-28 19:42:14,810 | INFO | aether2 | Model output shape: torch.Size([1, 342, 153])
2025-05-28 19:42:14,896 | INFO | aether2 | Model output shape: torch.Size([1, 343, 153])
2025-05-28 19:42:14,978 | INFO | aether2 | Model output shape: torch.Size([1, 344, 153])
2025-05-28 19:42:15,062 | INFO | aether2 | Model output shape: torch.Size([1, 345, 153])
2025-05-28 19:42:15,144 | INFO | aether2 | Model output shape: torch.Size([1, 346, 153])
2025-05-28 19:42:15,221 | INFO | aether2 | Model output shape: torch.Size([1, 347, 153])
2025-05-28 19:42:15,303 | INFO | aether2 | Model output shape: torch.Size([1, 348, 153])
2025-05-28 19:42:15,388 | INFO | aether2 | Model output shape: torch.Size([1, 349, 153])
2025-05-28 19:42:15,470 | INFO | aether2 | Model output shape: torch.Size([1, 350, 153])
2025-05-28 19:42:15,554 | INFO | aether2 | Model output shape: torch.Size([1, 351, 153])
2025-05-28 19:42:15,640 | INFO | aether2 | Model output shape: torch.Size([1, 352, 153])
2025-05-28 19:42:15,725 | INFO | aether2 | Model output shape: torch.Size([1, 353, 153])
2025-05-28 19:42:15,811 | INFO | aether2 | Model output shape: torch.Size([1, 354, 153])
2025-05-28 19:42:15,896 | INFO | aether2 | Model output shape: torch.Size([1, 355, 153])
2025-05-28 19:42:15,978 | INFO | aether2 | Model output shape: torch.Size([1, 356, 153])
2025-05-28 19:42:16,068 | INFO | aether2 | Model output shape: torch.Size([1, 357, 153])
2025-05-28 19:42:16,152 | INFO | aether2 | Model output shape: torch.Size([1, 358, 153])
2025-05-28 19:42:16,241 | INFO | aether2 | Model output shape: torch.Size([1, 359, 153])
2025-05-28 19:42:16,329 | INFO | aether2 | Model output shape: torch.Size([1, 360, 153])
2025-05-28 19:42:16,419 | INFO | aether2 | Model output shape: torch.Size([1, 361, 153])
2025-05-28 19:42:16,509 | INFO | aether2 | Model output shape: torch.Size([1, 362, 153])
2025-05-28 19:42:16,597 | INFO | aether2 | Model output shape: torch.Size([1, 363, 153])
2025-05-28 19:42:16,686 | INFO | aether2 | Model output shape: torch.Size([1, 364, 153])
2025-05-28 19:42:16,775 | INFO | aether2 | Model output shape: torch.Size([1, 365, 153])
2025-05-28 19:42:16,866 | INFO | aether2 | Model output shape: torch.Size([1, 366, 153])
2025-05-28 19:42:16,959 | INFO | aether2 | Model output shape: torch.Size([1, 367, 153])
2025-05-28 19:42:17,047 | INFO | aether2 | Model output shape: torch.Size([1, 368, 153])
2025-05-28 19:42:17,140 | INFO | aether2 | Model output shape: torch.Size([1, 369, 153])
2025-05-28 19:42:17,234 | INFO | aether2 | Model output shape: torch.Size([1, 370, 153])
2025-05-28 19:42:17,328 | INFO | aether2 | Model output shape: torch.Size([1, 371, 153])
2025-05-28 19:42:17,416 | INFO | aether2 | Model output shape: torch.Size([1, 372, 153])
2025-05-28 19:42:17,507 | INFO | aether2 | Model output shape: torch.Size([1, 373, 153])
2025-05-28 19:42:17,599 | INFO | aether2 | Model output shape: torch.Size([1, 374, 153])
2025-05-28 19:42:17,693 | INFO | aether2 | Model output shape: torch.Size([1, 375, 153])
2025-05-28 19:42:17,786 | INFO | aether2 | Model output shape: torch.Size([1, 376, 153])
2025-05-28 19:42:17,880 | INFO | aether2 | Model output shape: torch.Size([1, 377, 153])
2025-05-28 19:42:17,974 | INFO | aether2 | Model output shape: torch.Size([1, 378, 153])
2025-05-28 19:42:18,072 | INFO | aether2 | Model output shape: torch.Size([1, 379, 153])
2025-05-28 19:42:18,166 | INFO | aether2 | Model output shape: torch.Size([1, 380, 153])
2025-05-28 19:42:18,261 | INFO | aether2 | Model output shape: torch.Size([1, 381, 153])
2025-05-28 19:42:18,356 | INFO | aether2 | Model output shape: torch.Size([1, 382, 153])
2025-05-28 19:42:18,459 | INFO | aether2 | Model output shape: torch.Size([1, 383, 153])
2025-05-28 19:42:18,562 | INFO | aether2 | Model output shape: torch.Size([1, 384, 153])
2025-05-28 19:42:18,662 | INFO | aether2 | Model output shape: torch.Size([1, 385, 153])
2025-05-28 19:42:18,758 | INFO | aether2 | Model output shape: torch.Size([1, 386, 153])
2025-05-28 19:42:18,858 | INFO | aether2 | Model output shape: torch.Size([1, 387, 153])
2025-05-28 19:42:18,956 | INFO | aether2 | Model output shape: torch.Size([1, 388, 153])
2025-05-28 19:42:19,055 | INFO | aether2 | Model output shape: torch.Size([1, 389, 153])
2025-05-28 19:42:19,153 | INFO | aether2 | Model output shape: torch.Size([1, 390, 153])
2025-05-28 19:42:19,251 | INFO | aether2 | Model output shape: torch.Size([1, 391, 153])
2025-05-28 19:42:19,349 | INFO | aether2 | Model output shape: torch.Size([1, 392, 153])
2025-05-28 19:42:19,451 | INFO | aether2 | Model output shape: torch.Size([1, 393, 153])
2025-05-28 19:42:19,549 | INFO | aether2 | Model output shape: torch.Size([1, 394, 153])
2025-05-28 19:42:19,651 | INFO | aether2 | Model output shape: torch.Size([1, 395, 153])
2025-05-28 19:42:19,752 | INFO | aether2 | Model output shape: torch.Size([1, 396, 153])
2025-05-28 19:42:19,855 | INFO | aether2 | Model output shape: torch.Size([1, 397, 153])
2025-05-28 19:42:19,959 | INFO | aether2 | Model output shape: torch.Size([1, 398, 153])
2025-05-28 19:42:20,061 | INFO | aether2 | Model output shape: torch.Size([1, 399, 153])
2025-05-28 19:42:20,161 | INFO | aether2 | Model output shape: torch.Size([1, 400, 153])
2025-05-28 19:42:20,301 | INFO | aether2 | Model output shape: torch.Size([1, 401, 153])
2025-05-28 19:42:20,403 | INFO | aether2 | Model output shape: torch.Size([1, 402, 153])
2025-05-28 19:42:20,508 | INFO | aether2 | Model output shape: torch.Size([1, 403, 153])
2025-05-28 19:42:20,613 | INFO | aether2 | Model output shape: torch.Size([1, 404, 153])
2025-05-28 19:42:20,718 | INFO | aether2 | Model output shape: torch.Size([1, 405, 153])
2025-05-28 19:42:20,823 | INFO | aether2 | Model output shape: torch.Size([1, 406, 153])
2025-05-28 19:42:20,930 | INFO | aether2 | Model output shape: torch.Size([1, 407, 153])
2025-05-28 19:42:21,030 | INFO | aether2 | Model output shape: torch.Size([1, 408, 153])
2025-05-28 19:42:21,140 | INFO | aether2 | Model output shape: torch.Size([1, 409, 153])
2025-05-28 19:42:21,246 | INFO | aether2 | Model output shape: torch.Size([1, 410, 153])
2025-05-28 19:42:21,352 | INFO | aether2 | Model output shape: torch.Size([1, 411, 153])
2025-05-28 19:42:21,461 | INFO | aether2 | Model output shape: torch.Size([1, 412, 153])
2025-05-28 19:42:21,568 | INFO | aether2 | Model output shape: torch.Size([1, 413, 153])
2025-05-28 19:42:21,681 | INFO | aether2 | Model output shape: torch.Size([1, 414, 153])
2025-05-28 19:42:21,794 | INFO | aether2 | Model output shape: torch.Size([1, 415, 153])
2025-05-28 19:42:21,899 | INFO | aether2 | Model output shape: torch.Size([1, 416, 153])
2025-05-28 19:42:22,009 | INFO | aether2 | Model output shape: torch.Size([1, 417, 153])
2025-05-28 19:42:22,117 | INFO | aether2 | Model output shape: torch.Size([1, 418, 153])
2025-05-28 19:42:22,224 | INFO | aether2 | Model output shape: torch.Size([1, 419, 153])
2025-05-28 19:42:22,334 | INFO | aether2 | Model output shape: torch.Size([1, 420, 153])
2025-05-28 19:42:22,447 | INFO | aether2 | Model output shape: torch.Size([1, 421, 153])
2025-05-28 19:42:22,559 | INFO | aether2 | Model output shape: torch.Size([1, 422, 153])
2025-05-28 19:42:22,672 | INFO | aether2 | Model output shape: torch.Size([1, 423, 153])
2025-05-28 19:42:22,784 | INFO | aether2 | Model output shape: torch.Size([1, 424, 153])
2025-05-28 19:42:22,895 | INFO | aether2 | Model output shape: torch.Size([1, 425, 153])
2025-05-28 19:42:23,008 | INFO | aether2 | Model output shape: torch.Size([1, 426, 153])
2025-05-28 19:42:23,119 | INFO | aether2 | Model output shape: torch.Size([1, 427, 153])
2025-05-28 19:42:23,232 | INFO | aether2 | Model output shape: torch.Size([1, 428, 153])
2025-05-28 19:42:23,343 | INFO | aether2 | Model output shape: torch.Size([1, 429, 153])
2025-05-28 19:42:23,463 | INFO | aether2 | Model output shape: torch.Size([1, 430, 153])
2025-05-28 19:42:23,587 | INFO | aether2 | Model output shape: torch.Size([1, 431, 153])
2025-05-28 19:42:23,702 | INFO | aether2 | Model output shape: torch.Size([1, 432, 153])
2025-05-28 19:42:23,810 | INFO | aether2 | Model output shape: torch.Size([1, 433, 153])
2025-05-28 19:42:23,925 | INFO | aether2 | Model output shape: torch.Size([1, 434, 153])
2025-05-28 19:42:24,044 | INFO | aether2 | Model output shape: torch.Size([1, 435, 153])
2025-05-28 19:42:24,159 | INFO | aether2 | Model output shape: torch.Size([1, 436, 153])
2025-05-28 19:42:24,275 | INFO | aether2 | Model output shape: torch.Size([1, 437, 153])
2025-05-28 19:42:24,388 | INFO | aether2 | Model output shape: torch.Size([1, 438, 153])
2025-05-28 19:42:24,505 | INFO | aether2 | Model output shape: torch.Size([1, 439, 153])
2025-05-28 19:42:24,617 | INFO | aether2 | Model output shape: torch.Size([1, 440, 153])
2025-05-28 19:42:24,736 | INFO | aether2 | Model output shape: torch.Size([1, 441, 153])
2025-05-28 19:42:24,853 | INFO | aether2 | Model output shape: torch.Size([1, 442, 153])
2025-05-28 19:42:24,972 | INFO | aether2 | Model output shape: torch.Size([1, 443, 153])
2025-05-28 19:42:25,092 | INFO | aether2 | Model output shape: torch.Size([1, 444, 153])
2025-05-28 19:42:25,213 | INFO | aether2 | Model output shape: torch.Size([1, 445, 153])
2025-05-28 19:42:25,329 | INFO | aether2 | Model output shape: torch.Size([1, 446, 153])
2025-05-28 19:42:25,448 | INFO | aether2 | Model output shape: torch.Size([1, 447, 153])
2025-05-28 19:42:25,567 | INFO | aether2 | Model output shape: torch.Size([1, 448, 153])
2025-05-28 19:42:25,689 | INFO | aether2 | Model output shape: torch.Size([1, 449, 153])
2025-05-28 19:42:25,811 | INFO | aether2 | Model output shape: torch.Size([1, 450, 153])
2025-05-28 19:42:25,932 | INFO | aether2 | Model output shape: torch.Size([1, 451, 153])
2025-05-28 19:42:26,057 | INFO | aether2 | Model output shape: torch.Size([1, 452, 153])
2025-05-28 19:42:26,181 | INFO | aether2 | Model output shape: torch.Size([1, 453, 153])
2025-05-28 19:42:26,302 | INFO | aether2 | Model output shape: torch.Size([1, 454, 153])
2025-05-28 19:42:26,426 | INFO | aether2 | Model output shape: torch.Size([1, 455, 153])
2025-05-28 19:42:26,548 | INFO | aether2 | Model output shape: torch.Size([1, 456, 153])
2025-05-28 19:42:26,673 | INFO | aether2 | Model output shape: torch.Size([1, 457, 153])
2025-05-28 19:42:26,799 | INFO | aether2 | Model output shape: torch.Size([1, 458, 153])
2025-05-28 19:42:26,924 | INFO | aether2 | Model output shape: torch.Size([1, 459, 153])
2025-05-28 19:42:27,049 | INFO | aether2 | Model output shape: torch.Size([1, 460, 153])
2025-05-28 19:42:27,174 | INFO | aether2 | Model output shape: torch.Size([1, 461, 153])
2025-05-28 19:42:27,299 | INFO | aether2 | Model output shape: torch.Size([1, 462, 153])
2025-05-28 19:42:27,427 | INFO | aether2 | Model output shape: torch.Size([1, 463, 153])
2025-05-28 19:42:27,553 | INFO | aether2 | Model output shape: torch.Size([1, 464, 153])
2025-05-28 19:42:27,679 | INFO | aether2 | Model output shape: torch.Size([1, 465, 153])
2025-05-28 19:42:27,806 | INFO | aether2 | Model output shape: torch.Size([1, 466, 153])
2025-05-28 19:42:27,939 | INFO | aether2 | Model output shape: torch.Size([1, 467, 153])
2025-05-28 19:42:28,063 | INFO | aether2 | Model output shape: torch.Size([1, 468, 153])
2025-05-28 19:42:28,192 | INFO | aether2 | Model output shape: torch.Size([1, 469, 153])
2025-05-28 19:42:28,318 | INFO | aether2 | Model output shape: torch.Size([1, 470, 153])
2025-05-28 19:42:28,444 | INFO | aether2 | Model output shape: torch.Size([1, 471, 153])
2025-05-28 19:42:28,580 | INFO | aether2 | Model output shape: torch.Size([1, 472, 153])
2025-05-28 19:42:28,718 | INFO | aether2 | Model output shape: torch.Size([1, 473, 153])
2025-05-28 19:42:28,853 | INFO | aether2 | Model output shape: torch.Size([1, 474, 153])
2025-05-28 19:42:28,984 | INFO | aether2 | Model output shape: torch.Size([1, 475, 153])
2025-05-28 19:42:29,116 | INFO | aether2 | Model output shape: torch.Size([1, 476, 153])
2025-05-28 19:42:29,247 | INFO | aether2 | Model output shape: torch.Size([1, 477, 153])
2025-05-28 19:42:29,375 | INFO | aether2 | Model output shape: torch.Size([1, 478, 153])
2025-05-28 19:42:29,510 | INFO | aether2 | Model output shape: torch.Size([1, 479, 153])
2025-05-28 19:42:29,640 | INFO | aether2 | Model output shape: torch.Size([1, 480, 153])
2025-05-28 19:42:29,774 | INFO | aether2 | Model output shape: torch.Size([1, 481, 153])
2025-05-28 19:42:29,904 | INFO | aether2 | Model output shape: torch.Size([1, 482, 153])
2025-05-28 19:42:30,036 | INFO | aether2 | Model output shape: torch.Size([1, 483, 153])
2025-05-28 19:42:30,169 | INFO | aether2 | Model output shape: torch.Size([1, 484, 153])
2025-05-28 19:42:30,302 | INFO | aether2 | Model output shape: torch.Size([1, 485, 153])
2025-05-28 19:42:30,437 | INFO | aether2 | Model output shape: torch.Size([1, 486, 153])
2025-05-28 19:42:30,576 | INFO | aether2 | Model output shape: torch.Size([1, 487, 153])
2025-05-28 19:42:30,710 | INFO | aether2 | Model output shape: torch.Size([1, 488, 153])
2025-05-28 19:42:30,847 | INFO | aether2 | Model output shape: torch.Size([1, 489, 153])
2025-05-28 19:42:30,985 | INFO | aether2 | Model output shape: torch.Size([1, 490, 153])
2025-05-28 19:42:31,127 | INFO | aether2 | Model output shape: torch.Size([1, 491, 153])
2025-05-28 19:42:31,262 | INFO | aether2 | Model output shape: torch.Size([1, 492, 153])
2025-05-28 19:42:31,403 | INFO | aether2 | Model output shape: torch.Size([1, 493, 153])
2025-05-28 19:42:31,546 | INFO | aether2 | Model output shape: torch.Size([1, 494, 153])
2025-05-28 19:42:31,689 | INFO | aether2 | Model output shape: torch.Size([1, 495, 153])
2025-05-28 19:42:31,824 | INFO | aether2 | Model output shape: torch.Size([1, 496, 153])
2025-05-28 19:42:31,977 | INFO | aether2 | Model output shape: torch.Size([1, 497, 153])
2025-05-28 19:42:32,123 | INFO | aether2 | Model output shape: torch.Size([1, 498, 153])
2025-05-28 19:42:32,263 | INFO | aether2 | Model output shape: torch.Size([1, 499, 153])
2025-05-28 19:42:32,403 | INFO | aether2 | Model output shape: torch.Size([1, 500, 153])
2025-05-28 19:42:32,544 | INFO | aether2 | Model output shape: torch.Size([1, 501, 153])
2025-05-28 19:42:32,688 | INFO | aether2 | Model output shape: torch.Size([1, 502, 153])
2025-05-28 19:42:32,828 | INFO | aether2 | Model output shape: torch.Size([1, 503, 153])
2025-05-28 19:42:32,970 | INFO | aether2 | Model output shape: torch.Size([1, 504, 153])
2025-05-28 19:42:33,117 | INFO | aether2 | Model output shape: torch.Size([1, 505, 153])
2025-05-28 19:42:33,258 | INFO | aether2 | Model output shape: torch.Size([1, 506, 153])
2025-05-28 19:42:33,406 | INFO | aether2 | Model output shape: torch.Size([1, 507, 153])
2025-05-28 19:42:33,554 | INFO | aether2 | Model output shape: torch.Size([1, 508, 153])
2025-05-28 19:42:33,710 | INFO | aether2 | Model output shape: torch.Size([1, 509, 153])
2025-05-28 19:42:33,855 | INFO | aether2 | Model output shape: torch.Size([1, 510, 153])
2025-05-28 19:42:34,004 | INFO | aether2 | Model output shape: torch.Size([1, 511, 153])
2025-05-28 19:42:34,146 | INFO | aether2 | Model output shape: torch.Size([1, 512, 153])
2025-05-28 19:42:34,297 | INFO | aether2 | Model output shape: torch.Size([1, 513, 153])
2025-05-28 19:42:34,442 | INFO | aether2 | Model output shape: torch.Size([1, 514, 153])
2025-05-28 19:42:34,590 | INFO | aether2 | Model output shape: torch.Size([1, 515, 153])
2025-05-28 19:42:34,738 | INFO | aether2 | Model output shape: torch.Size([1, 516, 153])
2025-05-28 19:42:34,891 | INFO | aether2 | Model output shape: torch.Size([1, 517, 153])
2025-05-28 19:42:35,040 | INFO | aether2 | Model output shape: torch.Size([1, 518, 153])
2025-05-28 19:42:35,194 | INFO | aether2 | Model output shape: torch.Size([1, 519, 153])
2025-05-28 19:42:35,342 | INFO | aether2 | Model output shape: torch.Size([1, 520, 153])
2025-05-28 19:42:35,490 | INFO | aether2 | Model output shape: torch.Size([1, 521, 153])
2025-05-28 19:42:35,645 | INFO | aether2 | Model output shape: torch.Size([1, 522, 153])
2025-05-28 19:42:35,800 | INFO | aether2 | Model output shape: torch.Size([1, 523, 153])
2025-05-28 19:42:35,953 | INFO | aether2 | Model output shape: torch.Size([1, 524, 153])
2025-05-28 19:42:36,110 | INFO | aether2 | Model output shape: torch.Size([1, 525, 153])
2025-05-28 19:42:36,266 | INFO | aether2 | Model output shape: torch.Size([1, 526, 153])
2025-05-28 19:42:36,424 | INFO | aether2 | Model output shape: torch.Size([1, 527, 153])
2025-05-28 19:42:36,582 | INFO | aether2 | Model output shape: torch.Size([1, 528, 153])
2025-05-28 19:42:36,739 | INFO | aether2 | Model output shape: torch.Size([1, 529, 153])
2025-05-28 19:42:36,893 | INFO | aether2 | Model output shape: torch.Size([1, 530, 153])
2025-05-28 19:42:37,053 | INFO | aether2 | Model output shape: torch.Size([1, 531, 153])
2025-05-28 19:42:37,208 | INFO | aether2 | Model output shape: torch.Size([1, 532, 153])
2025-05-28 19:42:37,369 | INFO | aether2 | Model output shape: torch.Size([1, 533, 153])
2025-05-28 19:42:37,531 | INFO | aether2 | Model output shape: torch.Size([1, 534, 153])
2025-05-28 19:42:37,688 | INFO | aether2 | Model output shape: torch.Size([1, 535, 153])
2025-05-28 19:42:37,845 | INFO | aether2 | Model output shape: torch.Size([1, 536, 153])
2025-05-28 19:42:38,006 | INFO | aether2 | Model output shape: torch.Size([1, 537, 153])
2025-05-28 19:42:38,168 | INFO | aether2 | Model output shape: torch.Size([1, 538, 153])
2025-05-28 19:42:38,328 | INFO | aether2 | Model output shape: torch.Size([1, 539, 153])
2025-05-28 19:42:38,491 | INFO | aether2 | Model output shape: torch.Size([1, 540, 153])
2025-05-28 19:42:38,665 | INFO | aether2 | Model output shape: torch.Size([1, 541, 153])
2025-05-28 19:42:38,831 | INFO | aether2 | Model output shape: torch.Size([1, 542, 153])
2025-05-28 19:42:38,995 | INFO | aether2 | Model output shape: torch.Size([1, 543, 153])
2025-05-28 19:42:39,153 | INFO | aether2 | Model output shape: torch.Size([1, 544, 153])
2025-05-28 19:42:39,319 | INFO | aether2 | Model output shape: torch.Size([1, 545, 153])
2025-05-28 19:42:39,479 | INFO | aether2 | Model output shape: torch.Size([1, 546, 153])
2025-05-28 19:42:39,647 | INFO | aether2 | Model output shape: torch.Size([1, 547, 153])
2025-05-28 19:42:39,812 | INFO | aether2 | Model output shape: torch.Size([1, 548, 153])
2025-05-28 19:42:39,982 | INFO | aether2 | Model output shape: torch.Size([1, 549, 153])
2025-05-28 19:42:40,149 | INFO | aether2 | Model output shape: torch.Size([1, 550, 153])
2025-05-28 19:42:40,318 | INFO | aether2 | Model output shape: torch.Size([1, 551, 153])
2025-05-28 19:42:40,481 | INFO | aether2 | Model output shape: torch.Size([1, 552, 153])
2025-05-28 19:42:40,651 | INFO | aether2 | Model output shape: torch.Size([1, 553, 153])
2025-05-28 19:42:40,825 | INFO | aether2 | Model output shape: torch.Size([1, 554, 153])
2025-05-28 19:42:40,999 | INFO | aether2 | Model output shape: torch.Size([1, 555, 153])
2025-05-28 19:42:41,169 | INFO | aether2 | Model output shape: torch.Size([1, 556, 153])
2025-05-28 19:42:41,337 | INFO | aether2 | Model output shape: torch.Size([1, 557, 153])
2025-05-28 19:42:41,509 | INFO | aether2 | Model output shape: torch.Size([1, 558, 153])
2025-05-28 19:42:41,681 | INFO | aether2 | Model output shape: torch.Size([1, 559, 153])
2025-05-28 19:42:41,847 | INFO | aether2 | Model output shape: torch.Size([1, 560, 153])
2025-05-28 19:42:42,020 | INFO | aether2 | Model output shape: torch.Size([1, 561, 153])
2025-05-28 19:42:42,203 | INFO | aether2 | Model output shape: torch.Size([1, 562, 153])
2025-05-28 19:42:42,380 | INFO | aether2 | Model output shape: torch.Size([1, 563, 153])
2025-05-28 19:42:42,552 | INFO | aether2 | Model output shape: torch.Size([1, 564, 153])
2025-05-28 19:42:42,733 | INFO | aether2 | Model output shape: torch.Size([1, 565, 153])
2025-05-28 19:42:42,911 | INFO | aether2 | Model output shape: torch.Size([1, 566, 153])
2025-05-28 19:42:43,090 | INFO | aether2 | Model output shape: torch.Size([1, 567, 153])
2025-05-28 19:42:43,265 | INFO | aether2 | Model output shape: torch.Size([1, 568, 153])
2025-05-28 19:42:43,481 | INFO | aether2 | Model output shape: torch.Size([1, 569, 153])
2025-05-28 19:42:43,675 | INFO | aether2 | Model output shape: torch.Size([1, 570, 153])
2025-05-28 19:42:43,849 | INFO | aether2 | Model output shape: torch.Size([1, 571, 153])
2025-05-28 19:42:44,028 | INFO | aether2 | Model output shape: torch.Size([1, 572, 153])
2025-05-28 19:42:44,212 | INFO | aether2 | Model output shape: torch.Size([1, 573, 153])
2025-05-28 19:42:44,395 | INFO | aether2 | Model output shape: torch.Size([1, 574, 153])
2025-05-28 19:42:44,578 | INFO | aether2 | Model output shape: torch.Size([1, 575, 153])
2025-05-28 19:42:44,755 | INFO | aether2 | Model output shape: torch.Size([1, 576, 153])
2025-05-28 19:42:44,944 | INFO | aether2 | Model output shape: torch.Size([1, 577, 153])
2025-05-28 19:42:45,131 | INFO | aether2 | Model output shape: torch.Size([1, 578, 153])
2025-05-28 19:42:45,316 | INFO | aether2 | Model output shape: torch.Size([1, 579, 153])
2025-05-28 19:42:45,499 | INFO | aether2 | Model output shape: torch.Size([1, 580, 153])
2025-05-28 19:42:45,684 | INFO | aether2 | Model output shape: torch.Size([1, 581, 153])
2025-05-28 19:42:45,880 | INFO | aether2 | Model output shape: torch.Size([1, 582, 153])
2025-05-28 19:42:46,066 | INFO | aether2 | Model output shape: torch.Size([1, 583, 153])
2025-05-28 19:42:46,249 | INFO | aether2 | Model output shape: torch.Size([1, 584, 153])
2025-05-28 19:42:46,441 | INFO | aether2 | Model output shape: torch.Size([1, 585, 153])
2025-05-28 19:42:46,628 | INFO | aether2 | Model output shape: torch.Size([1, 586, 153])
2025-05-28 19:42:46,823 | INFO | aether2 | Model output shape: torch.Size([1, 587, 153])
2025-05-28 19:42:47,013 | INFO | aether2 | Model output shape: torch.Size([1, 588, 153])
2025-05-28 19:42:47,208 | INFO | aether2 | Model output shape: torch.Size([1, 589, 153])
2025-05-28 19:42:47,401 | INFO | aether2 | Model output shape: torch.Size([1, 590, 153])
2025-05-28 19:42:47,590 | INFO | aether2 | Model output shape: torch.Size([1, 591, 153])
2025-05-28 19:42:47,779 | INFO | aether2 | Model output shape: torch.Size([1, 592, 153])
2025-05-28 19:42:47,968 | INFO | aether2 | Model output shape: torch.Size([1, 593, 153])
2025-05-28 19:42:48,160 | INFO | aether2 | Model output shape: torch.Size([1, 594, 153])
2025-05-28 19:42:48,353 | INFO | aether2 | Model output shape: torch.Size([1, 595, 153])
2025-05-28 19:42:48,554 | INFO | aether2 | Model output shape: torch.Size([1, 596, 153])
2025-05-28 19:42:48,763 | INFO | aether2 | Model output shape: torch.Size([1, 597, 153])
2025-05-28 19:42:48,954 | INFO | aether2 | Model output shape: torch.Size([1, 598, 153])
2025-05-28 19:42:49,150 | INFO | aether2 | Model output shape: torch.Size([1, 599, 153])
2025-05-28 19:42:49,341 | INFO | aether2 | Model output shape: torch.Size([1, 600, 153])
2025-05-28 19:42:49,538 | INFO | aether2 | Model output shape: torch.Size([1, 601, 153])
2025-05-28 19:42:49,746 | INFO | aether2 | Model output shape: torch.Size([1, 602, 153])
2025-05-28 19:42:49,946 | INFO | aether2 | Model output shape: torch.Size([1, 603, 153])
2025-05-28 19:42:50,141 | INFO | aether2 | Model output shape: torch.Size([1, 604, 153])
2025-05-28 19:42:50,345 | INFO | aether2 | Model output shape: torch.Size([1, 605, 153])
2025-05-28 19:42:50,549 | INFO | aether2 | Model output shape: torch.Size([1, 606, 153])
2025-05-28 19:42:50,754 | INFO | aether2 | Model output shape: torch.Size([1, 607, 153])
2025-05-28 19:42:50,948 | INFO | aether2 | Model output shape: torch.Size([1, 608, 153])
2025-05-28 19:42:51,154 | INFO | aether2 | Model output shape: torch.Size([1, 609, 153])
2025-05-28 19:42:51,357 | INFO | aether2 | Model output shape: torch.Size([1, 610, 153])
2025-05-28 19:42:51,559 | INFO | aether2 | Model output shape: torch.Size([1, 611, 153])
2025-05-28 19:42:51,762 | INFO | aether2 | Model output shape: torch.Size([1, 612, 153])
2025-05-28 19:42:51,997 | INFO | aether2 | Model output shape: torch.Size([1, 613, 153])
2025-05-28 19:42:52,232 | INFO | aether2 | Model output shape: torch.Size([1, 614, 153])
2025-05-28 19:42:52,493 | INFO | aether2 | Model output shape: torch.Size([1, 615, 153])
2025-05-28 19:42:52,699 | INFO | aether2 | Model output shape: torch.Size([1, 616, 153])
2025-05-28 19:42:52,915 | INFO | aether2 | Model output shape: torch.Size([1, 617, 153])
2025-05-28 19:42:53,123 | INFO | aether2 | Model output shape: torch.Size([1, 618, 153])
2025-05-28 19:42:53,337 | INFO | aether2 | Model output shape: torch.Size([1, 619, 153])
2025-05-28 19:42:53,600 | INFO | aether2 | Model output shape: torch.Size([1, 620, 153])
2025-05-28 19:42:53,863 | INFO | aether2 | Model output shape: torch.Size([1, 621, 153])
2025-05-28 19:42:54,120 | INFO | aether2 | Model output shape: torch.Size([1, 622, 153])
2025-05-28 19:42:54,418 | INFO | aether2 | Model output shape: torch.Size([1, 623, 153])
2025-05-28 19:42:54,696 | INFO | aether2 | Model output shape: torch.Size([1, 624, 153])
2025-05-28 19:42:55,020 | INFO | aether2 | Model output shape: torch.Size([1, 625, 153])
2025-05-28 19:42:55,474 | INFO | aether2 | Model output shape: torch.Size([1, 626, 153])
2025-05-28 19:42:55,777 | INFO | aether2 | Model output shape: torch.Size([1, 627, 153])
2025-05-28 19:42:56,042 | INFO | aether2 | Model output shape: torch.Size([1, 628, 153])
2025-05-28 19:42:56,374 | INFO | aether2 | Model output shape: torch.Size([1, 629, 153])
2025-05-28 19:42:56,624 | INFO | aether2 | Model output shape: torch.Size([1, 630, 153])
2025-05-28 19:42:56,894 | INFO | aether2 | Model output shape: torch.Size([1, 631, 153])
2025-05-28 19:42:57,160 | INFO | aether2 | Model output shape: torch.Size([1, 632, 153])
2025-05-28 19:42:57,405 | INFO | aether2 | Model output shape: torch.Size([1, 633, 153])
2025-05-28 19:42:57,662 | INFO | aether2 | Model output shape: torch.Size([1, 634, 153])
2025-05-28 19:42:57,930 | INFO | aether2 | Model output shape: torch.Size([1, 635, 153])
2025-05-28 19:42:58,186 | INFO | aether2 | Model output shape: torch.Size([1, 636, 153])
2025-05-28 19:42:58,437 | INFO | aether2 | Model output shape: torch.Size([1, 637, 153])
2025-05-28 19:42:58,711 | INFO | aether2 | Model output shape: torch.Size([1, 638, 153])
2025-05-28 19:42:58,973 | INFO | aether2 | Model output shape: torch.Size([1, 639, 153])
2025-05-28 19:42:59,224 | INFO | aether2 | Model output shape: torch.Size([1, 640, 153])
2025-05-28 19:42:59,503 | INFO | aether2 | Model output shape: torch.Size([1, 641, 153])
2025-05-28 19:42:59,764 | INFO | aether2 | Model output shape: torch.Size([1, 642, 153])
2025-05-28 19:43:00,024 | INFO | aether2 | Model output shape: torch.Size([1, 643, 153])
2025-05-28 19:43:00,287 | INFO | aether2 | Model output shape: torch.Size([1, 644, 153])
2025-05-28 19:43:00,586 | INFO | aether2 | Model output shape: torch.Size([1, 645, 153])
2025-05-28 19:43:00,843 | INFO | aether2 | Model output shape: torch.Size([1, 646, 153])
2025-05-28 19:43:01,092 | INFO | aether2 | Model output shape: torch.Size([1, 647, 153])
2025-05-28 19:43:01,325 | INFO | aether2 | Model output shape: torch.Size([1, 648, 153])
2025-05-28 19:43:01,568 | INFO | aether2 | Model output shape: torch.Size([1, 649, 153])
2025-05-28 19:43:01,812 | INFO | aether2 | Model output shape: torch.Size([1, 650, 153])
2025-05-28 19:43:02,053 | INFO | aether2 | Model output shape: torch.Size([1, 651, 153])
2025-05-28 19:43:02,293 | INFO | aether2 | Model output shape: torch.Size([1, 652, 153])
2025-05-28 19:43:02,544 | INFO | aether2 | Model output shape: torch.Size([1, 653, 153])
2025-05-28 19:43:02,800 | INFO | aether2 | Model output shape: torch.Size([1, 654, 153])
2025-05-28 19:43:03,052 | INFO | aether2 | Model output shape: torch.Size([1, 655, 153])
2025-05-28 19:43:03,292 | INFO | aether2 | Model output shape: torch.Size([1, 656, 153])
2025-05-28 19:43:03,547 | INFO | aether2 | Model output shape: torch.Size([1, 657, 153])
2025-05-28 19:43:03,826 | INFO | aether2 | Model output shape: torch.Size([1, 658, 153])
2025-05-28 19:43:04,085 | INFO | aether2 | Model output shape: torch.Size([1, 659, 153])
2025-05-28 19:43:04,346 | INFO | aether2 | Model output shape: torch.Size([1, 660, 153])
2025-05-28 19:43:04,601 | INFO | aether2 | Model output shape: torch.Size([1, 661, 153])
2025-05-28 19:43:04,856 | INFO | aether2 | Model output shape: torch.Size([1, 662, 153])
2025-05-28 19:43:05,120 | INFO | aether2 | Model output shape: torch.Size([1, 663, 153])
2025-05-28 19:43:05,366 | INFO | aether2 | Model output shape: torch.Size([1, 664, 153])
2025-05-28 19:43:05,635 | INFO | aether2 | Model output shape: torch.Size([1, 665, 153])
2025-05-28 19:43:05,893 | INFO | aether2 | Model output shape: torch.Size([1, 666, 153])
2025-05-28 19:43:06,152 | INFO | aether2 | Model output shape: torch.Size([1, 667, 153])
2025-05-28 19:43:06,402 | INFO | aether2 | Model output shape: torch.Size([1, 668, 153])
2025-05-28 19:43:06,667 | INFO | aether2 | Model output shape: torch.Size([1, 669, 153])
2025-05-28 19:43:06,926 | INFO | aether2 | Model output shape: torch.Size([1, 670, 153])
2025-05-28 19:43:07,189 | INFO | aether2 | Model output shape: torch.Size([1, 671, 153])
2025-05-28 19:43:07,435 | INFO | aether2 | Model output shape: torch.Size([1, 672, 153])
2025-05-28 19:43:07,696 | INFO | aether2 | Model output shape: torch.Size([1, 673, 153])
2025-05-28 19:43:07,960 | INFO | aether2 | Model output shape: torch.Size([1, 674, 153])
2025-05-28 19:43:08,224 | INFO | aether2 | Model output shape: torch.Size([1, 675, 153])
2025-05-28 19:43:08,483 | INFO | aether2 | Model output shape: torch.Size([1, 676, 153])
2025-05-28 19:43:08,764 | INFO | aether2 | Model output shape: torch.Size([1, 677, 153])
2025-05-28 19:43:09,028 | INFO | aether2 | Model output shape: torch.Size([1, 678, 153])
2025-05-28 19:43:09,290 | INFO | aether2 | Model output shape: torch.Size([1, 679, 153])
2025-05-28 19:43:09,545 | INFO | aether2 | Model output shape: torch.Size([1, 680, 153])
2025-05-28 19:43:09,812 | INFO | aether2 | Model output shape: torch.Size([1, 681, 153])
2025-05-28 19:43:10,078 | INFO | aether2 | Model output shape: torch.Size([1, 682, 153])
2025-05-28 19:43:10,349 | INFO | aether2 | Model output shape: torch.Size([1, 683, 153])
2025-05-28 19:43:10,612 | INFO | aether2 | Model output shape: torch.Size([1, 684, 153])
2025-05-28 19:43:10,883 | INFO | aether2 | Model output shape: torch.Size([1, 685, 153])
2025-05-28 19:43:11,157 | INFO | aether2 | Model output shape: torch.Size([1, 686, 153])
2025-05-28 19:43:11,429 | INFO | aether2 | Model output shape: torch.Size([1, 687, 153])
2025-05-28 19:43:11,686 | INFO | aether2 | Model output shape: torch.Size([1, 688, 153])
2025-05-28 19:43:11,960 | INFO | aether2 | Model output shape: torch.Size([1, 689, 153])
2025-05-28 19:43:12,234 | INFO | aether2 | Model output shape: torch.Size([1, 690, 153])
2025-05-28 19:43:12,518 | INFO | aether2 | Model output shape: torch.Size([1, 691, 153])
2025-05-28 19:43:12,789 | INFO | aether2 | Model output shape: torch.Size([1, 692, 153])
2025-05-28 19:43:13,074 | INFO | aether2 | Model output shape: torch.Size([1, 693, 153])
2025-05-28 19:43:13,348 | INFO | aether2 | Model output shape: torch.Size([1, 694, 153])
2025-05-28 19:43:13,637 | INFO | aether2 | Model output shape: torch.Size([1, 695, 153])
2025-05-28 19:43:13,904 | INFO | aether2 | Model output shape: torch.Size([1, 696, 153])
2025-05-28 19:43:14,181 | INFO | aether2 | Model output shape: torch.Size([1, 697, 153])
2025-05-28 19:43:14,463 | INFO | aether2 | Model output shape: torch.Size([1, 698, 153])
2025-05-28 19:43:14,740 | INFO | aether2 | Model output shape: torch.Size([1, 699, 153])
2025-05-28 19:43:15,017 | INFO | aether2 | Model output shape: torch.Size([1, 700, 153])
2025-05-28 19:43:15,333 | INFO | aether2 | Model output shape: torch.Size([1, 701, 153])
2025-05-28 19:43:15,633 | INFO | aether2 | Model output shape: torch.Size([1, 702, 153])
2025-05-28 19:43:15,956 | INFO | aether2 | Model output shape: torch.Size([1, 703, 153])
2025-05-28 19:43:16,248 | INFO | aether2 | Model output shape: torch.Size([1, 704, 153])
2025-05-28 19:43:16,616 | INFO | aether2 | Model output shape: torch.Size([1, 705, 153])
2025-05-28 19:43:16,897 | INFO | aether2 | Model output shape: torch.Size([1, 706, 153])
2025-05-28 19:43:17,231 | INFO | aether2 | Model output shape: torch.Size([1, 707, 153])
2025-05-28 19:43:17,518 | INFO | aether2 | Model output shape: torch.Size([1, 708, 153])
2025-05-28 19:43:17,820 | INFO | aether2 | Model output shape: torch.Size([1, 709, 153])
2025-05-28 19:43:18,161 | INFO | aether2 | Model output shape: torch.Size([1, 710, 153])
2025-05-28 19:43:18,469 | INFO | aether2 | Model output shape: torch.Size([1, 711, 153])
2025-05-28 19:43:18,790 | INFO | aether2 | Model output shape: torch.Size([1, 712, 153])
2025-05-28 19:43:19,148 | INFO | aether2 | Model output shape: torch.Size([1, 713, 153])
2025-05-28 19:43:19,448 | INFO | aether2 | Model output shape: torch.Size([1, 714, 153])
2025-05-28 19:43:19,760 | INFO | aether2 | Model output shape: torch.Size([1, 715, 153])
2025-05-28 19:43:20,107 | INFO | aether2 | Model output shape: torch.Size([1, 716, 153])
2025-05-28 19:43:20,418 | INFO | aether2 | Model output shape: torch.Size([1, 717, 153])
2025-05-28 19:43:20,727 | INFO | aether2 | Model output shape: torch.Size([1, 718, 153])
2025-05-28 19:43:21,092 | INFO | aether2 | Model output shape: torch.Size([1, 719, 153])
2025-05-28 19:43:21,383 | INFO | aether2 | Model output shape: torch.Size([1, 720, 153])
2025-05-28 19:43:21,703 | INFO | aether2 | Model output shape: torch.Size([1, 721, 153])
2025-05-28 19:43:22,068 | INFO | aether2 | Model output shape: torch.Size([1, 722, 153])
2025-05-28 19:43:22,392 | INFO | aether2 | Model output shape: torch.Size([1, 723, 153])
2025-05-28 19:43:22,749 | INFO | aether2 | Model output shape: torch.Size([1, 724, 153])
2025-05-28 19:43:23,079 | INFO | aether2 | Model output shape: torch.Size([1, 725, 153])
2025-05-28 19:43:23,394 | INFO | aether2 | Model output shape: torch.Size([1, 726, 153])
2025-05-28 19:43:23,714 | INFO | aether2 | Model output shape: torch.Size([1, 727, 153])
2025-05-28 19:43:23,996 | INFO | aether2 | Model output shape: torch.Size([1, 728, 153])
2025-05-28 19:43:24,289 | INFO | aether2 | Model output shape: torch.Size([1, 729, 153])
2025-05-28 19:43:24,595 | INFO | aether2 | Model output shape: torch.Size([1, 730, 153])
2025-05-28 19:43:24,896 | INFO | aether2 | Model output shape: torch.Size([1, 731, 153])
2025-05-28 19:43:25,197 | INFO | aether2 | Model output shape: torch.Size([1, 732, 153])
2025-05-28 19:43:25,501 | INFO | aether2 | Model output shape: torch.Size([1, 733, 153])
2025-05-28 19:43:25,808 | INFO | aether2 | Model output shape: torch.Size([1, 734, 153])
2025-05-28 19:43:26,109 | INFO | aether2 | Model output shape: torch.Size([1, 735, 153])
2025-05-28 19:43:26,392 | INFO | aether2 | Model output shape: torch.Size([1, 736, 153])
2025-05-28 19:43:26,697 | INFO | aether2 | Model output shape: torch.Size([1, 737, 153])
2025-05-28 19:43:27,022 | INFO | aether2 | Model output shape: torch.Size([1, 738, 153])
2025-05-28 19:43:27,339 | INFO | aether2 | Model output shape: torch.Size([1, 739, 153])
2025-05-28 19:43:27,655 | INFO | aether2 | Model output shape: torch.Size([1, 740, 153])
2025-05-28 19:43:27,978 | INFO | aether2 | Model output shape: torch.Size([1, 741, 153])
2025-05-28 19:43:28,307 | INFO | aether2 | Model output shape: torch.Size([1, 742, 153])
2025-05-28 19:43:28,641 | INFO | aether2 | Model output shape: torch.Size([1, 743, 153])
2025-05-28 19:43:28,956 | INFO | aether2 | Model output shape: torch.Size([1, 744, 153])
2025-05-28 19:43:29,291 | INFO | aether2 | Model output shape: torch.Size([1, 745, 153])
2025-05-28 19:43:29,608 | INFO | aether2 | Model output shape: torch.Size([1, 746, 153])
2025-05-28 19:43:29,926 | INFO | aether2 | Model output shape: torch.Size([1, 747, 153])
2025-05-28 19:43:30,239 | INFO | aether2 | Model output shape: torch.Size([1, 748, 153])
2025-05-28 19:43:30,574 | INFO | aether2 | Model output shape: torch.Size([1, 749, 153])
2025-05-28 19:43:30,891 | INFO | aether2 | Model output shape: torch.Size([1, 750, 153])
2025-05-28 19:43:31,220 | INFO | aether2 | Model output shape: torch.Size([1, 751, 153])
2025-05-28 19:43:31,526 | INFO | aether2 | Model output shape: torch.Size([1, 752, 153])
2025-05-28 19:43:31,863 | INFO | aether2 | Model output shape: torch.Size([1, 753, 153])
2025-05-28 19:43:32,184 | INFO | aether2 | Model output shape: torch.Size([1, 754, 153])
2025-05-28 19:43:32,513 | INFO | aether2 | Model output shape: torch.Size([1, 755, 153])
2025-05-28 19:43:32,832 | INFO | aether2 | Model output shape: torch.Size([1, 756, 153])
2025-05-28 19:43:33,152 | INFO | aether2 | Model output shape: torch.Size([1, 757, 153])
2025-05-28 19:43:33,491 | INFO | aether2 | Model output shape: torch.Size([1, 758, 153])
2025-05-28 19:43:33,849 | INFO | aether2 | Model output shape: torch.Size([1, 759, 153])
2025-05-28 19:43:34,156 | INFO | aether2 | Model output shape: torch.Size([1, 760, 153])
2025-05-28 19:43:34,484 | INFO | aether2 | Model output shape: torch.Size([1, 761, 153])
2025-05-28 19:43:34,813 | INFO | aether2 | Model output shape: torch.Size([1, 762, 153])
2025-05-28 19:43:35,144 | INFO | aether2 | Model output shape: torch.Size([1, 763, 153])
2025-05-28 19:43:35,483 | INFO | aether2 | Model output shape: torch.Size([1, 764, 153])
2025-05-28 19:43:35,828 | INFO | aether2 | Model output shape: torch.Size([1, 765, 153])
2025-05-28 19:43:36,164 | INFO | aether2 | Model output shape: torch.Size([1, 766, 153])
2025-05-28 19:43:36,496 | INFO | aether2 | Model output shape: torch.Size([1, 767, 153])
2025-05-28 19:43:36,805 | INFO | aether2 | Model output shape: torch.Size([1, 768, 153])
2025-05-28 19:43:37,156 | INFO | aether2 | Model output shape: torch.Size([1, 769, 153])
2025-05-28 19:43:37,497 | INFO | aether2 | Model output shape: torch.Size([1, 770, 153])
2025-05-28 19:43:37,857 | INFO | aether2 | Model output shape: torch.Size([1, 771, 153])
2025-05-28 19:43:38,178 | INFO | aether2 | Model output shape: torch.Size([1, 772, 153])
2025-05-28 19:43:38,519 | INFO | aether2 | Model output shape: torch.Size([1, 773, 153])
2025-05-28 19:43:38,892 | INFO | aether2 | Model output shape: torch.Size([1, 774, 153])
2025-05-28 19:43:39,240 | INFO | aether2 | Model output shape: torch.Size([1, 775, 153])
2025-05-28 19:43:39,554 | INFO | aether2 | Model output shape: torch.Size([1, 776, 153])
2025-05-28 19:43:39,889 | INFO | aether2 | Model output shape: torch.Size([1, 777, 153])
2025-05-28 19:43:40,229 | INFO | aether2 | Model output shape: torch.Size([1, 778, 153])
2025-05-28 19:43:40,586 | INFO | aether2 | Model output shape: torch.Size([1, 779, 153])
2025-05-28 19:43:40,934 | INFO | aether2 | Model output shape: torch.Size([1, 780, 153])
2025-05-28 19:43:41,277 | INFO | aether2 | Model output shape: torch.Size([1, 781, 153])
2025-05-28 19:43:41,613 | INFO | aether2 | Model output shape: torch.Size([1, 782, 153])
2025-05-28 19:43:41,955 | INFO | aether2 | Model output shape: torch.Size([1, 783, 153])
2025-05-28 19:43:42,293 | INFO | aether2 | Model output shape: torch.Size([1, 784, 153])
2025-05-28 19:43:42,639 | INFO | aether2 | Model output shape: torch.Size([1, 785, 153])
2025-05-28 19:43:43,049 | INFO | aether2 | Model output shape: torch.Size([1, 786, 153])
2025-05-28 19:43:43,490 | INFO | aether2 | Model output shape: torch.Size([1, 787, 153])
2025-05-28 19:43:43,894 | INFO | aether2 | Model output shape: torch.Size([1, 788, 153])
2025-05-28 19:43:44,242 | INFO | aether2 | Model output shape: torch.Size([1, 789, 153])
2025-05-28 19:43:44,588 | INFO | aether2 | Model output shape: torch.Size([1, 790, 153])
2025-05-28 19:43:44,935 | INFO | aether2 | Model output shape: torch.Size([1, 791, 153])
2025-05-28 19:43:45,260 | INFO | aether2 | Model output shape: torch.Size([1, 792, 153])
2025-05-28 19:43:45,617 | INFO | aether2 | Model output shape: torch.Size([1, 793, 153])
2025-05-28 19:43:45,968 | INFO | aether2 | Model output shape: torch.Size([1, 794, 153])
2025-05-28 19:43:46,320 | INFO | aether2 | Model output shape: torch.Size([1, 795, 153])
2025-05-28 19:43:46,666 | INFO | aether2 | Model output shape: torch.Size([1, 796, 153])
2025-05-28 19:43:47,016 | INFO | aether2 | Model output shape: torch.Size([1, 797, 153])
2025-05-28 19:43:47,369 | INFO | aether2 | Model output shape: torch.Size([1, 798, 153])
2025-05-28 19:43:47,720 | INFO | aether2 | Model output shape: torch.Size([1, 799, 153])
2025-05-28 19:43:48,056 | INFO | aether2 | Model output shape: torch.Size([1, 800, 153])
2025-05-28 19:43:48,413 | INFO | aether2 | Model output shape: torch.Size([1, 801, 153])
2025-05-28 19:43:48,790 | INFO | aether2 | Model output shape: torch.Size([1, 802, 153])
2025-05-28 19:43:49,174 | INFO | aether2 | Model output shape: torch.Size([1, 803, 153])
2025-05-28 19:43:49,565 | INFO | aether2 | Model output shape: torch.Size([1, 804, 153])
2025-05-28 19:43:49,946 | INFO | aether2 | Model output shape: torch.Size([1, 805, 153])
2025-05-28 19:43:50,367 | INFO | aether2 | Model output shape: torch.Size([1, 806, 153])
2025-05-28 19:43:50,746 | INFO | aether2 | Model output shape: torch.Size([1, 807, 153])
2025-05-28 19:43:51,093 | INFO | aether2 | Model output shape: torch.Size([1, 808, 153])
2025-05-28 19:43:51,501 | INFO | aether2 | Model output shape: torch.Size([1, 809, 153])
2025-05-28 19:43:51,895 | INFO | aether2 | Model output shape: torch.Size([1, 810, 153])
2025-05-28 19:43:52,293 | INFO | aether2 | Model output shape: torch.Size([1, 811, 153])
2025-05-28 19:43:52,675 | INFO | aether2 | Model output shape: torch.Size([1, 812, 153])
2025-05-28 19:43:53,059 | INFO | aether2 | Model output shape: torch.Size([1, 813, 153])
2025-05-28 19:43:53,428 | INFO | aether2 | Model output shape: torch.Size([1, 814, 153])
2025-05-28 19:43:53,833 | INFO | aether2 | Model output shape: torch.Size([1, 815, 153])
2025-05-28 19:43:54,209 | INFO | aether2 | Model output shape: torch.Size([1, 816, 153])
2025-05-28 19:43:54,607 | INFO | aether2 | Model output shape: torch.Size([1, 817, 153])
2025-05-28 19:43:55,024 | INFO | aether2 | Model output shape: torch.Size([1, 818, 153])
2025-05-28 19:43:55,432 | INFO | aether2 | Model output shape: torch.Size([1, 819, 153])
2025-05-28 19:43:55,809 | INFO | aether2 | Model output shape: torch.Size([1, 820, 153])
2025-05-28 19:43:56,201 | INFO | aether2 | Model output shape: torch.Size([1, 821, 153])
2025-05-28 19:43:56,596 | INFO | aether2 | Model output shape: torch.Size([1, 822, 153])
2025-05-28 19:43:57,010 | INFO | aether2 | Model output shape: torch.Size([1, 823, 153])
2025-05-28 19:43:57,396 | INFO | aether2 | Model output shape: torch.Size([1, 824, 153])
2025-05-28 19:43:57,802 | INFO | aether2 | Model output shape: torch.Size([1, 825, 153])
2025-05-28 19:43:58,215 | INFO | aether2 | Model output shape: torch.Size([1, 826, 153])
2025-05-28 19:43:58,783 | INFO | aether2 | Model output shape: torch.Size([1, 827, 153])
2025-05-28 19:43:59,205 | INFO | aether2 | Model output shape: torch.Size([1, 828, 153])
2025-05-28 19:43:59,684 | INFO | aether2 | Model output shape: torch.Size([1, 829, 153])
2025-05-28 19:44:00,104 | INFO | aether2 | Model output shape: torch.Size([1, 830, 153])
2025-05-28 19:44:00,513 | INFO | aether2 | Model output shape: torch.Size([1, 831, 153])
2025-05-28 19:44:00,979 | INFO | aether2 | Model output shape: torch.Size([1, 832, 153])
2025-05-28 19:44:01,392 | INFO | aether2 | Model output shape: torch.Size([1, 833, 153])
2025-05-28 19:44:01,795 | INFO | aether2 | Model output shape: torch.Size([1, 834, 153])
2025-05-28 19:44:02,206 | INFO | aether2 | Model output shape: torch.Size([1, 835, 153])
2025-05-28 19:44:02,602 | INFO | aether2 | Model output shape: torch.Size([1, 836, 153])
2025-05-28 19:44:03,009 | INFO | aether2 | Model output shape: torch.Size([1, 837, 153])
2025-05-28 19:44:03,412 | INFO | aether2 | Model output shape: torch.Size([1, 838, 153])
2025-05-28 19:44:03,832 | INFO | aether2 | Model output shape: torch.Size([1, 839, 153])
2025-05-28 19:44:04,239 | INFO | aether2 | Model output shape: torch.Size([1, 840, 153])
2025-05-28 19:44:04,663 | INFO | aether2 | Model output shape: torch.Size([1, 841, 153])
2025-05-28 19:44:05,066 | INFO | aether2 | Model output shape: torch.Size([1, 842, 153])
2025-05-28 19:44:05,448 | INFO | aether2 | Model output shape: torch.Size([1, 843, 153])
2025-05-28 19:44:05,820 | INFO | aether2 | Model output shape: torch.Size([1, 844, 153])
2025-05-28 19:44:06,202 | INFO | aether2 | Model output shape: torch.Size([1, 845, 153])
2025-05-28 19:44:06,587 | INFO | aether2 | Model output shape: torch.Size([1, 846, 153])
2025-05-28 19:44:06,989 | INFO | aether2 | Model output shape: torch.Size([1, 847, 153])
2025-05-28 19:44:07,373 | INFO | aether2 | Model output shape: torch.Size([1, 848, 153])
2025-05-28 19:44:07,765 | INFO | aether2 | Model output shape: torch.Size([1, 849, 153])
2025-05-28 19:44:08,238 | INFO | aether2 | Model output shape: torch.Size([1, 850, 153])
2025-05-28 19:44:08,683 | INFO | aether2 | Model output shape: torch.Size([1, 851, 153])
2025-05-28 19:44:09,099 | INFO | aether2 | Model output shape: torch.Size([1, 852, 153])
2025-05-28 19:44:09,499 | INFO | aether2 | Model output shape: torch.Size([1, 853, 153])
2025-05-28 19:44:09,913 | INFO | aether2 | Model output shape: torch.Size([1, 854, 153])
2025-05-28 19:44:10,324 | INFO | aether2 | Model output shape: torch.Size([1, 855, 153])
2025-05-28 19:44:10,680 | INFO | aether2 | Model output shape: torch.Size([1, 856, 153])
2025-05-28 19:44:11,096 | INFO | aether2 | Model output shape: torch.Size([1, 857, 153])
2025-05-28 19:44:11,565 | INFO | aether2 | Model output shape: torch.Size([1, 858, 153])
2025-05-28 19:44:11,978 | INFO | aether2 | Model output shape: torch.Size([1, 859, 153])
2025-05-28 19:44:12,376 | INFO | aether2 | Model output shape: torch.Size([1, 860, 153])
2025-05-28 19:44:12,805 | INFO | aether2 | Model output shape: torch.Size([1, 861, 153])
2025-05-28 19:44:13,219 | INFO | aether2 | Model output shape: torch.Size([1, 862, 153])
2025-05-28 19:44:13,635 | INFO | aether2 | Model output shape: torch.Size([1, 863, 153])
2025-05-28 19:44:14,036 | INFO | aether2 | Model output shape: torch.Size([1, 864, 153])
2025-05-28 19:44:14,450 | INFO | aether2 | Model output shape: torch.Size([1, 865, 153])
2025-05-28 19:44:14,874 | INFO | aether2 | Model output shape: torch.Size([1, 866, 153])
2025-05-28 19:44:15,281 | INFO | aether2 | Model output shape: torch.Size([1, 867, 153])
2025-05-28 19:44:15,684 | INFO | aether2 | Model output shape: torch.Size([1, 868, 153])
2025-05-28 19:44:16,106 | INFO | aether2 | Model output shape: torch.Size([1, 869, 153])
2025-05-28 19:44:16,531 | INFO | aether2 | Model output shape: torch.Size([1, 870, 153])
2025-05-28 19:44:16,952 | INFO | aether2 | Model output shape: torch.Size([1, 871, 153])
2025-05-28 19:44:17,329 | INFO | aether2 | Model output shape: torch.Size([1, 872, 153])
2025-05-28 19:44:17,737 | INFO | aether2 | Model output shape: torch.Size([1, 873, 153])
2025-05-28 19:44:18,151 | INFO | aether2 | Model output shape: torch.Size([1, 874, 153])
2025-05-28 19:44:18,568 | INFO | aether2 | Model output shape: torch.Size([1, 875, 153])
2025-05-28 19:44:18,995 | INFO | aether2 | Model output shape: torch.Size([1, 876, 153])
2025-05-28 19:44:19,435 | INFO | aether2 | Model output shape: torch.Size([1, 877, 153])
2025-05-28 19:44:19,864 | INFO | aether2 | Model output shape: torch.Size([1, 878, 153])
2025-05-28 19:44:20,276 | INFO | aether2 | Model output shape: torch.Size([1, 879, 153])
2025-05-28 19:44:20,692 | INFO | aether2 | Model output shape: torch.Size([1, 880, 153])
2025-05-28 19:44:21,134 | INFO | aether2 | Model output shape: torch.Size([1, 881, 153])
2025-05-28 19:44:21,591 | INFO | aether2 | Model output shape: torch.Size([1, 882, 153])
2025-05-28 19:44:22,010 | INFO | aether2 | Model output shape: torch.Size([1, 883, 153])
2025-05-28 19:44:22,425 | INFO | aether2 | Model output shape: torch.Size([1, 884, 153])
2025-05-28 19:44:22,868 | INFO | aether2 | Model output shape: torch.Size([1, 885, 153])
2025-05-28 19:44:23,310 | INFO | aether2 | Model output shape: torch.Size([1, 886, 153])
2025-05-28 19:44:23,759 | INFO | aether2 | Model output shape: torch.Size([1, 887, 153])
2025-05-28 19:44:24,157 | INFO | aether2 | Model output shape: torch.Size([1, 888, 153])
2025-05-28 19:44:24,593 | INFO | aether2 | Model output shape: torch.Size([1, 889, 153])
2025-05-28 19:44:25,063 | INFO | aether2 | Model output shape: torch.Size([1, 890, 153])
2025-05-28 19:44:25,523 | INFO | aether2 | Model output shape: torch.Size([1, 891, 153])
2025-05-28 19:44:26,017 | INFO | aether2 | Model output shape: torch.Size([1, 892, 153])
2025-05-28 19:44:26,460 | INFO | aether2 | Model output shape: torch.Size([1, 893, 153])
2025-05-28 19:44:26,965 | INFO | aether2 | Model output shape: torch.Size([1, 894, 153])
2025-05-28 19:44:27,449 | INFO | aether2 | Model output shape: torch.Size([1, 895, 153])
2025-05-28 19:44:27,861 | INFO | aether2 | Model output shape: torch.Size([1, 896, 153])
2025-05-28 19:44:28,424 | INFO | aether2 | Model output shape: torch.Size([1, 897, 153])
2025-05-28 19:44:29,142 | INFO | aether2 | Model output shape: torch.Size([1, 898, 153])
2025-05-28 19:44:29,716 | INFO | aether2 | Model output shape: torch.Size([1, 899, 153])
2025-05-28 19:44:30,238 | INFO | aether2 | Model output shape: torch.Size([1, 900, 153])
2025-05-28 19:44:30,676 | INFO | aether2 | Model output shape: torch.Size([1, 901, 153])
2025-05-28 19:44:31,166 | INFO | aether2 | Model output shape: torch.Size([1, 902, 153])
2025-05-28 19:44:31,638 | INFO | aether2 | Model output shape: torch.Size([1, 903, 153])
2025-05-28 19:44:32,079 | INFO | aether2 | Model output shape: torch.Size([1, 904, 153])
2025-05-28 19:44:32,637 | INFO | aether2 | Model output shape: torch.Size([1, 905, 153])
2025-05-28 19:44:33,088 | INFO | aether2 | Model output shape: torch.Size([1, 906, 153])
2025-05-28 19:44:33,586 | INFO | aether2 | Model output shape: torch.Size([1, 907, 153])
2025-05-28 19:44:34,095 | INFO | aether2 | Model output shape: torch.Size([1, 908, 153])
2025-05-28 19:44:34,538 | INFO | aether2 | Model output shape: torch.Size([1, 909, 153])
2025-05-28 19:44:34,973 | INFO | aether2 | Model output shape: torch.Size([1, 910, 153])
2025-05-28 19:44:35,408 | INFO | aether2 | Model output shape: torch.Size([1, 911, 153])
2025-05-28 19:44:35,802 | INFO | aether2 | Model output shape: torch.Size([1, 912, 153])
2025-05-28 19:44:36,234 | INFO | aether2 | Model output shape: torch.Size([1, 913, 153])
2025-05-28 19:44:36,674 | INFO | aether2 | Model output shape: torch.Size([1, 914, 153])
2025-05-28 19:44:37,136 | INFO | aether2 | Model output shape: torch.Size([1, 915, 153])
2025-05-28 19:44:37,592 | INFO | aether2 | Model output shape: torch.Size([1, 916, 153])
2025-05-28 19:44:38,074 | INFO | aether2 | Model output shape: torch.Size([1, 917, 153])
2025-05-28 19:44:38,567 | INFO | aether2 | Model output shape: torch.Size([1, 918, 153])
2025-05-28 19:44:39,131 | INFO | aether2 | Model output shape: torch.Size([1, 919, 153])
2025-05-28 19:44:39,586 | INFO | aether2 | Model output shape: torch.Size([1, 920, 153])
2025-05-28 19:44:40,103 | INFO | aether2 | Model output shape: torch.Size([1, 921, 153])
2025-05-28 19:44:40,559 | INFO | aether2 | Model output shape: torch.Size([1, 922, 153])
2025-05-28 19:44:41,026 | INFO | aether2 | Model output shape: torch.Size([1, 923, 153])
2025-05-28 19:44:41,483 | INFO | aether2 | Model output shape: torch.Size([1, 924, 153])
2025-05-28 19:44:41,955 | INFO | aether2 | Model output shape: torch.Size([1, 925, 153])
2025-05-28 19:44:42,412 | INFO | aether2 | Model output shape: torch.Size([1, 926, 153])
2025-05-28 19:44:42,912 | INFO | aether2 | Model output shape: torch.Size([1, 927, 153])
2025-05-28 19:44:43,350 | INFO | aether2 | Model output shape: torch.Size([1, 928, 153])
2025-05-28 19:44:43,822 | INFO | aether2 | Model output shape: torch.Size([1, 929, 153])
2025-05-28 19:44:44,279 | INFO | aether2 | Model output shape: torch.Size([1, 930, 153])
2025-05-28 19:44:44,733 | INFO | aether2 | Model output shape: torch.Size([1, 931, 153])
2025-05-28 19:44:45,148 | INFO | aether2 | Model output shape: torch.Size([1, 932, 153])
2025-05-28 19:44:45,600 | INFO | aether2 | Model output shape: torch.Size([1, 933, 153])
2025-05-28 19:44:46,048 | INFO | aether2 | Model output shape: torch.Size([1, 934, 153])
2025-05-28 19:44:46,500 | INFO | aether2 | Model output shape: torch.Size([1, 935, 153])
2025-05-28 19:44:46,919 | INFO | aether2 | Model output shape: torch.Size([1, 936, 153])
2025-05-28 19:44:47,370 | INFO | aether2 | Model output shape: torch.Size([1, 937, 153])
2025-05-28 19:44:47,824 | INFO | aether2 | Model output shape: torch.Size([1, 938, 153])
2025-05-28 19:44:48,282 | INFO | aether2 | Model output shape: torch.Size([1, 939, 153])
2025-05-28 19:44:48,739 | INFO | aether2 | Model output shape: torch.Size([1, 940, 153])
2025-05-28 19:44:49,211 | INFO | aether2 | Model output shape: torch.Size([1, 941, 153])
2025-05-28 19:44:49,666 | INFO | aether2 | Model output shape: torch.Size([1, 942, 153])
2025-05-28 19:44:50,126 | INFO | aether2 | Model output shape: torch.Size([1, 943, 153])
2025-05-28 19:44:50,547 | INFO | aether2 | Model output shape: torch.Size([1, 944, 153])
2025-05-28 19:44:51,016 | INFO | aether2 | Model output shape: torch.Size([1, 945, 153])
2025-05-28 19:44:51,476 | INFO | aether2 | Model output shape: torch.Size([1, 946, 153])
2025-05-28 19:44:51,940 | INFO | aether2 | Model output shape: torch.Size([1, 947, 153])
2025-05-28 19:44:52,375 | INFO | aether2 | Model output shape: torch.Size([1, 948, 153])
2025-05-28 19:44:52,845 | INFO | aether2 | Model output shape: torch.Size([1, 949, 153])
2025-05-28 19:44:53,307 | INFO | aether2 | Model output shape: torch.Size([1, 950, 153])
2025-05-28 19:44:53,782 | INFO | aether2 | Model output shape: torch.Size([1, 951, 153])
2025-05-28 19:44:54,221 | INFO | aether2 | Model output shape: torch.Size([1, 952, 153])
2025-05-28 19:44:54,690 | INFO | aether2 | Model output shape: torch.Size([1, 953, 153])
2025-05-28 19:44:55,169 | INFO | aether2 | Model output shape: torch.Size([1, 954, 153])
2025-05-28 19:44:55,644 | INFO | aether2 | Model output shape: torch.Size([1, 955, 153])
2025-05-28 19:44:56,112 | INFO | aether2 | Model output shape: torch.Size([1, 956, 153])
2025-05-28 19:44:56,578 | INFO | aether2 | Model output shape: torch.Size([1, 957, 153])
2025-05-28 19:44:57,045 | INFO | aether2 | Model output shape: torch.Size([1, 958, 153])
2025-05-28 19:44:57,540 | INFO | aether2 | Model output shape: torch.Size([1, 959, 153])
2025-05-28 19:44:58,000 | INFO | aether2 | Model output shape: torch.Size([1, 960, 153])
2025-05-28 19:44:58,496 | INFO | aether2 | Model output shape: torch.Size([1, 961, 153])
2025-05-28 19:44:59,019 | INFO | aether2 | Model output shape: torch.Size([1, 962, 153])
2025-05-28 19:44:59,530 | INFO | aether2 | Model output shape: torch.Size([1, 963, 153])
2025-05-28 19:45:00,007 | INFO | aether2 | Model output shape: torch.Size([1, 964, 153])
2025-05-28 19:45:00,490 | INFO | aether2 | Model output shape: torch.Size([1, 965, 153])
2025-05-28 19:45:00,982 | INFO | aether2 | Model output shape: torch.Size([1, 966, 153])
2025-05-28 19:45:01,505 | INFO | aether2 | Model output shape: torch.Size([1, 967, 153])
2025-05-28 19:45:01,954 | INFO | aether2 | Model output shape: torch.Size([1, 968, 153])
2025-05-28 19:45:02,441 | INFO | aether2 | Model output shape: torch.Size([1, 969, 153])
2025-05-28 19:45:02,927 | INFO | aether2 | Model output shape: torch.Size([1, 970, 153])
2025-05-28 19:45:03,416 | INFO | aether2 | Model output shape: torch.Size([1, 971, 153])
2025-05-28 19:45:03,916 | INFO | aether2 | Model output shape: torch.Size([1, 972, 153])
2025-05-28 19:45:04,411 | INFO | aether2 | Model output shape: torch.Size([1, 973, 153])
2025-05-28 19:45:04,895 | INFO | aether2 | Model output shape: torch.Size([1, 974, 153])
2025-05-28 19:45:05,404 | INFO | aether2 | Model output shape: torch.Size([1, 975, 153])
2025-05-28 19:45:05,848 | INFO | aether2 | Model output shape: torch.Size([1, 976, 153])
2025-05-28 19:45:06,337 | INFO | aether2 | Model output shape: torch.Size([1, 977, 153])
2025-05-28 19:45:06,823 | INFO | aether2 | Model output shape: torch.Size([1, 978, 153])
2025-05-28 19:45:07,321 | INFO | aether2 | Model output shape: torch.Size([1, 979, 153])
2025-05-28 19:45:07,792 | INFO | aether2 | Model output shape: torch.Size([1, 980, 153])
2025-05-28 19:45:08,283 | INFO | aether2 | Model output shape: torch.Size([1, 981, 153])
2025-05-28 19:45:08,779 | INFO | aether2 | Model output shape: torch.Size([1, 982, 153])
2025-05-28 19:45:09,280 | INFO | aether2 | Model output shape: torch.Size([1, 983, 153])
2025-05-28 19:45:09,745 | INFO | aether2 | Model output shape: torch.Size([1, 984, 153])
2025-05-28 19:45:10,245 | INFO | aether2 | Model output shape: torch.Size([1, 985, 153])
2025-05-28 19:45:10,750 | INFO | aether2 | Model output shape: torch.Size([1, 986, 153])
2025-05-28 19:45:11,251 | INFO | aether2 | Model output shape: torch.Size([1, 987, 153])
2025-05-28 19:45:11,743 | INFO | aether2 | Model output shape: torch.Size([1, 988, 153])
2025-05-28 19:45:12,248 | INFO | aether2 | Model output shape: torch.Size([1, 989, 153])
2025-05-28 19:45:12,752 | INFO | aether2 | Model output shape: torch.Size([1, 990, 153])
2025-05-28 19:45:13,259 | INFO | aether2 | Model output shape: torch.Size([1, 991, 153])
2025-05-28 19:45:13,717 | INFO | aether2 | Model output shape: torch.Size([1, 992, 153])
2025-05-28 19:45:14,241 | INFO | aether2 | Model output shape: torch.Size([1, 993, 153])
2025-05-28 19:45:14,748 | INFO | aether2 | Model output shape: torch.Size([1, 994, 153])
2025-05-28 19:45:15,267 | INFO | aether2 | Model output shape: torch.Size([1, 995, 153])
2025-05-28 19:45:15,764 | INFO | aether2 | Model output shape: torch.Size([1, 996, 153])
2025-05-28 19:45:16,271 | INFO | aether2 | Model output shape: torch.Size([1, 997, 153])
2025-05-28 19:45:16,780 | INFO | aether2 | Model output shape: torch.Size([1, 998, 153])
2025-05-28 19:45:17,291 | INFO | aether2 | Model output shape: torch.Size([1, 999, 153])
2025-05-28 19:45:17,291 | WARNING | aether2 | Sekvensen har n竇t 1000 tokens, stoppar generering!
2025-05-28 19:45:32,958 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:45:33,064 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:45:33,064 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:45:33,064 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:45:33,177 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:45:33,185 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:45:33,185 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:45:41,938 | INFO | aether2 | GOAL: hi
2025-05-28 19:45:41,942 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:45:41,961 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:45:41,962 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 19:48:53,958 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:48:54,060 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:48:54,060 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:48:54,061 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:48:54,187 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:48:54,193 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:48:54,194 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:49:20,001 | INFO | aether2 | GOAL: hi
2025-05-28 19:49:20,004 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:49:20,028 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:49:20,029 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 19:49:42,568 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 19:49:42,571 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 19:49:42,588 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 19:49:42,588 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 19:51:16,991 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 19:51:17,090 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 19:51:17,091 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 19:51:17,091 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 19:51:17,212 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 19:51:17,220 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 19:51:17,220 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 19:56:11,872 | INFO | aether2 | GOAL: hi
2025-05-28 19:56:11,875 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 19:56:11,894 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 19:56:11,895 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:07:22,666 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:07:22,769 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:07:22,769 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:07:22,769 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:07:22,885 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:07:22,892 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:07:22,892 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:07:33,729 | INFO | aether2 | GOAL: hi
2025-05-28 20:07:33,732 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:07:33,750 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:07:33,751 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:12:31,343 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:12:31,451 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:12:31,451 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:12:31,451 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:12:31,564 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:12:31,571 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:12:31,572 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:12:39,415 | INFO | aether2 | GOAL: hi
2025-05-28 20:12:39,418 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:12:39,436 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:12:39,437 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:12:53,943 | INFO | aether2 | GOAL: hi
2025-05-28 20:12:53,946 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:12:53,959 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:12:53,959 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:13:49,418 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:13:49,521 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:13:49,521 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:13:49,521 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:13:49,645 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:13:49,650 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:13:49,650 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:14:16,443 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:14:16,544 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:14:16,544 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:14:16,545 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:14:16,660 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:14:16,667 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:14:16,667 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:15:35,709 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:15:35,753 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:15:35,753 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:15:35,754 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:15:35,866 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:15:35,869 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:15:35,869 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:15:39,403 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:15:39,448 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:15:39,448 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:15:39,449 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:15:39,543 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:15:39,545 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:15:39,546 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:15:47,813 | INFO | aether2 | GOAL: hi
2025-05-28 20:15:47,814 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:15:47,827 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:15:47,827 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:16:31,654 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:16:31,755 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:16:31,755 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:16:31,755 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:16:31,870 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:16:31,874 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:16:31,874 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:17:08,498 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:17:08,603 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:17:08,603 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:17:08,603 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:17:08,704 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:17:08,710 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:17:08,710 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:18:12,895 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:18:12,997 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:18:12,997 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:18:12,998 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:18:13,110 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:18:13,116 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:18:13,116 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:21:50,297 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:21:50,447 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:21:50,448 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:21:50,448 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:21:50,580 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:21:50,588 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:21:50,588 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:24:27,418 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:24:27,527 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:24:27,528 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:24:27,528 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:24:27,655 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:24:27,662 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:24:27,662 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:27:01,416 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:27:01,516 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:27:01,517 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:27:01,517 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:27:01,629 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:27:01,634 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:27:01,634 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:30:30,300 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:30:30,432 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:30:30,432 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:30:30,433 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:30:30,542 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:30:30,549 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:30:30,549 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:31:14,540 | INFO | aether2 | GOAL: hi
2025-05-28 20:31:14,543 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:31:14,565 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:31:14,566 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:33:16,580 | INFO | aether2 | Starting Aether...
2025-05-28 20:33:16,742 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:33:16,809 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:33:16,809 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:33:16,878 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:33:16,878 | INFO | aether2 | Aether is ready. Ask me anything!
2025-05-28 20:33:21,916 | INFO | aether2 | GOAL: hi
2025-05-28 20:33:21,918 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:33:21,931 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:33:21,931 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:33:32,732 | INFO | aether2 | Starting Aether...
2025-05-28 20:33:32,892 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:33:32,967 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:33:32,967 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:33:33,028 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:33:33,028 | INFO | aether2 | Aether is ready. Ask me anything!
2025-05-28 20:37:13,508 | INFO | aether2 | Interrupted by user. Exiting...
2025-05-28 20:37:27,531 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 20:37:27,644 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 20:37:27,644 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 20:37:27,644 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 20:37:27,747 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 20:37:27,753 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 20:37:27,753 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 20:37:40,160 | INFO | aether2 | Starting Aether...
2025-05-28 20:37:40,331 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:37:40,415 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:37:40,415 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:37:40,488 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:37:40,488 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 20:37:46,582 | INFO | aether2 | GOAL: hi
2025-05-28 20:37:46,583 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:37:46,598 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:37:46,599 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:44:51,902 | INFO | aether2 | Starting Aether...
2025-05-28 20:44:52,095 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:44:52,165 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:44:52,166 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:44:52,223 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:44:52,224 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 20:44:59,926 | INFO | aether2 | GOAL: hi
2025-05-28 20:44:59,927 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:44:59,942 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:44:59,942 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:49:10,197 | INFO | aether2 | Starting Aether...
2025-05-28 20:49:10,379 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:49:10,460 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:49:10,461 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:49:10,523 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:49:10,523 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 20:49:16,582 | INFO | aether2 | GOAL: hi
2025-05-28 20:49:16,583 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:49:16,600 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:49:16,601 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:50:13,363 | INFO | aether2 | Starting Aether...
2025-05-28 20:50:13,536 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:50:13,602 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:50:13,602 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:50:13,670 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:50:13,670 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 20:50:22,159 | INFO | aether2 | GOAL: hi
2025-05-28 20:50:22,160 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:50:22,177 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:50:22,178 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:55:15,745 | INFO | aether2 | Starting Aether...
2025-05-28 20:55:15,916 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:55:15,980 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:55:15,981 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:55:16,040 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:55:16,040 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 20:55:26,971 | INFO | aether2 | GOAL: hi
2025-05-28 20:55:26,972 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:55:26,988 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:55:26,988 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 20:59:34,661 | INFO | aether2 | Starting Aether...
2025-05-28 20:59:34,853 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 20:59:34,925 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 20:59:34,925 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 20:59:34,989 | INFO | aether2 | Model loaded successfully.
2025-05-28 20:59:34,989 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 20:59:41,241 | INFO | aether2 | GOAL: hi
2025-05-28 20:59:41,242 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 20:59:41,256 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 20:59:41,257 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:02:49,536 | INFO | aether2 | Starting Aether...
2025-05-28 21:02:49,737 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 21:02:49,809 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 21:02:49,810 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 21:02:49,878 | INFO | aether2 | Model loaded successfully.
2025-05-28 21:02:49,878 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 21:02:57,094 | INFO | aether2 | GOAL: hi
2025-05-28 21:02:57,095 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 21:02:57,111 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 21:02:57,112 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:02:57,337 | INFO | aether2 | Response: {"response": "<SOS> hi <EOS> delivery"}
2025-05-28 21:04:06,690 | INFO | aether2 | Interrupted by user. Exiting...
2025-05-28 21:04:20,704 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:04:20,817 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:04:20,818 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 21:04:20,818 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:04:20,927 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:04:20,933 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:04:20,934 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:04:25,806 | INFO | aether2 | GOAL: hi
2025-05-28 21:04:25,810 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 21:04:25,825 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 21:04:25,826 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:05:29,307 | INFO | aether2 | GOAL: hi
2025-05-28 21:05:29,310 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 21:05:29,325 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 21:05:29,325 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:06:32,986 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:06:32,990 | INFO | aether2 | Tokenized input: [1, 153, 154, 50, 77, 155, 156, 98, 2]
2025-05-28 21:06:48,353 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:06:48,356 | INFO | aether2 | Tokenized input: [1, 153, 154, 50, 77, 155, 156, 98, 2]
2025-05-28 21:08:23,727 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:08:23,832 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:08:23,833 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 21:08:23,833 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:08:23,950 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:08:23,957 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:08:23,957 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:08:26,811 | INFO | aether2 | GOAL: hi
2025-05-28 21:08:26,815 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 21:08:26,832 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 21:08:26,833 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:08:37,559 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 21:08:37,562 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 21:08:37,580 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 21:08:37,580 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:15:42,158 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:15:42,261 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:15:42,261 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 21:15:42,262 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:15:42,370 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:15:42,376 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:15:42,377 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:15:47,015 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:15:47,019 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:15:47,039 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:15:47,040 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:16:26,809 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:16:26,923 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:16:26,923 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 21:16:26,924 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:16:27,048 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:16:27,056 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:16:27,056 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:16:31,320 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:16:31,325 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:16:31,347 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:16:31,348 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:17:07,510 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:17:07,514 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:17:07,530 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:17:07,530 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:19:54,913 | INFO | aether2 | GOAL: wikipedia what압 is the capital of sweden ?
2025-05-28 21:20:49,144 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:20:49,148 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:20:49,164 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:20:49,165 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:25:03,755 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:25:03,857 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:25:03,857 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 21:25:03,857 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:25:03,961 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:25:03,966 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:25:03,966 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:25:52,846 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:25:52,954 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:25:52,954 | INFO | aether2 | Keeping latest tokenizer vocabulary.
2025-05-28 21:25:52,954 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:25:53,073 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:25:53,078 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:25:53,078 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:25:57,754 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 21:25:57,760 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 21:25:57,781 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 21:25:57,782 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:27:33,321 | INFO | aether2 | GOAL: hi
2025-05-28 21:27:33,324 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 21:27:33,341 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 21:27:33,342 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:29:43,411 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:29:43,512 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:29:43,512 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:29:43,512 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:29:43,619 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:29:43,619 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:30:15,875 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:30:15,973 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:30:15,973 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:30:15,974 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:30:16,097 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:30:16,104 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:30:16,104 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:30:38,729 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:30:38,733 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:30:38,750 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:30:38,750 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:35:33,750 | INFO | aether2 | GOAL: hi
2025-05-28 21:35:33,753 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 21:35:33,768 | INFO | aether2 | Model output shape: torch.Size([1, 3, 153])
2025-05-28 21:35:33,768 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:36:12,339 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:36:12,443 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:36:12,443 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:36:12,444 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:36:12,558 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:36:12,565 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:36:12,566 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:36:40,222 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:36:40,226 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:36:40,247 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:36:40,248 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:43:35,706 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:43:35,807 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:43:35,808 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:43:35,808 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:43:35,923 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:43:35,930 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:43:35,930 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:43:40,445 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 21:43:40,450 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 21:43:40,469 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:43:40,470 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:44:28,000 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-28 21:44:28,004 | INFO | aether2 | Tokenized input: [1, 94, 3, 73, 3, 3, 3, 3, 2]
2025-05-28 21:44:28,019 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:44:28,020 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:53:10,506 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:53:10,608 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:53:10,608 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:53:10,609 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:53:10,729 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:53:10,735 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:53:10,735 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:53:18,157 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-28 21:53:18,162 | INFO | aether2 | Tokenized input: [1, 94, 3, 73, 3, 3, 3, 3, 2]
2025-05-28 21:53:18,183 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:53:18,184 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 21:55:06,433 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:55:06,531 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:55:06,532 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:55:06,532 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:55:06,635 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:55:06,642 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:55:06,642 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:58:32,554 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 21:58:32,657 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 21:58:32,657 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 21:58:32,658 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 21:58:32,752 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 21:58:32,758 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 21:58:32,758 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 21:58:37,569 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-28 21:58:37,573 | INFO | aether2 | Tokenized input: [1, 94, 3, 73, 3, 3, 3, 3, 2]
2025-05-28 21:58:37,590 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 21:58:37,591 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:02:14,100 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:02:14,203 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:02:14,203 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:02:14,203 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:02:14,318 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:02:14,323 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:02:14,323 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:02:44,372 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:02:44,469 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:02:44,470 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:02:44,470 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:02:44,575 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:02:44,581 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:02:44,582 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:03:36,900 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 22:03:36,904 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 22:03:36,923 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 22:03:36,924 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:08:10,631 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:08:10,733 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:08:10,734 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:08:10,734 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:08:10,840 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:08:10,846 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:08:10,846 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:08:19,381 | INFO | aether2 | GOAL: wahts your name
2025-05-28 22:08:19,386 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 22:08:19,405 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 22:08:19,406 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:09:16,451 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:09:16,548 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:09:16,548 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:09:16,548 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:09:16,649 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:09:16,655 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:09:16,655 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:09:22,384 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:09:22,388 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 22:09:22,404 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 22:09:22,404 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:12:55,876 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:12:55,987 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:12:55,988 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:12:55,988 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:12:56,107 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:12:56,114 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:12:56,114 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:14:34,757 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:14:34,857 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:14:34,857 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:14:34,858 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:14:34,953 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:14:34,961 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:14:34,961 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:14:42,082 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:14:42,087 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 22:14:42,117 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 22:14:42,118 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:15:41,628 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 22:15:41,632 | INFO | aether2 | Tokenized input: [1, 3, 3, 50, 77, 3, 3, 98, 2]
2025-05-28 22:15:41,646 | INFO | aether2 | Model output shape: torch.Size([1, 9, 153])
2025-05-28 22:15:41,647 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:17:51,753 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:17:51,857 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:17:51,857 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:17:51,857 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:17:51,963 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:17:51,968 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:17:51,968 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:17:56,034 | INFO | aether2 | GOAL: wahts your name
2025-05-28 22:17:56,038 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 22:17:56,058 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 22:17:56,058 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:20:21,972 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:20:22,109 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:20:22,110 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:20:22,110 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:20:22,233 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:20:22,241 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:20:22,241 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:20:30,782 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:20:30,786 | INFO | aether2 | Tokenized input: [1, 3, 134, 3, 2]
2025-05-28 22:20:30,805 | INFO | aether2 | Model output shape: torch.Size([1, 5, 153])
2025-05-28 22:20:30,806 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:22:14,400 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:22:14,513 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:22:14,514 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:22:14,514 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:22:14,635 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:22:14,644 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:22:14,644 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:22:21,768 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:22:21,892 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 22:24:11,952 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:24:12,051 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 22:24:12,052 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:24:12,052 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 22:24:12,153 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:24:12,162 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:24:12,162 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:24:21,566 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:24:21,569 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 22:24:21,588 | INFO | aether2 | Model output shape: torch.Size([1, 5, 155])
2025-05-28 22:24:21,589 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:25:10,335 | INFO | aether2 | GOAL: hi
2025-05-28 22:25:10,338 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 22:25:10,350 | INFO | aether2 | Model output shape: torch.Size([1, 3, 155])
2025-05-28 22:25:10,350 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:31:49,194 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:31:49,290 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 22:31:49,290 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:31:49,291 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 22:31:49,386 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:31:49,393 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:31:49,394 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:31:58,443 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 22:31:58,494 | INFO | aether2 | Tokenized input: [1, 155, 156, 50, 77, 157, 158, 98, 2]
2025-05-28 22:32:52,108 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:32:52,205 | INFO | aether2 | Tokenizer vocab loaded from file with size 159
2025-05-28 22:32:52,206 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:32:52,206 | INFO | aether2 | Setting vocab_size to 159 before loading checkpoint.
2025-05-28 22:32:52,310 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:32:52,318 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:32:52,318 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:33:00,131 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:33:00,134 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 22:33:00,149 | INFO | aether2 | Model output shape: torch.Size([1, 5, 159])
2025-05-28 22:33:00,149 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 22:33:31,034 | INFO | aether2 | Starting Aether...
2025-05-28 22:33:31,212 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 22:33:31,274 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 22:33:31,274 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 22:33:31,344 | WARNING | aether2 | Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([159, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([159, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([159]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 22:33:32,717 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 22:33:32,756 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 22:33:32,756 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 22:33:32,757 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 22:33:32,833 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 22:33:32,836 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 22:33:32,836 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 22:33:32,888 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 22:33:42,922 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:33:42,927 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 22:34:20,112 | INFO | aether2 | Starting Aether...
2025-05-28 22:34:20,292 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 22:34:20,358 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 22:34:20,358 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 22:34:20,422 | INFO | aether2 | Model loaded successfully.
2025-05-28 22:34:20,423 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 22:34:36,388 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:34:36,393 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 22:39:04,309 | INFO | aether2 | Starting Aether...
2025-05-28 22:39:04,485 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 22:39:04,567 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 22:39:04,568 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 22:39:04,630 | INFO | aether2 | Model loaded successfully.
2025-05-28 22:39:04,630 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 22:40:19,436 | INFO | aether2 | Interrupted by user. Exiting...
2025-05-28 22:40:26,503 | INFO | aether2 | Starting Aether...
2025-05-28 22:40:26,673 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 22:40:26,750 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 22:40:26,751 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 22:40:26,802 | INFO | aether2 | Model loaded successfully.
2025-05-28 22:40:26,803 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 22:40:39,164 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:40:39,169 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 22:51:47,610 | INFO | aether2 | Starting Aether...
2025-05-28 22:51:47,824 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 22:51:47,896 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 22:51:47,897 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 22:51:47,959 | INFO | aether2 | Model loaded successfully.
2025-05-28 22:51:47,959 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 22:52:01,063 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 22:52:01,069 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:03:09,035 | INFO | aether2 | Starting Aether...
2025-05-28 23:03:09,221 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 23:03:09,300 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:03:09,300 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 23:03:09,356 | INFO | aether2 | Model loaded successfully.
2025-05-28 23:03:09,356 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 23:03:22,295 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:03:22,300 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:05:36,717 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:05:36,830 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 23:05:36,830 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:05:36,831 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:05:36,831 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 23:05:36,950 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:05:36,957 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:05:36,957 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:06:26,160 | INFO | aether2 | GOAL: hi
2025-05-28 23:06:26,167 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 23:06:26,193 | INFO | aether2 | Model output shape: torch.Size([1, 3, 155])
2025-05-28 23:06:26,194 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:07:19,832 | INFO | aether2 | Starting Aether...
2025-05-28 23:07:20,011 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 23:07:20,085 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:07:20,085 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 23:07:20,153 | WARNING | aether2 | Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([155, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([155, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([155]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 23:07:21,629 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:07:21,671 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 23:07:21,671 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:07:21,672 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:07:21,672 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 23:07:21,752 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:07:21,756 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:07:21,756 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:07:21,808 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 23:07:35,506 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:07:35,512 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:09:50,041 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:09:50,152 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 23:10:25,749 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:10:25,849 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 23:10:25,855 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:10:25,856 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:10:25,856 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 23:10:25,956 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:10:25,963 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:10:25,963 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:10:32,644 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:10:32,648 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:10:32,667 | INFO | aether2 | Model output shape: torch.Size([1, 5, 155])
2025-05-28 23:10:32,667 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:11:47,443 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:11:47,541 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 23:11:47,542 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:11:47,542 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:11:47,543 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 23:11:47,641 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:11:47,648 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:11:47,648 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:12:00,884 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:12:00,889 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:12:00,909 | INFO | aether2 | Model output shape: torch.Size([1, 5, 155])
2025-05-28 23:12:00,909 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:16:50,457 | INFO | aether2 | Starting Aether...
2025-05-28 23:16:50,634 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 23:16:50,715 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:16:50,715 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 23:16:50,788 | WARNING | aether2 | Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([155, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([155, 256]) from checkpoint, the shape in current model is torch.Size([153, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([155]) from checkpoint, the shape in current model is torch.Size([153]).)
2025-05-28 23:16:52,257 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:16:52,298 | INFO | aether2 | Tokenizer vocab loaded from file with size 153
2025-05-28 23:16:52,299 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:16:52,299 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:16:52,299 | INFO | aether2 | Setting vocab_size to 153 before loading checkpoint.
2025-05-28 23:16:52,370 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:16:52,373 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:16:52,373 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:16:52,426 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 23:17:02,981 | INFO | aether2 | Interrupted by user. Exiting...
2025-05-28 23:17:11,046 | INFO | aether2 | Starting Aether...
2025-05-28 23:17:11,242 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 23:17:11,335 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:17:11,335 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 23:17:11,424 | INFO | aether2 | Model loaded successfully.
2025-05-28 23:17:11,424 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 23:17:41,982 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:17:41,987 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:24:09,203 | INFO | aether2 | Starting Aether...
2025-05-28 23:24:09,394 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-28 23:24:09,477 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:24:09,478 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-28 23:24:09,546 | INFO | aether2 | Model loaded successfully.
2025-05-28 23:24:09,547 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-28 23:24:22,060 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:24:22,066 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:24:49,986 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:24:50,083 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 23:24:50,084 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:24:50,085 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:24:50,085 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 23:24:50,198 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:24:50,206 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:24:50,207 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:24:59,090 | INFO | aether2 | GOAL: hi
2025-05-28 23:24:59,094 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 23:24:59,112 | INFO | aether2 | Model output shape: torch.Size([1, 3, 155])
2025-05-28 23:24:59,112 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:25:06,128 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:25:06,131 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:25:06,146 | INFO | aether2 | Model output shape: torch.Size([1, 5, 155])
2025-05-28 23:25:06,147 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:29:11,641 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:29:11,743 | INFO | aether2 | Tokenizer vocab loaded from file with size 155
2025-05-28 23:29:11,743 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:29:11,743 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:29:11,743 | INFO | aether2 | Setting vocab_size to 155 before loading checkpoint.
2025-05-28 23:29:11,847 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:29:11,861 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:29:11,862 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:29:22,995 | INFO | aether2 | GOAL: hi
2025-05-28 23:29:22,999 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-28 23:29:23,017 | INFO | aether2 | Model output shape: torch.Size([1, 3, 155])
2025-05-28 23:29:23,018 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:29:34,372 | INFO | aether2 | GOAL: wahts your name
2025-05-28 23:29:34,391 | INFO | aether2 | Tokenized input: [1, 155, 134, 154, 2]
2025-05-28 23:37:03,110 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:38:15,105 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:39:22,357 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:39:22,463 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:39:22,463 | INFO | aether2 | Token embedding updated with vocab size 156
2025-05-28 23:39:22,548 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:41:49,588 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:41:49,690 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:41:49,690 | INFO | aether2 | Token embedding updated with vocab size 156
2025-05-28 23:41:49,784 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:42:19,355 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:42:19,462 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:42:19,462 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:42:19,462 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:42:19,462 | INFO | aether2 | Setting vocab_size to 156 before loading checkpoint.
2025-05-28 23:42:19,552 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:42:19,562 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:42:19,562 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:43:51,356 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:43:51,453 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:43:51,454 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:43:51,454 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:43:51,454 | INFO | aether2 | Setting vocab_size to 156 before loading checkpoint.
2025-05-28 23:43:51,556 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:43:51,566 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:43:51,566 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:43:54,578 | INFO | aether2 | GOAL: train model
2025-05-28 23:43:54,704 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:43:54,808 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:43:54,814 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:43:54,814 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:43:54,814 | INFO | aether2 | Setting vocab_size to 156 before loading checkpoint.
2025-05-28 23:43:54,947 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:43:54,956 | INFO | aether2 |  Starting model training for 10 epochs...
2025-05-28 23:43:54,956 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:43:55,040 | INFO | aether2 | Training finished.
2025-05-28 23:44:38,155 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:44:38,251 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:44:38,252 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:44:38,252 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:44:38,253 | INFO | aether2 | Setting vocab_size to 156 before loading checkpoint.
2025-05-28 23:44:38,339 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:44:38,356 | INFO | aether2 |  Starting model training for 30 epochs...
2025-05-28 23:44:38,356 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:44:47,796 | INFO | aether2 | GOAL: train model
2025-05-28 23:44:47,891 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:44:47,995 | INFO | aether2 | Tokenizer vocab loaded from file with size 156
2025-05-28 23:44:47,995 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:44:47,995 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:44:47,995 | INFO | aether2 | Setting vocab_size to 156 before loading checkpoint.
2025-05-28 23:44:48,090 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:44:48,108 | INFO | aether2 |  Starting model training for 30 epochs...
2025-05-28 23:44:48,108 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:44:48,178 | INFO | aether2 | Training finished.
2025-05-28 23:45:09,320 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-28 23:45:09,371 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-28 23:45:09,382 | ERROR | aether2 | Token index 159 is out of range.
2025-05-28 23:46:23,017 | INFO | aether2 | GOAL: train model
2025-05-28 23:46:23,169 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:46:23,280 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:46:23,280 | INFO | aether2 | Using embed_size = 256
2025-05-28 23:46:23,280 | INFO | aether2 | Keeping latest tokenizer vocabulary and ignoring checkpoint's smaller vocab.
2025-05-28 23:46:23,280 | INFO | aether2 | Setting vocab_size to 160 before loading checkpoint.
2025-05-28 23:46:23,383 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:46:23,398 | INFO | aether2 |  Starting model training for 30 epochs...
2025-05-28 23:46:23,401 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:46:23,481 | INFO | aether2 | Training finished.
2025-05-28 23:48:52,695 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:48:52,795 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:48:52,796 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:48:52,912 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:48:52,913 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:48:52,920 | INFO | aether2 |  Starting model training for 30 epochs...
2025-05-28 23:48:52,920 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:49:02,133 | INFO | aether2 | GOAL: whats your name ?
2025-05-28 23:49:02,137 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-28 23:49:02,157 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-28 23:49:06,875 | INFO | aether2 | GOAL: train model
2025-05-28 23:49:07,004 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:49:07,114 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:49:07,115 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:49:07,244 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:49:07,244 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:49:07,252 | INFO | aether2 |  Starting model training for 30 epochs...
2025-05-28 23:49:07,252 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:49:07,340 | INFO | aether2 | Training finished.
2025-05-28 23:50:19,478 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-28 23:50:19,478 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:50:19,500 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-28 23:50:19,515 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-28 23:50:19,515 | INFO | aether2 | Using vocab_size = 4 for model initialization.
2025-05-28 23:50:21,871 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:50:22,013 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:50:22,013 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:50:22,121 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:50:22,121 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:50:22,133 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-28 23:50:22,133 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:50:33,902 | INFO | aether2 | GOAL: train model
2025-05-28 23:50:33,903 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-28 23:50:33,904 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:50:33,915 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-28 23:50:33,922 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-28 23:50:33,922 | INFO | aether2 | Using vocab_size = 160 for model initialization.
2025-05-28 23:50:34,034 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:50:34,135 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:50:34,149 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:50:34,265 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:50:34,265 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:50:34,274 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-28 23:50:34,274 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:50:34,353 | INFO | aether2 | Training finished.
2025-05-28 23:54:38,581 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-28 23:54:38,581 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:54:38,597 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-28 23:54:38,616 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-28 23:54:38,616 | INFO | aether2 | Using vocab_size = 4 for model initialization.
2025-05-28 23:54:40,758 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:54:40,865 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:54:40,866 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:54:40,960 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:54:40,960 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:54:40,974 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-28 23:54:40,974 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:54:48,250 | INFO | aether2 | GOAL: train model
2025-05-28 23:54:48,251 | INFO | aether2 | Starting model training process...
2025-05-28 23:54:48,252 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-28 23:54:48,252 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:54:48,261 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-28 23:54:48,275 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-28 23:54:48,275 | INFO | aether2 | Using vocab_size = 160 for model initialization.
2025-05-28 23:54:48,366 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:54:48,483 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:54:48,483 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:54:48,592 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:54:48,592 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:54:48,592 | INFO | aether2 | Starting model training for 30 epochs...
2025-05-28 23:54:48,601 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-28 23:54:48,666 | INFO | aether2 | Training finished.
2025-05-28 23:56:01,046 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-28 23:56:01,046 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-28 23:56:01,076 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-28 23:56:01,092 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-28 23:56:01,094 | INFO | aether2 | Using vocab_size = 4 for model initialization.
2025-05-28 23:56:03,231 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-28 23:56:03,330 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-28 23:56:03,330 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-28 23:56:03,424 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-28 23:56:03,424 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 323
2025-05-28 23:56:03,438 | INFO | aether2 | Starting model training for 400 epochs...
2025-05-28 23:59:28,151 | INFO | aether2 | Epoch 324: Loss = 5.2849
2025-05-28 23:59:28,272 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 00:01:22,656 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 00:01:22,656 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:01:22,675 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 00:01:22,691 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-29 00:01:22,692 | INFO | aether2 | Using vocab_size = 4 for model initialization.
2025-05-29 00:01:25,042 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 00:01:25,170 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-29 00:01:25,170 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-29 00:01:25,292 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 00:01:25,293 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 325
2025-05-29 00:01:25,302 | INFO | aether2 | Starting model training for 400 epochs...
2025-05-29 00:04:51,669 | INFO | aether2 | Epoch 326: Loss = 5.2869
2025-05-29 00:04:51,745 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 00:05:59,561 | INFO | aether2 | Vocab loaded with size: 160
2025-05-29 00:05:59,586 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 00:05:59,586 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:05:59,605 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 00:05:59,626 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data_en.json
2025-05-29 00:05:59,626 | INFO | aether2 | Using vocab_size = 160 for model initialization.
2025-05-29 00:06:01,964 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 00:06:02,074 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-29 00:06:02,074 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-29 00:06:02,187 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 00:06:02,187 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 327
2025-05-29 00:06:02,187 | INFO | aether2 | Starting model training for 400 epochs...
2025-05-29 00:06:57,529 | INFO | aether2 | Starting Aether...
2025-05-29 00:06:57,529 | INFO | aether2 | Vocab loaded with size: 160
2025-05-29 00:06:57,800 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-29 00:06:57,916 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:06:57,917 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-29 00:06:58,023 | INFO | aether2 | Model loaded successfully.
2025-05-29 00:06:58,023 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-29 00:07:09,744 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 00:07:09,746 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 00:08:57,253 | INFO | aether2 | Starting Aether...
2025-05-29 00:08:57,270 | INFO | aether2 | Vocab loaded with size: 160
2025-05-29 00:08:57,558 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-29 00:08:57,674 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:08:57,676 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-29 00:08:57,768 | INFO | aether2 | Model loaded successfully.
2025-05-29 00:08:57,768 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-29 00:09:09,006 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 00:09:09,008 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 00:09:09,038 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 00:09:09,327 | INFO | aether2 | Response: {"response": "<SOS> whats your name <EOS> name"}
2025-05-29 00:09:31,493 | INFO | aether2 | Epoch 328: Loss = 5.2870
2025-05-29 00:09:31,578 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 00:10:21,964 | INFO | aether2 | Exiting Aether.
2025-05-29 00:10:30,405 | INFO | aether2 | Starting Aether...
2025-05-29 00:10:30,442 | INFO | aether2 | Vocab loaded with size: 160
2025-05-29 00:10:30,446 | INFO | aether2 | Token embedding initialized with vocab size: 160
2025-05-29 00:10:30,701 | INFO | aether2 | Tokenizer vocabulary updated with 40000 examples.
2025-05-29 00:10:30,822 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:10:30,822 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-29 00:10:30,927 | INFO | aether2 | Model loaded successfully.
2025-05-29 00:10:30,927 | INFO | aether2 | Aether is ready. Type something to chat!
2025-05-29 00:10:42,080 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 00:10:42,083 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 00:10:42,122 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 00:10:42,274 | INFO | aether2 | Response: {"response": "<SOS> whats your name <EOS> name"}
2025-05-29 00:13:00,121 | INFO | aether2 | Epoch 329: Loss = 5.2867
2025-05-29 00:13:00,201 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 00:16:27,904 | INFO | aether2 | Epoch 330: Loss = 5.2871
2025-05-29 00:16:27,984 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 00:17:51,831 | INFO | aether2 | Exiting Aether.
2025-05-29 00:18:06,615 | INFO | aether2 | Vocab loaded with size: 160
2025-05-29 00:18:06,617 | INFO | aether2 | Token embedding initialized with vocab size: 160
2025-05-29 00:18:06,646 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 00:18:06,646 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:18:06,661 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 00:18:06,687 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 00:18:06,688 | INFO | aether2 | Using vocab_size = 160 for model initialization.
2025-05-29 00:18:09,322 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 00:18:09,425 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-29 00:18:09,425 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-29 00:18:09,518 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 00:18:09,518 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 331
2025-05-29 00:18:09,527 | WARNING | aether2 | start_epoch (331) is greater than total epochs (20). Skipping training.
2025-05-29 00:18:25,898 | INFO | aether2 | Vocab loaded with size: 160
2025-05-29 00:18:25,898 | INFO | aether2 | Token embedding initialized with vocab size: 160
2025-05-29 00:18:25,947 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 00:18:25,947 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 00:18:25,966 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 00:18:25,982 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 00:18:25,984 | INFO | aether2 | Using vocab_size = 160 for model initialization.
2025-05-29 00:18:28,140 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 00:18:28,240 | INFO | aether2 | Tokenizer vocab loaded from file with size 160
2025-05-29 00:18:28,240 | INFO | aether2 | Token embedding updated with vocab size 160
2025-05-29 00:18:28,333 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 00:18:28,335 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 331
2025-05-29 00:18:28,335 | INFO | aether2 | Starting model training for 500 epochs...
2025-05-29 01:19:06,399 | INFO | aether2 | Vocab loaded with size: 166
2025-05-29 01:19:06,399 | INFO | aether2 | Token embedding initialized with vocab size: 166
2025-05-29 01:19:06,437 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 01:19:06,437 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 01:19:06,452 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 01:19:06,459 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 01:19:06,459 | INFO | aether2 | Using vocab_size = 166 for model initialization.
2025-05-29 01:19:08,775 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 01:19:08,879 | INFO | aether2 | Tokenizer vocab loaded from file with size 166
2025-05-29 01:19:08,879 | INFO | aether2 | Token embedding updated with vocab size 166
2025-05-29 01:19:08,988 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 01:19:08,988 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 331
2025-05-29 01:19:08,999 | INFO | aether2 | Starting model training for 500 epochs...
2025-05-29 01:21:23,775 | INFO | aether2 | Starting Aether...
2025-05-29 01:21:23,775 | INFO | aether2 | Vocab loaded with size: 166
2025-05-29 01:21:23,782 | INFO | aether2 | Token embedding initialized with vocab size: 166
2025-05-29 01:21:24,035 | INFO | aether2 | Tokenizer vocabulary updated with 40004 examples.
2025-05-29 01:21:24,145 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 01:21:24,145 | INFO | aether2 | Files in current directory: ['.git', '.gitattributes', '.gitignore', '.mvn', '.venv', '.vscode', 'aether_model.pth', 'ai_Model', 'checkpoint_latest.pth', 'HELP.md', 'logs', 'mvnw', 'mvnw.cmd', 'pom.xml', 'readME', 'README.md', 'src', 'target', 'tokenizer_vocab.json', 'training_data_10000_en.json', 'words_dictionary.json']
2025-05-29 01:21:24,241 | WARNING | aether2 | Model not found  training new model... (Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([166, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([166, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([166]).)
2025-05-29 01:21:24,243 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 01:21:24,243 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 01:21:24,265 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 01:21:24,287 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 01:21:24,287 | INFO | aether2 | Using vocab_size = 166 for model initialization.
2025-05-29 01:21:26,336 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 01:21:26,392 | INFO | aether2 | Tokenizer vocab loaded from file with size 166
2025-05-29 01:21:26,392 | INFO | aether2 | Token embedding updated with vocab size 166
2025-05-29 01:21:26,511 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 01:21:26,511 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 331
2025-05-29 01:21:26,516 | INFO | aether2 | Starting model training for 500 epochs...
2025-05-29 01:22:34,445 | INFO | aether2 | Epoch 332: Loss = 5.3613
2025-05-29 01:22:34,524 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:25:55,998 | INFO | aether2 | Epoch 333: Loss = 5.3618
2025-05-29 01:25:56,076 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:26:51,209 | INFO | aether2 | Vocab loaded with size: 166
2025-05-29 01:26:51,209 | INFO | aether2 | Token embedding initialized with vocab size: 166
2025-05-29 01:26:51,277 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 01:26:51,277 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 01:26:51,299 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 01:26:51,321 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 01:26:51,322 | INFO | aether2 | Using vocab_size = 166 for model initialization.
2025-05-29 01:26:53,678 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 01:26:53,786 | INFO | aether2 | Tokenizer vocab loaded from file with size 166
2025-05-29 01:26:53,786 | INFO | aether2 | Token embedding updated with vocab size 166
2025-05-29 01:27:35,269 | INFO | aether2 | Vocab loaded with size: 166
2025-05-29 01:27:35,269 | INFO | aether2 | Token embedding initialized with vocab size: 166
2025-05-29 01:27:35,300 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 01:27:35,302 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 01:27:35,321 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 01:27:35,338 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 01:27:35,339 | INFO | aether2 | Using vocab_size = 166 for model initialization.
2025-05-29 01:27:37,479 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 01:27:37,571 | INFO | aether2 | Tokenizer vocab loaded from file with size 166
2025-05-29 01:27:37,571 | INFO | aether2 | Token embedding updated with vocab size 166
2025-05-29 01:27:37,670 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 01:27:37,671 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 334
2025-05-29 01:27:37,678 | WARNING | aether2 | start_epoch (334) is greater than total epochs (200). Skipping training.
2025-05-29 01:29:08,358 | INFO | aether2 | Vocab loaded with size: 166
2025-05-29 01:29:08,359 | INFO | aether2 | Token embedding initialized with vocab size: 166
2025-05-29 01:29:08,389 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 01:29:08,390 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 01:29:08,406 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 01:29:08,420 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 01:29:08,421 | INFO | aether2 | Using vocab_size = 166 for model initialization.
2025-05-29 01:29:10,946 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 01:29:11,043 | INFO | aether2 | Tokenizer vocab loaded from file with size 166
2025-05-29 01:29:11,043 | INFO | aether2 | Token embedding updated with vocab size 166
2025-05-29 01:29:11,160 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 01:29:11,160 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 334
2025-05-29 01:29:11,168 | INFO | aether2 | Starting model training for 350 epochs...
2025-05-29 01:32:32,405 | INFO | aether2 | Epoch 335: Loss = 5.3610
2025-05-29 01:32:32,501 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:35:59,483 | INFO | aether2 | Epoch 336: Loss = 5.3615
2025-05-29 01:35:59,564 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:39:27,155 | INFO | aether2 | Epoch 337: Loss = 5.3581
2025-05-29 01:39:27,229 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:42:48,968 | INFO | aether2 | Epoch 338: Loss = 5.3611
2025-05-29 01:42:49,046 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:46:02,250 | INFO | aether2 | Epoch 339: Loss = 5.3579
2025-05-29 01:46:02,329 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:49:14,843 | INFO | aether2 | Epoch 340: Loss = 5.3619
2025-05-29 01:49:14,925 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:52:27,055 | INFO | aether2 | Epoch 341: Loss = 5.3597
2025-05-29 01:52:27,139 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:55:37,396 | INFO | aether2 | Epoch 342: Loss = 5.3619
2025-05-29 01:55:37,485 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 01:58:59,553 | INFO | aether2 | Epoch 343: Loss = 5.3629
2025-05-29 01:58:59,639 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:02:26,470 | INFO | aether2 | Epoch 344: Loss = 5.3616
2025-05-29 02:02:26,552 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:05:52,907 | INFO | aether2 | Epoch 345: Loss = 5.3614
2025-05-29 02:05:52,981 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:09:15,647 | INFO | aether2 | Epoch 346: Loss = 5.3601
2025-05-29 02:09:15,731 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:12:44,079 | INFO | aether2 | Epoch 347: Loss = 5.3628
2025-05-29 02:12:44,153 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:16:11,946 | INFO | aether2 | Epoch 348: Loss = 5.3604
2025-05-29 02:16:12,043 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:19:39,107 | INFO | aether2 | Epoch 349: Loss = 5.3582
2025-05-29 02:19:39,184 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:23:04,414 | INFO | aether2 | Epoch 350: Loss = 5.3596
2025-05-29 02:23:04,489 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 02:23:04,489 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-29 02:26:18,151 | INFO | aether2 | GOAL: exit
2025-05-29 02:26:18,185 | INFO | aether2 | Tokenized input: [1, 166, 2]
2025-05-29 02:26:18,185 | ERROR | aether2 | Token index 166 is out of range.
2025-05-29 02:27:00,675 | INFO | aether2 | Vocab loaded with size: 167
2025-05-29 02:27:00,675 | INFO | aether2 | Token embedding initialized with vocab size: 167
2025-05-29 02:27:00,725 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 02:27:00,725 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 02:27:00,743 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 02:27:00,759 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 02:27:00,759 | INFO | aether2 | Using vocab_size = 167 for model initialization.
2025-05-29 02:27:03,040 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 02:27:03,162 | INFO | aether2 | Tokenizer vocab loaded from file with size 167
2025-05-29 02:27:03,162 | INFO | aether2 | Token embedding updated with vocab size 167
2025-05-29 02:27:03,283 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 02:27:03,284 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 02:27:03,291 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 02:27:10,316 | INFO | aether2 | GOAL: hi
2025-05-29 02:27:10,320 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-29 02:27:10,336 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:27:36,078 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 02:27:36,081 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 02:27:36,094 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:29:14,605 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 02:29:14,609 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 02:29:14,626 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:29:33,128 | INFO | aether2 | GOAL: it a town
2025-05-29 02:29:33,160 | INFO | aether2 | Tokenized input: [1, 167, 80, 168, 2]
2025-05-29 02:29:33,161 | ERROR | aether2 | Token index 168 is out of range.
2025-05-29 02:29:57,362 | INFO | aether2 | GOAL: its a town
2025-05-29 02:29:57,382 | INFO | aether2 | Tokenized input: [1, 169, 80, 168, 2]
2025-05-29 02:29:57,383 | ERROR | aether2 | Token index 169 is out of range.
2025-05-29 02:30:35,862 | INFO | aether2 | Vocab loaded with size: 170
2025-05-29 02:30:35,863 | INFO | aether2 | Token embedding initialized with vocab size: 170
2025-05-29 02:30:35,889 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 02:30:35,889 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 02:30:35,907 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 02:30:35,923 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 02:30:35,924 | INFO | aether2 | Using vocab_size = 170 for model initialization.
2025-05-29 02:30:38,246 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 02:30:38,347 | INFO | aether2 | Tokenizer vocab loaded from file with size 170
2025-05-29 02:30:38,348 | INFO | aether2 | Token embedding updated with vocab size 170
2025-05-29 02:30:38,459 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 02:30:38,459 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 02:30:38,468 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 02:31:02,036 | INFO | aether2 | GOAL: its a town
2025-05-29 02:31:02,039 | INFO | aether2 | Tokenized input: [1, 169, 80, 168, 2]
2025-05-29 02:31:02,059 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:32:15,244 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 02:32:15,247 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 02:32:15,263 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:35:53,235 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-29 02:35:53,313 | INFO | aether2 | Tokenized input: [1, 94, 170, 73, 171, 172, 173, 174, 2]
2025-05-29 02:35:53,314 | ERROR | aether2 | Token index 174 is out of range.
2025-05-29 02:36:51,586 | INFO | aether2 | Vocab loaded with size: 175
2025-05-29 02:36:51,586 | INFO | aether2 | Token embedding initialized with vocab size: 175
2025-05-29 02:36:51,616 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 02:36:51,616 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 02:36:51,632 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 02:36:51,649 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 02:36:51,652 | INFO | aether2 | Using vocab_size = 175 for model initialization.
2025-05-29 02:36:54,057 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 02:36:54,169 | INFO | aether2 | Tokenizer vocab loaded from file with size 175
2025-05-29 02:36:54,169 | INFO | aether2 | Token embedding updated with vocab size 175
2025-05-29 02:36:54,293 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 02:36:54,293 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 02:36:54,308 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 02:37:01,152 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 02:37:01,155 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 02:37:01,177 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:37:09,167 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-29 02:37:09,170 | INFO | aether2 | Tokenized input: [1, 94, 170, 73, 171, 172, 173, 174, 2]
2025-05-29 02:37:09,183 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:37:25,319 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 02:37:25,322 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 02:37:25,339 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:37:36,802 | INFO | aether2 | GOAL: my name is O
2025-05-29 02:37:36,817 | INFO | aether2 | Tokenized input: [1, 51, 154, 50, 175, 2]
2025-05-29 02:37:36,817 | ERROR | aether2 | Token index 175 is out of range.
2025-05-29 02:49:13,471 | INFO | aether2 | Vocab loaded with size: 176
2025-05-29 02:49:13,471 | INFO | aether2 | Token embedding initialized with vocab size: 176
2025-05-29 02:49:13,510 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 02:49:13,510 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 02:49:13,528 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 02:49:13,545 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 02:49:13,546 | INFO | aether2 | Using vocab_size = 176 for model initialization.
2025-05-29 02:49:15,847 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 02:49:15,958 | INFO | aether2 | Tokenizer vocab loaded from file with size 176
2025-05-29 02:49:15,958 | INFO | aether2 | Token embedding updated with vocab size 176
2025-05-29 02:49:16,063 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 02:49:16,063 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 02:49:16,085 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 02:49:42,051 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 02:49:42,055 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 02:49:42,062 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:49:54,652 | INFO | aether2 | GOAL: my name is q
2025-05-29 02:56:33,860 | INFO | aether2 | Vocab loaded with size: 176
2025-05-29 02:56:33,860 | INFO | aether2 | Token embedding initialized with vocab size: 176
2025-05-29 02:56:33,889 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 02:56:33,889 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 02:56:33,904 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 02:56:33,955 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 02:56:33,955 | INFO | aether2 | Using vocab_size = 176 for model initialization.
2025-05-29 02:56:36,227 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 02:56:36,323 | INFO | aether2 | Tokenizer vocab loaded from file with size 176
2025-05-29 02:56:36,323 | INFO | aether2 | Token embedding updated with vocab size 176
2025-05-29 02:56:36,435 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 02:56:36,435 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 02:56:36,451 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 02:57:05,509 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 02:57:05,513 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 02:57:05,536 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 02:57:17,030 | INFO | aether2 | GOAL: its a big town
2025-05-29 02:57:17,048 | INFO | aether2 | Tokenized input: [1, 169, 80, 176, 168, 2]
2025-05-29 02:57:17,048 | ERROR | aether2 | Token index 176 is out of range.
2025-05-29 02:59:48,060 | INFO | aether2 | Vocab loaded with size: 177
2025-05-29 02:59:48,060 | INFO | aether2 | Token embedding initialized with vocab size: 177
2025-05-29 02:59:48,089 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 02:59:48,089 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 02:59:48,107 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 02:59:48,124 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 02:59:48,124 | INFO | aether2 | Using vocab_size = 177 for model initialization.
2025-05-29 02:59:50,388 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 02:59:50,487 | INFO | aether2 | Tokenizer vocab loaded from file with size 177
2025-05-29 02:59:50,488 | INFO | aether2 | Token embedding updated with vocab size 177
2025-05-29 02:59:50,589 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 02:59:50,589 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 02:59:50,609 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 02:59:57,147 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 02:59:57,150 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 02:59:57,171 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 03:00:08,309 | INFO | aether2 | GOAL: how is king in swenden ?
2025-05-29 03:00:08,341 | INFO | aether2 | Tokenized input: [1, 94, 50, 177, 173, 178, 2]
2025-05-29 03:00:08,342 | ERROR | aether2 | Token index 178 is out of range.
2025-05-29 03:08:06,334 | INFO | aether2 | Vocab loaded with size: 179
2025-05-29 03:08:06,335 | INFO | aether2 | Token embedding initialized with vocab size: 179
2025-05-29 03:08:06,360 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 03:08:06,360 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 03:08:06,379 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 03:08:06,398 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 03:08:06,399 | INFO | aether2 | Using vocab_size = 179 for model initialization.
2025-05-29 03:08:08,740 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 03:08:08,831 | INFO | aether2 | Tokenizer vocab loaded from file with size 179
2025-05-29 03:08:08,831 | INFO | aether2 | Token embedding updated with vocab size 179
2025-05-29 03:08:08,959 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 03:08:08,959 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 03:08:08,974 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 03:08:08,974 | WARNING | aether2 | start_epoch (351) is greater than total epochs (350). Skipping training.
2025-05-29 03:08:20,446 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 03:08:20,449 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 03:08:20,472 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 03:08:38,446 | INFO | aether2 | GOAL: im from a town named G樓le
2025-05-29 03:08:38,507 | INFO | aether2 | Tokenized input: [1, 179, 180, 80, 168, 181, 182, 2]
2025-05-29 03:08:38,507 | ERROR | aether2 | Token index 182 is out of range.
2025-05-29 03:09:06,518 | INFO | aether2 | GOAL: ho are you
2025-05-29 03:09:06,534 | INFO | aether2 | Tokenized input: [1, 183, 161, 87, 2]
2025-05-29 03:09:06,534 | ERROR | aether2 | Token index 183 is out of range.
2025-05-29 03:10:35,681 | INFO | aether2 | Vocab loaded with size: 184
2025-05-29 03:10:35,697 | INFO | aether2 | Token embedding initialized with vocab size: 184
2025-05-29 03:10:35,727 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 03:10:35,727 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 03:10:35,741 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 03:10:35,762 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 03:10:35,762 | INFO | aether2 | Using vocab_size = 184 for model initialization.
2025-05-29 03:10:38,023 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 03:10:38,118 | INFO | aether2 | Tokenizer vocab loaded from file with size 184
2025-05-29 03:10:38,118 | INFO | aether2 | Token embedding updated with vocab size 184
2025-05-29 03:10:38,225 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 03:10:38,225 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 351
2025-05-29 03:10:38,244 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 03:10:38,244 | INFO | aether2 | Starting model training for 1000 epochs...
2025-05-29 03:14:00,525 | INFO | aether2 | Epoch 352: Loss = 5.2540
2025-05-29 03:14:00,607 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:17:07,679 | INFO | aether2 | Epoch 353: Loss = 5.2536
2025-05-29 03:17:07,760 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:20:10,129 | INFO | aether2 | Epoch 354: Loss = 5.2561
2025-05-29 03:20:10,191 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:23:08,833 | INFO | aether2 | Epoch 355: Loss = 5.2553
2025-05-29 03:23:08,890 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:26:08,009 | INFO | aether2 | Epoch 356: Loss = 5.2572
2025-05-29 03:26:08,083 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:29:12,012 | INFO | aether2 | Epoch 357: Loss = 5.2569
2025-05-29 03:29:12,094 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:32:36,124 | INFO | aether2 | Epoch 358: Loss = 5.2561
2025-05-29 03:32:36,202 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:36:03,393 | INFO | aether2 | Epoch 359: Loss = 5.2577
2025-05-29 03:36:03,476 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:39:29,067 | INFO | aether2 | Epoch 360: Loss = 5.2556
2025-05-29 03:39:29,146 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:42:42,195 | INFO | aether2 | Epoch 361: Loss = 5.2575
2025-05-29 03:42:42,273 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:45:55,984 | INFO | aether2 | Epoch 362: Loss = 5.2548
2025-05-29 03:45:56,062 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:49:12,106 | INFO | aether2 | Epoch 363: Loss = 5.2526
2025-05-29 03:49:12,186 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:52:49,305 | INFO | aether2 | Epoch 364: Loss = 5.2556
2025-05-29 03:52:49,386 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:56:23,324 | INFO | aether2 | Epoch 365: Loss = 5.2559
2025-05-29 03:56:23,410 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 03:59:24,155 | INFO | aether2 | Epoch 366: Loss = 5.2565
2025-05-29 03:59:24,232 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:02:21,466 | INFO | aether2 | Epoch 367: Loss = 5.2564
2025-05-29 04:02:21,523 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:05:19,014 | INFO | aether2 | Epoch 368: Loss = 5.2551
2025-05-29 04:05:19,078 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:08:17,373 | INFO | aether2 | Epoch 369: Loss = 5.2563
2025-05-29 04:08:17,483 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:11:14,267 | INFO | aether2 | Epoch 370: Loss = 5.2568
2025-05-29 04:11:14,342 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:14:11,638 | INFO | aether2 | Epoch 371: Loss = 5.2558
2025-05-29 04:14:11,699 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:17:09,320 | INFO | aether2 | Epoch 372: Loss = 5.2553
2025-05-29 04:17:09,397 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:20:06,702 | INFO | aether2 | Epoch 373: Loss = 5.2557
2025-05-29 04:20:06,780 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:23:03,567 | INFO | aether2 | Epoch 374: Loss = 5.2565
2025-05-29 04:23:03,635 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:26:01,096 | INFO | aether2 | Epoch 375: Loss = 5.2542
2025-05-29 04:26:01,160 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:28:59,890 | INFO | aether2 | Epoch 376: Loss = 5.2524
2025-05-29 04:28:59,959 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:31:56,920 | INFO | aether2 | Epoch 377: Loss = 5.2574
2025-05-29 04:31:56,991 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:34:55,061 | INFO | aether2 | Epoch 378: Loss = 5.2529
2025-05-29 04:34:55,137 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:37:52,155 | INFO | aether2 | Epoch 379: Loss = 5.2534
2025-05-29 04:37:52,218 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:40:50,497 | INFO | aether2 | Epoch 380: Loss = 5.2551
2025-05-29 04:40:50,576 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:43:48,421 | INFO | aether2 | Epoch 381: Loss = 5.2556
2025-05-29 04:43:48,498 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:46:45,974 | INFO | aether2 | Epoch 382: Loss = 5.2567
2025-05-29 04:46:46,046 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:49:43,609 | INFO | aether2 | Epoch 383: Loss = 5.2554
2025-05-29 04:49:43,681 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:52:42,046 | INFO | aether2 | Epoch 384: Loss = 5.2559
2025-05-29 04:52:42,140 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:55:39,026 | INFO | aether2 | Epoch 385: Loss = 5.2578
2025-05-29 04:55:39,097 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 04:58:36,749 | INFO | aether2 | Epoch 386: Loss = 5.2572
2025-05-29 04:58:36,811 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:01:36,203 | INFO | aether2 | Epoch 387: Loss = 5.2521
2025-05-29 05:01:36,287 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:04:32,518 | INFO | aether2 | Epoch 388: Loss = 5.2574
2025-05-29 05:04:32,595 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:07:30,083 | INFO | aether2 | Epoch 389: Loss = 5.2558
2025-05-29 05:07:30,135 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:10:27,811 | INFO | aether2 | Epoch 390: Loss = 5.2563
2025-05-29 05:10:27,893 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:13:24,389 | INFO | aether2 | Epoch 391: Loss = 5.2575
2025-05-29 05:13:24,466 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:16:21,911 | INFO | aether2 | Epoch 392: Loss = 5.2574
2025-05-29 05:16:21,981 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:19:19,825 | INFO | aether2 | Epoch 393: Loss = 5.2559
2025-05-29 05:19:19,889 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:22:16,627 | INFO | aether2 | Epoch 394: Loss = 5.2563
2025-05-29 05:22:16,692 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:25:14,223 | INFO | aether2 | Epoch 395: Loss = 5.2533
2025-05-29 05:25:14,287 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:28:12,013 | INFO | aether2 | Epoch 396: Loss = 5.2546
2025-05-29 05:28:12,105 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:31:10,529 | INFO | aether2 | Epoch 397: Loss = 5.2546
2025-05-29 05:31:10,609 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:34:08,465 | INFO | aether2 | Epoch 398: Loss = 5.2557
2025-05-29 05:34:08,533 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:37:06,447 | INFO | aether2 | Epoch 399: Loss = 5.2558
2025-05-29 05:37:06,524 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:40:04,411 | INFO | aether2 | Epoch 400: Loss = 5.2550
2025-05-29 05:40:04,481 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:43:01,621 | INFO | aether2 | Epoch 401: Loss = 5.2570
2025-05-29 05:43:01,698 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:46:00,198 | INFO | aether2 | Epoch 402: Loss = 5.2571
2025-05-29 05:46:00,270 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:48:56,767 | INFO | aether2 | Epoch 403: Loss = 5.2580
2025-05-29 05:48:56,838 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:51:54,363 | INFO | aether2 | Epoch 404: Loss = 5.2564
2025-05-29 05:51:54,438 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:54:51,507 | INFO | aether2 | Epoch 405: Loss = 5.2563
2025-05-29 05:54:51,579 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 05:57:49,749 | INFO | aether2 | Epoch 406: Loss = 5.2564
2025-05-29 05:57:49,820 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:00:47,324 | INFO | aether2 | Epoch 407: Loss = 5.2565
2025-05-29 06:00:47,390 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:03:45,999 | INFO | aether2 | Epoch 408: Loss = 5.2536
2025-05-29 06:03:46,065 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:06:45,217 | INFO | aether2 | Epoch 409: Loss = 5.2534
2025-05-29 06:06:45,291 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:09:42,521 | INFO | aether2 | Epoch 410: Loss = 5.2551
2025-05-29 06:09:42,594 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:12:40,085 | INFO | aether2 | Epoch 411: Loss = 5.2551
2025-05-29 06:12:40,144 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:15:38,867 | INFO | aether2 | Epoch 412: Loss = 5.2529
2025-05-29 06:15:38,936 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:18:37,155 | INFO | aether2 | Epoch 413: Loss = 5.2539
2025-05-29 06:18:37,233 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:21:34,702 | INFO | aether2 | Epoch 414: Loss = 5.2552
2025-05-29 06:21:34,785 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:24:32,956 | INFO | aether2 | Epoch 415: Loss = 5.2547
2025-05-29 06:24:33,027 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:27:29,621 | INFO | aether2 | Epoch 416: Loss = 5.2568
2025-05-29 06:27:29,714 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:30:27,268 | INFO | aether2 | Epoch 417: Loss = 5.2539
2025-05-29 06:30:27,361 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:33:24,874 | INFO | aether2 | Epoch 418: Loss = 5.2554
2025-05-29 06:33:24,951 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:36:22,596 | INFO | aether2 | Epoch 419: Loss = 5.2558
2025-05-29 06:36:22,692 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:39:20,272 | INFO | aether2 | Epoch 420: Loss = 5.2543
2025-05-29 06:39:20,342 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:42:18,480 | INFO | aether2 | Epoch 421: Loss = 5.2558
2025-05-29 06:42:18,561 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:45:16,354 | INFO | aether2 | Epoch 422: Loss = 5.2562
2025-05-29 06:45:16,429 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:48:15,030 | INFO | aether2 | Epoch 423: Loss = 5.2565
2025-05-29 06:48:15,084 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:51:13,755 | INFO | aether2 | Epoch 424: Loss = 5.2545
2025-05-29 06:51:13,831 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:54:11,977 | INFO | aether2 | Epoch 425: Loss = 5.2552
2025-05-29 06:54:12,056 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 06:57:09,534 | INFO | aether2 | Epoch 426: Loss = 5.2550
2025-05-29 06:57:09,610 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:00:08,196 | INFO | aether2 | Epoch 427: Loss = 5.2544
2025-05-29 07:00:08,267 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:03:06,264 | INFO | aether2 | Epoch 428: Loss = 5.2548
2025-05-29 07:03:06,336 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:06:03,846 | INFO | aether2 | Epoch 429: Loss = 5.2568
2025-05-29 07:06:03,915 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:09:01,288 | INFO | aether2 | Epoch 430: Loss = 5.2567
2025-05-29 07:09:01,362 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:11:59,321 | INFO | aether2 | Epoch 431: Loss = 5.2556
2025-05-29 07:11:59,391 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:14:57,586 | INFO | aether2 | Epoch 432: Loss = 5.2546
2025-05-29 07:14:57,667 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:17:54,598 | INFO | aether2 | Epoch 433: Loss = 5.2579
2025-05-29 07:17:54,664 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:20:51,687 | INFO | aether2 | Epoch 434: Loss = 5.2555
2025-05-29 07:20:51,753 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:23:49,139 | INFO | aether2 | Epoch 435: Loss = 5.2565
2025-05-29 07:23:49,228 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:26:46,447 | INFO | aether2 | Epoch 436: Loss = 5.2554
2025-05-29 07:26:46,512 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:29:45,171 | INFO | aether2 | Epoch 437: Loss = 5.2536
2025-05-29 07:29:45,242 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:32:45,195 | INFO | aether2 | Epoch 438: Loss = 5.2571
2025-05-29 07:32:45,288 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:35:42,245 | INFO | aether2 | Epoch 439: Loss = 5.2531
2025-05-29 07:35:42,313 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:38:38,701 | INFO | aether2 | Epoch 440: Loss = 5.2551
2025-05-29 07:38:38,781 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:41:35,557 | INFO | aether2 | Epoch 441: Loss = 5.2557
2025-05-29 07:41:35,628 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:44:32,204 | INFO | aether2 | Epoch 442: Loss = 5.2546
2025-05-29 07:44:32,269 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:47:28,118 | INFO | aether2 | Epoch 443: Loss = 5.2583
2025-05-29 07:47:28,180 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:50:23,530 | INFO | aether2 | Epoch 444: Loss = 5.2590
2025-05-29 07:50:23,592 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:53:20,093 | INFO | aether2 | Epoch 445: Loss = 5.2556
2025-05-29 07:53:20,170 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:56:16,094 | INFO | aether2 | Epoch 446: Loss = 5.2549
2025-05-29 07:56:16,176 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 07:59:14,086 | INFO | aether2 | Epoch 447: Loss = 5.2561
2025-05-29 07:59:14,155 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 08:02:11,640 | INFO | aether2 | Epoch 448: Loss = 5.2557
2025-05-29 08:02:11,702 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 08:05:08,452 | INFO | aether2 | Epoch 449: Loss = 5.2545
2025-05-29 08:05:08,529 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 08:08:04,249 | INFO | aether2 | Epoch 450: Loss = 5.2566
2025-05-29 08:08:04,307 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-29 21:52:40,496 | INFO | aether2 | Vocab loaded with size: 184
2025-05-29 21:52:40,497 | INFO | aether2 | Token embedding initialized with vocab size: 184
2025-05-29 21:52:40,578 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 21:52:40,578 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 21:52:40,612 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 21:52:40,648 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 21:52:40,649 | INFO | aether2 | Using vocab_size = 184 for model initialization.
2025-05-29 21:52:43,825 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 21:52:43,952 | INFO | aether2 | Tokenizer vocab loaded from file with size 184
2025-05-29 21:52:43,952 | INFO | aether2 | Token embedding updated with vocab size 184
2025-05-29 21:52:44,077 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 21:52:44,077 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 21:52:44,095 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 21:52:44,095 | INFO | aether2 | Starting model training for 1000 epochs...
2025-05-29 21:53:34,062 | INFO | aether2 | Vocab loaded with size: 184
2025-05-29 21:53:34,065 | INFO | aether2 | Token embedding initialized with vocab size: 184
2025-05-29 21:53:34,150 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 21:53:34,151 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 21:53:34,185 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 21:53:34,212 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 21:53:34,214 | INFO | aether2 | Using vocab_size = 184 for model initialization.
2025-05-29 21:53:39,523 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 21:53:39,631 | INFO | aether2 | Tokenizer vocab loaded from file with size 184
2025-05-29 21:53:39,632 | INFO | aether2 | Token embedding updated with vocab size 184
2025-05-29 21:53:39,769 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 21:53:39,770 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 21:53:39,793 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 21:53:39,794 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 21:54:00,374 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 21:54:00,379 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 21:54:00,451 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 21:54:17,995 | INFO | aether2 | GOAL: how do i loop code in C++
2025-05-29 21:54:17,998 | INFO | aether2 | Tokenized input: [1, 94, 170, 73, 171, 172, 173, 174, 2]
2025-05-29 21:54:18,016 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 21:54:41,322 | INFO | aether2 | GOAL: in sweden there is a town named s漆erhamn
2025-05-29 21:54:41,355 | INFO | aether2 | Tokenized input: [1, 173, 98, 184, 50, 80, 168, 181, 185, 2]
2025-05-29 21:54:41,355 | ERROR | aether2 | Token index 185 is out of range.
2025-05-29 22:01:59,667 | INFO | aether2 | Vocab loaded with size: 186
2025-05-29 22:01:59,674 | INFO | aether2 | Token embedding initialized with vocab size: 186
2025-05-29 22:01:59,705 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:01:59,705 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:01:59,725 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:01:59,743 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:01:59,744 | INFO | aether2 | Using vocab_size = 186 for model initialization.
2025-05-29 22:02:02,138 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:02:02,239 | INFO | aether2 | Tokenizer vocab loaded from file with size 186
2025-05-29 22:02:02,239 | INFO | aether2 | Token embedding updated with vocab size 186
2025-05-29 22:02:02,363 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:02:02,363 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:02:02,376 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:02:02,382 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:02:33,734 | INFO | aether2 | GOAL: give me a name of a town ?
2025-05-29 22:02:33,771 | INFO | aether2 | Tokenized input: [1, 186, 187, 80, 154, 159, 80, 168, 2]
2025-05-29 22:02:33,771 | ERROR | aether2 | Token index 187 is out of range.
2025-05-29 22:06:37,702 | INFO | aether2 | Vocab loaded with size: 188
2025-05-29 22:06:37,703 | INFO | aether2 | Token embedding initialized with vocab size: 188
2025-05-29 22:06:37,746 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:06:37,747 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:06:37,768 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:06:37,792 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:06:37,794 | INFO | aether2 | Using vocab_size = 188 for model initialization.
2025-05-29 22:06:41,327 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:06:41,425 | INFO | aether2 | Tokenizer vocab loaded from file with size 188
2025-05-29 22:06:41,426 | INFO | aether2 | Token embedding updated with vocab size 188
2025-05-29 22:06:41,539 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:06:41,539 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:06:41,559 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:06:41,559 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:06:58,052 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 22:06:58,057 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 22:06:58,079 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 22:07:31,183 | INFO | aether2 | GOAL: the name of the capital of sweden is stockholm
2025-05-29 22:07:31,194 | INFO | aether2 | Tokenized input: [1, 77, 154, 159, 77, 158, 159, 98, 50, 188, 2]
2025-05-29 22:07:31,201 | ERROR | aether2 | Token index 188 is out of range.
2025-05-29 22:09:19,975 | INFO | aether2 | Vocab loaded with size: 189
2025-05-29 22:09:19,975 | INFO | aether2 | Token embedding initialized with vocab size: 189
2025-05-29 22:09:20,005 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:09:20,005 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:09:20,019 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:09:20,035 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:09:20,037 | INFO | aether2 | Using vocab_size = 189 for model initialization.
2025-05-29 22:09:22,415 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:09:22,515 | INFO | aether2 | Tokenizer vocab loaded from file with size 189
2025-05-29 22:09:22,515 | INFO | aether2 | Token embedding updated with vocab size 189
2025-05-29 22:09:22,637 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:09:22,637 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:09:22,655 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:09:22,656 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:10:41,920 | INFO | aether2 | Vocab loaded with size: 189
2025-05-29 22:10:41,921 | INFO | aether2 | Token embedding initialized with vocab size: 189
2025-05-29 22:10:41,945 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:10:41,945 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:10:41,959 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:10:41,974 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:10:41,975 | INFO | aether2 | Using vocab_size = 189 for model initialization.
2025-05-29 22:10:44,256 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:10:44,361 | INFO | aether2 | Tokenizer vocab loaded from file with size 189
2025-05-29 22:10:44,362 | INFO | aether2 | Token embedding updated with vocab size 189
2025-05-29 22:10:44,487 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:10:44,487 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:10:44,502 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:10:44,503 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:11:11,212 | INFO | aether2 | GOAL: hello whats your name ?
2025-05-29 22:11:11,233 | INFO | aether2 | Tokenized input: [1, 189, 153, 134, 154, 2]
2025-05-29 22:11:11,234 | ERROR | aether2 | Token index 189 is out of range.
2025-05-29 22:13:27,981 | INFO | aether2 | Vocab loaded with size: 190
2025-05-29 22:13:27,981 | INFO | aether2 | Token embedding initialized with vocab size: 190
2025-05-29 22:13:28,009 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:13:28,009 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:13:28,026 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:13:28,044 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:13:28,044 | INFO | aether2 | Using vocab_size = 190 for model initialization.
2025-05-29 22:13:30,381 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:13:30,480 | INFO | aether2 | Tokenizer vocab loaded from file with size 190
2025-05-29 22:13:30,480 | INFO | aether2 | Token embedding updated with vocab size 190
2025-05-29 22:13:30,602 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:13:30,603 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:13:30,619 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:13:30,621 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:14:00,993 | INFO | aether2 | GOAL: give me a name of a street in sweden ?
2025-05-29 22:14:01,014 | INFO | aether2 | Tokenized input: [1, 186, 187, 80, 154, 159, 80, 190, 173, 98, 2]
2025-05-29 22:14:01,019 | ERROR | aether2 | Token index 190 is out of range.
2025-05-29 22:15:11,957 | INFO | aether2 | Vocab loaded with size: 191
2025-05-29 22:15:11,957 | INFO | aether2 | Token embedding initialized with vocab size: 191
2025-05-29 22:15:11,986 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:15:11,986 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:15:12,003 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:15:12,022 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:15:12,023 | INFO | aether2 | Using vocab_size = 191 for model initialization.
2025-05-29 22:15:14,270 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:15:14,383 | INFO | aether2 | Tokenizer vocab loaded from file with size 191
2025-05-29 22:15:14,383 | INFO | aether2 | Token embedding updated with vocab size 191
2025-05-29 22:15:14,499 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:15:14,500 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:15:14,540 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:15:14,540 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:15:18,691 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 22:15:18,695 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 22:15:18,727 | INFO | aether2 | Modellen genererade <EOS>, stoppar.
2025-05-29 22:15:40,684 | INFO | aether2 | GOAL: how do you feel give me a anser good or bad
2025-05-29 22:15:40,758 | INFO | aether2 | Tokenized input: [1, 94, 170, 87, 191, 186, 187, 80, 192, 193, 194, 195, 2]
2025-05-29 22:15:40,758 | ERROR | aether2 | Token index 195 is out of range.
2025-05-29 22:18:51,833 | INFO | aether2 | Vocab loaded with size: 196
2025-05-29 22:18:51,834 | INFO | aether2 | Token embedding initialized with vocab size: 196
2025-05-29 22:18:51,864 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:18:51,864 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:18:51,880 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:18:51,898 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:18:51,899 | INFO | aether2 | Using vocab_size = 196 for model initialization.
2025-05-29 22:18:54,408 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:18:54,514 | INFO | aether2 | Tokenizer vocab loaded from file with size 196
2025-05-29 22:18:54,514 | INFO | aether2 | Token embedding updated with vocab size 196
2025-05-29 22:18:54,626 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:18:54,626 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:18:54,647 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:18:54,647 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:19:19,677 | INFO | aether2 | GOAL: give me names ?
2025-05-29 22:19:19,695 | INFO | aether2 | Tokenized input: [1, 186, 187, 196, 2]
2025-05-29 22:19:19,695 | ERROR | aether2 | Token index 196 exceeds allowed vocabulary size (196). Using <UNK> token.
2025-05-29 22:21:59,792 | INFO | aether2 | Vocab loaded with size: 197
2025-05-29 22:21:59,792 | INFO | aether2 | Token embedding initialized with vocab size: 197
2025-05-29 22:21:59,820 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:21:59,820 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:21:59,837 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:21:59,857 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:21:59,858 | INFO | aether2 | Using vocab_size = 197 for model initialization.
2025-05-29 22:22:02,243 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:22:02,348 | INFO | aether2 | Tokenizer vocab loaded from file with size 197
2025-05-29 22:22:02,352 | INFO | aether2 | Token embedding updated with vocab size 197
2025-05-29 22:22:02,482 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:22:02,482 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:22:02,495 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:22:02,501 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:22:08,758 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 22:22:08,759 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 22:22:34,402 | INFO | aether2 | GOAL: give me two names ?
2025-05-29 22:22:34,419 | INFO | aether2 | Tokenized input: [1, 186, 187, 197, 196, 2]
2025-05-29 22:22:34,419 | ERROR | aether2 | Token index 197 exceeds allowed vocabulary size (197). Using <UNK> token.
2025-05-29 22:24:47,677 | INFO | aether2 | Vocab loaded with size: 198
2025-05-29 22:24:47,677 | INFO | aether2 | Token embedding initialized with vocab size: 198
2025-05-29 22:24:47,709 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:24:47,710 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:24:47,728 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:24:47,745 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:24:47,747 | INFO | aether2 | Using vocab_size = 198 for model initialization.
2025-05-29 22:24:50,270 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:24:50,382 | INFO | aether2 | Tokenizer vocab loaded from file with size 198
2025-05-29 22:24:50,383 | INFO | aether2 | Token embedding updated with vocab size 198
2025-05-29 22:24:50,496 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:24:50,496 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:24:50,515 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:24:50,516 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:25:10,347 | INFO | aether2 | GOAL: give me names of towns in sweden
2025-05-29 22:25:10,364 | INFO | aether2 | Tokenized input: [1, 186, 187, 196, 159, 198, 173, 98, 2]
2025-05-29 22:25:10,364 | ERROR | aether2 | Token index 198 exceeds allowed vocabulary size (198). Using <UNK> token.
2025-05-29 22:29:04,915 | INFO | aether2 | Vocab loaded with size: 199
2025-05-29 22:29:04,915 | INFO | aether2 | Token embedding initialized with vocab size: 199
2025-05-29 22:29:04,947 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:29:04,947 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:29:04,962 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:29:04,972 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:29:04,979 | INFO | aether2 | Using vocab_size = 199 for model initialization.
2025-05-29 22:29:07,312 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:29:07,423 | INFO | aether2 | Tokenizer vocab loaded from file with size 199
2025-05-29 22:29:07,424 | INFO | aether2 | Token embedding updated with vocab size 199
2025-05-29 22:29:07,574 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:29:07,576 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:29:07,595 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:29:07,597 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:29:13,342 | INFO | aether2 | GOAL: what압 is the capital of sweden ?
2025-05-29 22:29:13,342 | INFO | aether2 | Tokenized input: [1, 156, 157, 50, 77, 158, 159, 98, 2]
2025-05-29 22:29:27,403 | INFO | aether2 | GOAL: whats your name ?
2025-05-29 22:29:27,403 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-29 22:30:16,438 | INFO | aether2 | GOAL: The sun is shining brightly today.
2025-05-29 22:30:16,502 | INFO | aether2 | Tokenized input: [1, 77, 199, 50, 200, 201, 202, 2]
2025-05-29 22:30:16,502 | ERROR | aether2 | Token index 202 exceeds allowed vocabulary size (199). Using <UNK> token.
2025-05-29 22:30:50,028 | INFO | aether2 | GOAL: I love reading books in my free time.
2025-05-29 22:30:50,028 | WARNING | aether2 | Updating token embedding layer to match new vocab size (203)
2025-05-29 22:30:50,132 | INFO | aether2 | Tokenized input: [1, 73, 203, 204, 205, 173, 51, 206, 96, 2]
2025-05-29 22:30:50,132 | ERROR | aether2 | Token index 206 exceeds allowed vocabulary size (203). Using <UNK> token.
2025-05-29 22:31:27,635 | INFO | aether2 | Vocab loaded with size: 207
2025-05-29 22:31:27,635 | INFO | aether2 | Token embedding initialized with vocab size: 207
2025-05-29 22:31:27,662 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:31:27,662 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:31:27,690 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:31:27,714 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:31:27,715 | INFO | aether2 | Using vocab_size = 207 for model initialization.
2025-05-29 22:31:30,005 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:31:30,108 | INFO | aether2 | Tokenizer vocab loaded from file with size 207
2025-05-29 22:31:30,109 | INFO | aether2 | Token embedding updated with vocab size 207
2025-05-29 22:31:30,220 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:31:30,220 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:31:30,238 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:31:30,239 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:31:40,246 | INFO | aether2 | GOAL: We had pizza for dinner last night.
2025-05-29 22:31:40,322 | INFO | aether2 | Tokenized input: [1, 138, 207, 208, 114, 209, 210, 211, 2]
2025-05-29 22:31:40,323 | ERROR | aether2 | Token index 211 exceeds allowed vocabulary size (207). Using <UNK> token.
2025-05-29 22:37:45,103 | INFO | aether2 | Vocab loaded with size: 212
2025-05-29 22:37:45,103 | INFO | aether2 | Token embedding initialized with vocab size: 212
2025-05-29 22:37:45,131 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:37:45,134 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:37:45,149 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:37:45,167 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:37:45,170 | INFO | aether2 | Using vocab_size = 212 for model initialization.
2025-05-29 22:37:47,600 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:37:47,704 | INFO | aether2 | Tokenizer vocab loaded from file with size 212
2025-05-29 22:37:47,707 | INFO | aether2 | Token embedding updated with vocab size 212
2025-05-29 22:37:47,831 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:37:47,831 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:37:47,850 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:37:47,851 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:38:01,846 | INFO | aether2 | GOAL: My cat is sleeping on the sofa.
2025-05-29 22:38:01,911 | INFO | aether2 | Tokenized input: [1, 51, 212, 50, 213, 214, 77, 215, 2]
2025-05-29 22:38:01,912 | ERROR | aether2 | Token index 215 exceeds embedding size (212)
2025-05-29 22:38:30,034 | INFO | aether2 | GOAL: It often rains in the spring.
2025-05-29 22:38:30,034 | WARNING | aether2 | Updating token embedding and model to new vocab size (216)
2025-05-29 22:38:30,136 | INFO | aether2 | Model and embedding updated to match new vocab size.
2025-05-29 22:38:30,192 | INFO | aether2 | Tokenized input: [1, 167, 216, 217, 173, 77, 218, 2]
2025-05-29 22:38:30,193 | ERROR | aether2 | Token index 218 exceeds embedding size (216)
2025-05-29 22:38:55,455 | INFO | aether2 | Vocab loaded with size: 219
2025-05-29 22:38:55,455 | INFO | aether2 | Token embedding initialized with vocab size: 219
2025-05-29 22:38:55,485 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:38:55,485 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:38:55,503 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:38:55,518 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:38:55,519 | INFO | aether2 | Using vocab_size = 219 for model initialization.
2025-05-29 22:38:57,907 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:38:58,015 | INFO | aether2 | Tokenizer vocab loaded from file with size 219
2025-05-29 22:38:58,015 | INFO | aether2 | Token embedding updated with vocab size 219
2025-05-29 22:38:58,149 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:38:58,150 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:38:58,171 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:38:58,172 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:39:26,228 | INFO | aether2 | GOAL: I forgot my umbrella at the restaurant.
2025-05-29 22:39:26,302 | INFO | aether2 | Tokenized input: [1, 73, 219, 51, 220, 221, 77, 222, 2]
2025-05-29 22:39:26,302 | ERROR | aether2 | Token index 222 exceeds embedding size (219)
2025-05-29 22:39:58,226 | INFO | aether2 | GOAL: We listened to music during the long drive.
2025-05-29 22:39:58,227 | WARNING | aether2 | Updating token embedding and model to new vocab size (223)
2025-05-29 22:39:58,319 | INFO | aether2 | Model and embedding updated to match new vocab size.
2025-05-29 22:39:58,403 | INFO | aether2 | Tokenized input: [1, 138, 223, 97, 224, 225, 77, 95, 226, 2]
2025-05-29 22:39:58,404 | ERROR | aether2 | Token index 226 exceeds embedding size (223)
2025-05-29 22:42:29,791 | INFO | aether2 | Vocab loaded with size: 227
2025-05-29 22:42:29,791 | INFO | aether2 | Token embedding initialized with vocab size: 227
2025-05-29 22:42:29,827 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 22:42:29,827 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 22:42:29,845 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 22:42:29,864 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 22:42:29,865 | INFO | aether2 | Using vocab_size = 227 for model initialization.
2025-05-29 22:42:32,404 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 22:42:32,514 | INFO | aether2 | Tokenizer vocab loaded from file with size 227
2025-05-29 22:42:32,515 | INFO | aether2 | Token embedding updated with vocab size 227
2025-05-29 22:42:32,637 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 22:42:32,637 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 22:42:32,656 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 22:42:32,656 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 22:43:03,823 | INFO | aether2 | GOAL: They planted flowers in the garden.
2025-05-29 22:43:03,887 | INFO | aether2 | Tokenized input: [1, 227, 228, 229, 173, 77, 230, 2]
2025-05-29 22:43:03,888 | ERROR | aether2 | Token index 230 exceeds embedding size (227)
2025-05-29 23:05:39,709 | INFO | aether2 | Vocab loaded with size: 231
2025-05-29 23:05:39,709 | INFO | aether2 | Token embedding initialized with vocab size: 231
2025-05-29 23:05:39,739 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 23:05:39,739 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 23:05:39,754 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 23:05:39,773 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 23:05:39,774 | INFO | aether2 | Using vocab_size = 231 for model initialization.
2025-05-29 23:05:42,180 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 23:05:42,275 | INFO | aether2 | Tokenizer vocab loaded from file with size 231
2025-05-29 23:05:42,275 | INFO | aether2 | Token embedding updated with vocab size 231
2025-05-29 23:05:42,393 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 23:05:42,393 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 23:05:42,411 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 23:05:42,411 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 23:06:03,213 | INFO | aether2 | GOAL: He fixed his bike with some tools.
2025-05-29 23:06:03,315 | INFO | aether2 | Tokenized input: [1, 231, 232, 233, 234, 84, 235, 236, 2]
2025-05-29 23:06:03,316 | ERROR | aether2 | Token index 236 exceeds embedding size (231)
2025-05-29 23:06:29,854 | INFO | aether2 | GOAL: The ice cream melted in the sun.
2025-05-29 23:06:29,855 | WARNING | aether2 | Updating model and embeddings to vocab size 237
2025-05-29 23:06:30,005 | INFO | aether2 | Tokenized input: [1, 77, 237, 238, 239, 173, 77, 199, 2]
2025-05-29 23:06:30,006 | ERROR | aether2 | Token index 239 exceeds embedding size (237)
2025-05-29 23:09:57,197 | INFO | aether2 | GOAL: We watched the fireworks from the balcony.
2025-05-29 23:09:57,198 | WARNING | aether2 | Updating model and embeddings to vocab size 240
2025-05-29 23:09:57,361 | INFO | aether2 | Tokenized input: [1, 138, 240, 77, 241, 180, 77, 242, 2]
2025-05-29 23:09:57,363 | ERROR | aether2 | Token index 242 exceeds embedding size (240)
2025-05-29 23:23:24,468 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:23:24,468 | INFO | aether2 | Token embedding initialized with vocab size: 243
2025-05-29 23:23:24,499 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 23:23:24,500 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 23:23:24,517 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 23:23:24,538 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 23:23:24,538 | INFO | aether2 | Using vocab_size = 243 for model initialization.
2025-05-29 23:23:27,001 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 23:23:27,107 | INFO | aether2 | Tokenizer vocab loaded from file with size 243
2025-05-29 23:23:27,107 | INFO | aether2 | Token embedding updated with vocab size 243
2025-05-29 23:23:27,226 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 23:23:27,226 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 23:23:27,246 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 23:23:27,246 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 23:34:27,502 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:34:44,104 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:36:07,361 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:36:07,361 | INFO | aether2 | Token embedding initialized with vocab size: 243
2025-05-29 23:36:07,388 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 23:36:07,388 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 23:36:07,405 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 23:36:07,422 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 23:36:07,422 | INFO | aether2 | Using vocab_size = 243 for model initialization.
2025-05-29 23:36:09,742 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 23:36:09,840 | INFO | aether2 | Tokenizer vocab loaded from file with size 243
2025-05-29 23:36:09,841 | INFO | aether2 | Token embedding updated with vocab size 243
2025-05-29 23:36:09,948 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 23:36:09,948 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 23:36:09,963 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 23:36:09,963 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 23:42:09,999 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:44:09,422 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:44:09,508 | INFO | aether2 | Initialized StackedTransformer with vocab_size 243
2025-05-29 23:45:47,497 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:45:47,504 | INFO | aether2 | Token embedding initialized with vocab size: 243
2025-05-29 23:45:47,532 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 23:45:47,532 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 23:45:47,552 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 23:45:47,571 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 23:45:47,573 | INFO | aether2 | Using vocab_size = 243 for model initialization.
2025-05-29 23:45:49,960 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 23:45:50,065 | INFO | aether2 | Tokenizer vocab loaded from file with size 243
2025-05-29 23:45:50,066 | INFO | aether2 | Token embedding updated with vocab size 243
2025-05-29 23:45:50,192 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 23:45:50,192 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 23:45:50,211 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 23:45:50,212 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 23:50:12,946 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:50:12,947 | INFO | aether2 | Token embedding initialized with vocab size: 243
2025-05-29 23:50:12,973 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 23:50:12,973 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 23:50:12,987 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 23:50:13,000 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 23:50:13,001 | INFO | aether2 | Using vocab_size = 243 for model initialization.
2025-05-29 23:50:15,458 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 23:50:15,557 | INFO | aether2 | Tokenizer vocab loaded from file with size 243
2025-05-29 23:50:15,558 | INFO | aether2 | Token embedding updated with vocab size 243
2025-05-29 23:50:15,680 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 23:50:15,681 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 23:50:15,700 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 23:50:15,701 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 23:53:27,721 | INFO | aether2 | Vocab loaded with size: 243
2025-05-29 23:53:27,721 | INFO | aether2 | Token embedding initialized with vocab size: 243
2025-05-29 23:53:27,748 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-29 23:53:27,748 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-29 23:53:27,762 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-29 23:53:27,778 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-29 23:53:27,779 | INFO | aether2 | Using vocab_size = 243 for model initialization.
2025-05-29 23:53:30,202 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-29 23:53:30,313 | INFO | aether2 | Tokenizer vocab loaded from file with size 243
2025-05-29 23:53:30,314 | INFO | aether2 | Token embedding updated with vocab size 243
2025-05-29 23:53:30,436 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-29 23:53:30,436 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-29 23:53:30,458 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-29 23:53:30,458 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-29 23:55:00,833 | INFO | aether2 | GOAL: He found a coin under the table.
2025-05-29 23:55:00,899 | INFO | aether2 | Tokenized input: [1, 231, 243, 80, 244, 245, 77, 246, 2]
2025-05-29 23:55:00,899 | ERROR | aether2 | Token index 246 exceeds embedding size (243)
2025-05-29 23:55:08,281 | INFO | aether2 | GOAL: hi
2025-05-29 23:55:08,281 | WARNING | aether2 | Updating model and embeddings to vocab size 247
2025-05-29 23:55:08,378 | INFO | aether2 | Tokenized input: [1, 71, 2]
2025-05-30 17:27:30,411 | INFO | aether2 | Vocab loaded with size: 247
2025-05-30 17:27:30,412 | INFO | aether2 | Token embedding initialized with vocab size: 247
2025-05-30 17:27:30,481 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 17:27:30,481 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 17:27:30,515 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 17:27:30,550 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 17:27:30,552 | INFO | aether2 | Using vocab_size = 247 for model initialization.
2025-05-30 17:27:34,109 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 17:27:34,235 | INFO | aether2 | Tokenizer vocab loaded from file with size 247
2025-05-30 17:27:34,236 | INFO | aether2 | Token embedding updated with vocab size 247
2025-05-30 17:27:34,352 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 17:27:34,352 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 17:27:34,371 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 17:27:34,373 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-30 17:28:14,272 | INFO | aether2 | GOAL: The bus arrived ten minutes late.
2025-05-30 17:28:14,355 | INFO | aether2 | Tokenized input: [1, 77, 247, 248, 249, 250, 251, 2]
2025-05-30 17:28:14,355 | ERROR | aether2 | Token index 251 exceeds embedding size (247)
2025-05-30 17:30:26,465 | INFO | aether2 | Vocab loaded with size: 252
2025-05-30 17:30:26,466 | INFO | aether2 | Token embedding initialized with vocab size: 252
2025-05-30 17:30:26,497 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 17:30:26,497 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 17:30:26,515 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 17:30:26,534 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 17:30:26,535 | INFO | aether2 | Using vocab_size = 252 for model initialization.
2025-05-30 17:30:29,117 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 17:30:29,240 | INFO | aether2 | Tokenizer vocab loaded from file with size 252
2025-05-30 17:30:29,245 | INFO | aether2 | Token embedding updated with vocab size 252
2025-05-30 17:30:29,397 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 17:30:29,398 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 17:30:29,418 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 17:30:29,419 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-30 17:30:42,324 | INFO | aether2 | GOAL: He fixed his bike with some tools.
2025-05-30 17:30:42,326 | INFO | aether2 | Tokenized input: [1, 231, 232, 233, 234, 84, 235, 236, 2]
2025-05-30 17:31:05,995 | INFO | aether2 | GOAL: The movie was funny and exciting.
2025-05-30 17:31:06,063 | INFO | aether2 | Tokenized input: [1, 77, 252, 253, 254, 59, 255, 2]
2025-05-30 17:31:06,064 | WARNING | aether2 | Updating model and embeddings to vocab size 256 (max token index: 255)
2025-05-30 17:31:06,173 | INFO | aether2 | Model and embedding updated.
2025-05-30 17:32:31,576 | INFO | aether2 | GOAL: We traveled by train across the country.
2025-05-30 17:32:31,660 | INFO | aether2 | Tokenized input: [1, 138, 256, 257, 258, 259, 77, 260, 2]
2025-05-30 17:32:31,660 | WARNING | aether2 | Updating model and embeddings to vocab size 261 (max token index: 260)
2025-05-30 17:32:31,757 | INFO | aether2 | Model and embedding updated.
2025-05-30 17:42:35,167 | INFO | aether2 | Vocab loaded with size: 261
2025-05-30 17:42:35,168 | INFO | aether2 | Token embedding initialized with vocab size: 261
2025-05-30 17:42:35,197 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 17:42:35,197 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 17:42:35,213 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 17:42:35,231 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 17:42:35,588 | INFO | aether2 | Tokenizer updated and saved with 261 tokens.
2025-05-30 17:45:58,313 | INFO | aether2 | Vocab loaded with size: 261
2025-05-30 17:45:58,314 | INFO | aether2 | Token embedding initialized with vocab size: 261
2025-05-30 17:45:58,352 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 17:45:58,352 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 17:45:58,381 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 17:45:58,404 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 17:45:58,728 | INFO | aether2 | Tokenizer updated and saved with 261 tokens.
2025-05-30 17:45:58,729 | INFO | aether2 | Using vocab_size = 261 for model initialization.
2025-05-30 17:46:01,118 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 17:46:01,223 | INFO | aether2 | Tokenizer vocab loaded from file with size 261
2025-05-30 17:46:01,226 | INFO | aether2 | Token embedding updated with vocab size 261
2025-05-30 17:46:01,352 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 17:46:01,352 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 17:46:01,375 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 17:46:01,376 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-30 17:46:23,384 | INFO | aether2 | Vocab loaded with size: 261
2025-05-30 17:46:23,385 | INFO | aether2 | Token embedding initialized with vocab size: 261
2025-05-30 17:46:23,420 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 17:46:23,420 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 17:46:23,444 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 17:46:23,467 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 17:46:23,970 | INFO | aether2 | Tokenizer updated and saved with 261 tokens.
2025-05-30 17:46:23,971 | INFO | aether2 | Using vocab_size = 261 for model initialization.
2025-05-30 17:46:27,257 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 17:46:27,367 | INFO | aether2 | Tokenizer vocab loaded from file with size 261
2025-05-30 17:46:27,368 | INFO | aether2 | Token embedding updated with vocab size 261
2025-05-30 17:46:27,488 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 17:46:27,488 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 17:46:27,506 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 17:46:27,506 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-30 17:46:39,012 | INFO | aether2 | GOAL: Lightning flashed across the night sky.
2025-05-30 17:46:39,066 | INFO | aether2 | Tokenized input: [1, 261, 262, 259, 77, 211, 263, 2]
2025-05-30 17:46:39,066 | WARNING | aether2 | Updating model and embeddings to vocab size 264 (max token index: 263)
2025-05-30 17:46:39,180 | INFO | aether2 | Model and embedding updated.
2025-05-30 17:58:06,676 | INFO | aether2 | Vocab loaded with size: 264
2025-05-30 17:58:06,676 | INFO | aether2 | Token embedding initialized with vocab size: 264
2025-05-30 17:58:06,704 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 17:58:06,704 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 17:58:06,719 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 17:58:06,734 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 17:58:07,490 | INFO | aether2 | Tokenizer updated and saved with 266 tokens.
2025-05-30 17:58:07,490 | INFO | aether2 | Using vocab_size = 266 for model initialization.
2025-05-30 17:58:09,864 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 17:58:09,975 | INFO | aether2 | Tokenizer vocab loaded from file with size 266
2025-05-30 17:58:09,976 | INFO | aether2 | Token embedding updated with vocab size 266
2025-05-30 17:58:10,088 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 17:58:10,088 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 17:58:10,104 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 17:58:10,105 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-30 17:58:26,691 | INFO | aether2 | GOAL: She lit a candle when the lights went out.
2025-05-30 17:58:26,788 | INFO | aether2 | Tokenized input: [1, 266, 267, 80, 268, 269, 77, 270, 271, 151, 264, 2]
2025-05-30 17:58:26,788 | WARNING | aether2 | Updating model and embeddings to vocab size 272 (max token index: 271)
2025-05-30 17:58:26,892 | INFO | aether2 | Model and embedding updated.
2025-05-30 17:59:09,791 | INFO | aether2 | GOAL: The wind blew the leaves off the trees.
2025-05-30 17:59:09,878 | INFO | aether2 | Tokenized input: [1, 77, 272, 273, 77, 274, 275, 77, 276, 264, 2]
2025-05-30 17:59:09,878 | WARNING | aether2 | Updating model and embeddings to vocab size 277 (max token index: 276)
2025-05-30 17:59:09,984 | INFO | aether2 | Model and embedding updated.
2025-05-30 18:02:23,178 | INFO | aether2 | Vocab loaded with size: 277
2025-05-30 18:02:23,178 | INFO | aether2 | Token embedding initialized with vocab size: 277
2025-05-30 18:02:23,207 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 18:02:23,208 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 18:02:23,224 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 18:02:23,241 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 18:02:23,975 | INFO | aether2 | Tokenizer updated and saved with 277 tokens.
2025-05-30 18:02:23,975 | INFO | aether2 | Using vocab_size = 277 for model initialization.
2025-05-30 18:02:26,466 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 18:02:26,568 | INFO | aether2 | Tokenizer vocab loaded from file with size 277
2025-05-30 18:02:26,570 | INFO | aether2 | Token embedding updated with vocab size 277
2025-05-30 18:02:26,685 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 18:02:26,685 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 18:02:26,705 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 18:02:26,706 | WARNING | aether2 | start_epoch (451) is greater than total epochs (450). Skipping training.
2025-05-30 18:02:43,524 | INFO | aether2 | GOAL: She slipped on the wet floor.
2025-05-30 18:02:43,576 | INFO | aether2 | Tokenized input: [1, 266, 277, 214, 77, 278, 279, 264, 2]
2025-05-30 18:02:43,576 | WARNING | aether2 | Updating model and embeddings to vocab size 280 (max token index: 279)
2025-05-30 18:02:43,684 | INFO | aether2 | Model and embedding updated.
2025-05-30 18:05:20,452 | INFO | aether2 | Vocab loaded with size: 280
2025-05-30 18:05:20,454 | INFO | aether2 | Token embedding initialized with vocab size: 280
2025-05-30 18:05:20,491 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 18:05:20,492 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 18:05:20,507 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 18:05:20,522 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 18:05:21,249 | INFO | aether2 | Tokenizer updated and saved with 280 tokens.
2025-05-30 18:05:21,250 | INFO | aether2 | Using vocab_size = 280 for model initialization.
2025-05-30 18:05:23,682 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 18:05:23,789 | INFO | aether2 | Tokenizer vocab loaded from file with size 280
2025-05-30 18:05:23,790 | INFO | aether2 | Token embedding updated with vocab size 280
2025-05-30 18:05:23,907 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 18:05:23,907 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 451
2025-05-30 18:05:23,925 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 18:05:23,926 | INFO | aether2 | Starting model training for 600 epochs...
2025-05-30 18:09:14,858 | INFO | aether2 | Epoch 452: Loss = 5.7128
2025-05-30 18:09:14,947 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:13:04,076 | INFO | aether2 | Epoch 453: Loss = 5.7141
2025-05-30 18:13:04,154 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:16:52,207 | INFO | aether2 | Epoch 454: Loss = 5.7163
2025-05-30 18:16:52,298 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:20:52,708 | INFO | aether2 | Epoch 455: Loss = 5.7141
2025-05-30 18:20:52,791 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:24:57,225 | INFO | aether2 | Epoch 456: Loss = 5.7152
2025-05-30 18:24:57,311 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:28:59,316 | INFO | aether2 | Epoch 457: Loss = 5.7145
2025-05-30 18:28:59,400 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:33:00,204 | INFO | aether2 | Epoch 458: Loss = 5.7155
2025-05-30 18:33:00,282 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:36:59,951 | INFO | aether2 | Epoch 459: Loss = 5.7140
2025-05-30 18:37:00,042 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:41:01,643 | INFO | aether2 | Epoch 460: Loss = 5.7153
2025-05-30 18:41:01,725 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:45:05,176 | INFO | aether2 | Epoch 461: Loss = 5.7142
2025-05-30 18:45:05,260 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:49:03,325 | INFO | aether2 | Epoch 462: Loss = 5.7147
2025-05-30 18:49:03,407 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:53:06,656 | INFO | aether2 | Epoch 463: Loss = 5.7153
2025-05-30 18:53:06,783 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 18:57:14,086 | INFO | aether2 | Epoch 464: Loss = 5.7140
2025-05-30 18:57:14,178 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:01:13,882 | INFO | aether2 | Epoch 465: Loss = 5.7157
2025-05-30 19:01:13,962 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:05:11,282 | INFO | aether2 | Epoch 466: Loss = 5.7135
2025-05-30 19:05:11,358 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:09:14,846 | INFO | aether2 | Epoch 467: Loss = 5.7155
2025-05-30 19:09:14,938 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:13:24,942 | INFO | aether2 | Epoch 468: Loss = 5.7155
2025-05-30 19:13:25,022 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:17:22,658 | INFO | aether2 | Epoch 469: Loss = 5.7149
2025-05-30 19:17:22,732 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:21:19,940 | INFO | aether2 | Epoch 470: Loss = 5.7138
2025-05-30 19:21:20,033 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:25:25,497 | INFO | aether2 | Epoch 471: Loss = 5.7130
2025-05-30 19:25:25,577 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:29:33,813 | INFO | aether2 | Epoch 472: Loss = 5.7134
2025-05-30 19:29:33,899 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:33:27,909 | INFO | aether2 | Epoch 473: Loss = 5.7152
2025-05-30 19:33:27,993 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:37:29,336 | INFO | aether2 | Epoch 474: Loss = 5.7139
2025-05-30 19:37:29,421 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:41:30,492 | INFO | aether2 | Epoch 475: Loss = 5.7136
2025-05-30 19:41:30,599 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:45:27,342 | INFO | aether2 | Epoch 476: Loss = 5.7166
2025-05-30 19:45:27,429 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:49:26,837 | INFO | aether2 | Epoch 477: Loss = 5.7151
2025-05-30 19:49:26,912 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:53:29,457 | INFO | aether2 | Epoch 478: Loss = 5.7167
2025-05-30 19:53:29,535 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 19:57:23,316 | INFO | aether2 | Epoch 479: Loss = 5.7130
2025-05-30 19:57:23,393 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:01:18,508 | INFO | aether2 | Epoch 480: Loss = 5.7146
2025-05-30 20:01:18,586 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:05:04,923 | INFO | aether2 | Epoch 481: Loss = 5.7143
2025-05-30 20:05:05,006 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:08:53,646 | INFO | aether2 | Epoch 482: Loss = 5.7143
2025-05-30 20:08:53,732 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:12:37,573 | INFO | aether2 | Epoch 483: Loss = 5.7146
2025-05-30 20:12:37,650 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:16:23,005 | INFO | aether2 | Epoch 484: Loss = 5.7111
2025-05-30 20:16:23,095 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:20:10,062 | INFO | aether2 | Epoch 485: Loss = 5.7138
2025-05-30 20:20:10,139 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:23:54,504 | INFO | aether2 | Epoch 486: Loss = 5.7147
2025-05-30 20:23:54,581 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:27:39,632 | INFO | aether2 | Epoch 487: Loss = 5.7142
2025-05-30 20:27:39,714 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:31:27,627 | INFO | aether2 | Epoch 488: Loss = 5.7119
2025-05-30 20:31:27,706 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:35:13,747 | INFO | aether2 | Epoch 489: Loss = 5.7148
2025-05-30 20:35:13,838 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:38:59,908 | INFO | aether2 | Epoch 490: Loss = 5.7163
2025-05-30 20:38:59,982 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:42:42,626 | INFO | aether2 | Epoch 491: Loss = 5.7161
2025-05-30 20:42:42,705 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:46:28,351 | INFO | aether2 | Epoch 492: Loss = 5.7164
2025-05-30 20:46:28,441 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:50:36,671 | INFO | aether2 | Epoch 493: Loss = 5.7157
2025-05-30 20:50:36,752 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:54:35,840 | INFO | aether2 | Epoch 494: Loss = 5.7155
2025-05-30 20:54:35,926 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 20:58:38,807 | INFO | aether2 | Epoch 495: Loss = 5.7172
2025-05-30 20:58:38,892 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:02:38,870 | INFO | aether2 | Epoch 496: Loss = 5.7157
2025-05-30 21:02:38,953 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:06:39,640 | INFO | aether2 | Epoch 497: Loss = 5.7152
2025-05-30 21:06:39,730 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:10:37,381 | INFO | aether2 | Epoch 498: Loss = 5.7160
2025-05-30 21:10:37,460 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:14:31,425 | INFO | aether2 | Epoch 499: Loss = 5.7150
2025-05-30 21:14:31,517 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:18:25,294 | INFO | aether2 | Epoch 500: Loss = 5.7151
2025-05-30 21:18:25,378 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:22:36,025 | INFO | aether2 | Epoch 501: Loss = 5.7155
2025-05-30 21:22:36,104 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:26:28,895 | INFO | aether2 | Epoch 502: Loss = 5.7144
2025-05-30 21:26:28,980 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:30:26,119 | INFO | aether2 | Epoch 503: Loss = 5.7140
2025-05-30 21:30:26,201 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:34:23,218 | INFO | aether2 | Epoch 504: Loss = 5.7138
2025-05-30 21:34:23,307 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:38:14,497 | INFO | aether2 | Epoch 505: Loss = 5.7166
2025-05-30 21:38:14,576 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:42:19,358 | INFO | aether2 | Epoch 506: Loss = 5.7138
2025-05-30 21:42:19,438 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:46:18,961 | INFO | aether2 | Epoch 507: Loss = 5.7145
2025-05-30 21:46:19,070 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:50:07,735 | INFO | aether2 | Epoch 508: Loss = 5.7139
2025-05-30 21:50:07,810 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:54:01,207 | INFO | aether2 | Epoch 509: Loss = 5.7147
2025-05-30 21:54:01,291 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 21:56:17,463 | INFO | aether2 | Vocab loaded with size: 280
2025-05-30 21:56:17,464 | INFO | aether2 | Token embedding initialized with vocab size: 280
2025-05-30 21:56:17,496 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 21:56:17,496 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 21:56:17,511 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 21:56:17,529 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 21:56:18,216 | INFO | aether2 | Tokenizer updated and saved with 280 tokens.
2025-05-30 21:56:18,216 | INFO | aether2 | Using vocab_size = 280 for model initialization.
2025-05-30 21:56:20,699 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 21:56:20,812 | INFO | aether2 | Tokenizer vocab loaded from file with size 280
2025-05-30 21:56:20,813 | INFO | aether2 | Token embedding updated with vocab size 280
2025-05-30 21:56:20,931 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 21:56:20,931 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 510
2025-05-30 21:56:20,951 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 21:56:20,952 | INFO | aether2 | Starting model training for 600 epochs...
2025-05-30 22:00:15,837 | INFO | aether2 | Epoch 511: Loss = 5.7803
2025-05-30 22:00:15,921 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:04:16,282 | INFO | aether2 | Epoch 512: Loss = 5.7800
2025-05-30 22:04:16,356 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:08:16,326 | INFO | aether2 | Epoch 513: Loss = 5.7788
2025-05-30 22:08:16,405 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:12:14,235 | INFO | aether2 | Epoch 514: Loss = 5.7806
2025-05-30 22:12:14,318 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:16:12,872 | INFO | aether2 | Epoch 515: Loss = 5.7787
2025-05-30 22:16:12,962 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:20:14,087 | INFO | aether2 | Epoch 516: Loss = 5.7804
2025-05-30 22:20:14,168 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:24:15,673 | INFO | aether2 | Epoch 517: Loss = 5.7771
2025-05-30 22:24:15,749 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:28:19,409 | INFO | aether2 | Epoch 518: Loss = 5.7795
2025-05-30 22:28:19,487 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:32:16,302 | INFO | aether2 | Epoch 519: Loss = 5.7787
2025-05-30 22:32:16,381 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:36:15,501 | INFO | aether2 | Epoch 520: Loss = 5.7809
2025-05-30 22:36:15,592 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:40:18,070 | INFO | aether2 | Epoch 521: Loss = 5.7817
2025-05-30 22:40:18,152 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:44:19,983 | INFO | aether2 | Epoch 522: Loss = 5.7797
2025-05-30 22:44:20,060 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:48:25,648 | INFO | aether2 | Epoch 523: Loss = 5.7806
2025-05-30 22:48:25,726 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:52:28,141 | INFO | aether2 | Epoch 524: Loss = 5.7803
2025-05-30 22:52:28,217 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 22:56:17,812 | INFO | aether2 | Epoch 525: Loss = 5.7807
2025-05-30 22:56:17,895 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 23:00:22,822 | INFO | aether2 | Epoch 526: Loss = 5.7790
2025-05-30 23:00:22,902 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 23:04:26,160 | INFO | aether2 | Epoch 527: Loss = 5.7793
2025-05-30 23:04:26,256 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 23:08:30,797 | INFO | aether2 | Epoch 528: Loss = 5.7800
2025-05-30 23:08:30,874 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 23:12:39,354 | INFO | aether2 | Epoch 529: Loss = 5.7798
2025-05-30 23:12:39,467 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 23:16:43,206 | INFO | aether2 | Epoch 530: Loss = 5.7791
2025-05-30 23:16:43,285 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-30 23:26:16,499 | INFO | aether2 | Vocab loaded with size: 280
2025-05-30 23:26:16,502 | INFO | aether2 | Token embedding initialized with vocab size: 280
2025-05-30 23:26:16,557 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:26:16,557 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:26:16,584 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:26:16,616 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:26:17,824 | INFO | aether2 | Tokenizer updated and saved with 280 tokens.
2025-05-30 23:26:17,826 | INFO | aether2 | Using vocab_size = 280 for model initialization.
2025-05-30 23:26:21,478 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:26:21,651 | INFO | aether2 | Tokenizer vocab loaded from file with size 280
2025-05-30 23:26:21,652 | INFO | aether2 | Token embedding updated with vocab size 280
2025-05-30 23:26:21,792 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:26:21,792 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:26:21,816 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:26:21,817 | WARNING | aether2 | start_epoch (531) is greater than total epochs (200). Skipping training.
2025-05-30 23:27:09,442 | INFO | aether2 | Vocab loaded with size: 280
2025-05-30 23:27:09,443 | INFO | aether2 | Token embedding initialized with vocab size: 280
2025-05-30 23:27:09,480 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:27:09,481 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:27:09,496 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:27:09,513 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:27:10,206 | INFO | aether2 | Tokenizer updated and saved with 280 tokens.
2025-05-30 23:27:10,207 | INFO | aether2 | Using vocab_size = 280 for model initialization.
2025-05-30 23:27:12,643 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:27:12,743 | INFO | aether2 | Tokenizer vocab loaded from file with size 280
2025-05-30 23:27:12,744 | INFO | aether2 | Token embedding updated with vocab size 280
2025-05-30 23:27:12,876 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:27:12,876 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:27:12,898 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:27:12,899 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:29:17,840 | INFO | aether2 | GOAL: tell me a question ?
2025-05-30 23:29:17,876 | INFO | aether2 | Tokenized input: [1, 280, 187, 80, 281, 2]
2025-05-30 23:29:17,876 | WARNING | aether2 | Updating model and embeddings to vocab size 282 (max token index: 281)
2025-05-30 23:29:17,984 | INFO | aether2 | Model and embedding updated.
2025-05-30 23:38:40,526 | INFO | aether2 | Vocab loaded with size: 282
2025-05-30 23:38:40,527 | INFO | aether2 | Token embedding initialized with vocab size: 282
2025-05-30 23:38:40,830 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:38:40,830 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:38:40,847 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:38:40,863 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:38:41,567 | INFO | aether2 | Tokenizer updated and saved with 282 tokens.
2025-05-30 23:38:41,568 | INFO | aether2 | Using vocab_size = 282 for model initialization.
2025-05-30 23:38:44,152 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:38:44,268 | INFO | aether2 | Tokenizer vocab loaded from file with size 282
2025-05-30 23:38:44,269 | INFO | aether2 | Token embedding updated with vocab size 282
2025-05-30 23:38:44,418 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:38:44,418 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:38:44,448 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:38:44,449 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:39:00,399 | INFO | aether2 | Vocab loaded with size: 282
2025-05-30 23:39:00,400 | INFO | aether2 | Token embedding initialized with vocab size: 282
2025-05-30 23:39:00,660 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:39:00,660 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:39:00,677 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:39:00,696 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:39:01,422 | INFO | aether2 | Tokenizer updated and saved with 282 tokens.
2025-05-30 23:39:01,422 | INFO | aether2 | Using vocab_size = 282 for model initialization.
2025-05-30 23:39:03,891 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:39:03,990 | INFO | aether2 | Tokenizer vocab loaded from file with size 282
2025-05-30 23:39:03,991 | INFO | aether2 | Token embedding updated with vocab size 282
2025-05-30 23:39:04,103 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:39:04,103 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:39:04,125 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:39:04,125 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:41:41,045 | INFO | aether2 | Vocab loaded with size: 282
2025-05-30 23:41:41,046 | INFO | aether2 | Token embedding initialized with vocab size: 282
2025-05-30 23:41:41,073 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:41:41,074 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:41:41,088 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:41:41,106 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:41:41,811 | INFO | aether2 | Tokenizer updated and saved with 282 tokens.
2025-05-30 23:41:41,812 | INFO | aether2 | Using vocab_size = 282 for model initialization.
2025-05-30 23:41:44,249 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:41:44,350 | INFO | aether2 | Tokenizer vocab loaded from file with size 282
2025-05-30 23:41:44,351 | INFO | aether2 | Token embedding updated with vocab size 282
2025-05-30 23:41:44,467 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:41:44,467 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:41:44,489 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:41:44,489 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:43:56,566 | INFO | aether2 | Vocab loaded with size: 282
2025-05-30 23:43:56,567 | INFO | aether2 | Token embedding initialized with vocab size: 282
2025-05-30 23:43:56,591 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:43:56,591 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:43:56,610 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:43:56,625 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:43:57,315 | INFO | aether2 | Tokenizer updated and saved with 282 tokens.
2025-05-30 23:43:57,316 | INFO | aether2 | Using vocab_size = 282 for model initialization.
2025-05-30 23:43:59,732 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:43:59,849 | INFO | aether2 | Tokenizer vocab loaded from file with size 282
2025-05-30 23:43:59,850 | INFO | aether2 | Token embedding updated with vocab size 282
2025-05-30 23:43:59,985 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:43:59,985 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:44:00,004 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:44:00,005 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:45:22,358 | INFO | aether2 | Vocab loaded with size: 282
2025-05-30 23:45:22,359 | INFO | aether2 | Token embedding initialized with vocab size: 282
2025-05-30 23:45:22,390 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:45:22,390 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:45:22,409 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:45:22,426 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:45:23,286 | INFO | aether2 | Tokenizer updated and saved with 282 tokens.
2025-05-30 23:45:23,287 | INFO | aether2 | Using vocab_size = 282 for model initialization.
2025-05-30 23:45:25,653 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:45:25,752 | INFO | aether2 | Tokenizer vocab loaded from file with size 282
2025-05-30 23:45:25,753 | INFO | aether2 | Token embedding updated with vocab size 282
2025-05-30 23:45:25,867 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:45:25,867 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:45:25,886 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:45:25,887 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:47:46,832 | INFO | aether2 | Vocab loaded with size: 282
2025-05-30 23:47:46,833 | INFO | aether2 | Token embedding initialized with vocab size: 282
2025-05-30 23:47:46,861 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:47:46,862 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:47:46,877 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:47:46,897 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:47:47,591 | INFO | aether2 | Tokenizer updated and saved with 282 tokens.
2025-05-30 23:47:47,591 | INFO | aether2 | Using vocab_size = 282 for model initialization.
2025-05-30 23:47:50,197 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:47:50,371 | INFO | aether2 | Tokenizer vocab loaded from file with size 282
2025-05-30 23:47:50,372 | INFO | aether2 | Token embedding updated with vocab size 282
2025-05-30 23:47:50,484 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:47:50,484 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:47:50,506 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:47:50,506 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:48:18,138 | INFO | aether2 | GOAL: how was your day today ?
2025-05-30 23:48:18,157 | INFO | aether2 | Tokenized input: [1, 94, 253, 134, 282, 202, 2]
2025-05-30 23:48:18,157 | WARNING | aether2 | Updating model and embeddings to vocab size 283 (max token index: 282)
2025-05-30 23:48:18,269 | INFO | aether2 | Model and embedding updated.
2025-05-30 23:49:55,011 | INFO | aether2 | Vocab loaded with size: 283
2025-05-30 23:49:55,011 | INFO | aether2 | Token embedding initialized with vocab size: 283
2025-05-30 23:49:55,036 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:49:55,036 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:49:55,052 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:49:55,070 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:49:55,788 | INFO | aether2 | Tokenizer updated and saved with 283 tokens.
2025-05-30 23:49:55,788 | INFO | aether2 | Using vocab_size = 283 for model initialization.
2025-05-30 23:49:58,168 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:49:58,277 | INFO | aether2 | Tokenizer vocab loaded from file with size 283
2025-05-30 23:49:58,278 | INFO | aether2 | Token embedding updated with vocab size 283
2025-05-30 23:49:58,398 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:49:58,398 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:49:58,416 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:49:58,417 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:50:27,520 | INFO | aether2 | GOAL: He wore gloves because it was cold outside.
2025-05-30 23:50:27,614 | INFO | aether2 | Tokenized input: [1, 231, 283, 284, 285, 167, 253, 286, 287, 264, 2]
2025-05-30 23:50:27,614 | WARNING | aether2 | Updating model and embeddings to vocab size 288 (max token index: 287)
2025-05-30 23:50:27,713 | INFO | aether2 | Model and embedding updated.
2025-05-30 23:50:53,817 | INFO | aether2 | Vocab loaded with size: 288
2025-05-30 23:50:53,818 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-30 23:50:53,847 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:50:53,848 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:50:53,861 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:50:53,879 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:50:54,574 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-30 23:50:54,574 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-30 23:50:56,998 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:50:57,103 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-30 23:50:57,104 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-30 23:50:57,223 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:50:57,223 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:50:57,243 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:50:57,244 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:55:32,953 | INFO | aether2 | Vocab loaded with size: 288
2025-05-30 23:55:32,955 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-30 23:55:32,984 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:55:32,985 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:55:33,002 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:55:33,020 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:55:33,732 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-30 23:55:33,733 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-30 23:55:36,106 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:55:36,209 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-30 23:55:36,210 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-30 23:55:36,324 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:55:36,324 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:55:36,344 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:55:36,346 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:56:10,037 | INFO | aether2 | Vocab loaded with size: 288
2025-05-30 23:56:10,038 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-30 23:56:10,066 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:56:10,066 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:56:10,083 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:56:10,099 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:56:10,798 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-30 23:56:10,799 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-30 23:56:13,236 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:56:13,339 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-30 23:56:13,340 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-30 23:56:13,450 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:56:13,451 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:56:13,473 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:56:13,473 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:56:33,712 | INFO | aether2 | Vocab loaded with size: 288
2025-05-30 23:56:33,713 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-30 23:56:33,742 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:56:33,742 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:56:33,758 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:56:33,776 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:56:34,521 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-30 23:56:34,522 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-30 23:56:37,094 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:56:37,200 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-30 23:56:37,201 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-30 23:56:37,317 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:56:37,317 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:56:37,335 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:56:37,336 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-30 23:58:33,759 | INFO | aether2 | Vocab loaded with size: 288
2025-05-30 23:58:33,759 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-30 23:58:33,787 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-30 23:58:33,787 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-30 23:58:33,804 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-30 23:58:33,820 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-30 23:58:34,530 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-30 23:58:34,531 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-30 23:58:37,051 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-30 23:58:37,159 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-30 23:58:37,160 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-30 23:58:37,279 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-30 23:58:37,279 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-30 23:58:37,301 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-30 23:58:37,302 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:01:13,070 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:01:13,071 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:01:13,099 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:01:13,099 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:01:13,116 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:01:13,133 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:01:13,882 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:01:13,883 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:01:16,365 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:01:16,475 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:01:16,476 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:01:16,598 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:01:16,599 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:01:16,617 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:01:16,618 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:03:49,533 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:03:49,535 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:03:49,662 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:03:49,662 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:03:49,679 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:03:49,694 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:03:50,397 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:03:50,398 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:03:52,873 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:03:52,993 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:03:52,994 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:03:53,105 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:03:53,106 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:03:53,126 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:03:53,127 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:04:46,407 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:04:46,408 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:04:46,436 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:04:46,436 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:04:46,452 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:04:46,471 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:04:47,202 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:04:47,203 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:04:49,801 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:04:49,907 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:04:49,908 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:04:50,035 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:04:50,035 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:04:50,057 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:04:50,058 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:06:23,860 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:06:23,860 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:06:23,891 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:06:23,893 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:06:23,907 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:06:23,927 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:06:24,644 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:06:24,645 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:06:27,169 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:06:27,275 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:06:27,276 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:06:27,390 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:06:27,390 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:06:27,411 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:06:27,412 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:09:35,141 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:09:35,142 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:09:35,170 | INFO | aether2 | Model not initialized  initializing from config...
2025-05-31 00:09:35,391 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([288]).
2025-05-31 00:09:35,393 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:09:35,393 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:09:35,408 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:09:35,427 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:09:36,138 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:09:36,139 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:09:38,816 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:09:38,915 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:09:38,916 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:09:39,043 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:09:39,044 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:09:39,063 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:09:39,064 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:09:58,827 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:09:58,828 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:09:58,858 | INFO | aether2 | Model not initialized  initializing from config...
2025-05-31 00:09:58,963 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:09:58,964 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:09:58,979 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:09:58,998 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:09:59,693 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:09:59,693 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:10:02,040 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:10:02,152 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:10:02,153 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:10:02,274 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:10:02,274 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:10:02,295 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:10:02,297 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:10:21,483 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:10:21,484 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:10:21,510 | INFO | aether2 | Model not initialized  initializing from config...
2025-05-31 00:10:21,736 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([288]).
2025-05-31 00:10:21,737 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:10:21,737 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:10:21,755 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:10:21,771 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:10:22,492 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:10:22,493 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:10:24,868 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:10:24,966 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:10:24,968 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:10:25,083 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:10:25,083 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:10:25,101 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:10:25,102 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:10:34,485 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:10:34,486 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:10:34,511 | INFO | aether2 | Model not initialized  initializing from config...
2025-05-31 00:10:34,747 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([288]).
2025-05-31 00:10:34,748 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:10:34,748 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:10:34,767 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:10:34,784 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:10:35,536 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:10:35,537 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:10:38,034 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:10:38,134 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:10:38,135 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:10:38,256 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:10:38,256 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:10:38,277 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:10:38,277 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:11:59,829 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:11:59,830 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:11:59,858 | INFO | aether2 | Model not initialized  initializing from config...
2025-05-31 00:12:00,092 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([288]).
2025-05-31 00:12:00,094 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:12:00,094 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:12:00,112 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:12:00,130 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:12:00,846 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:12:00,847 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:12:03,397 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:12:03,502 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:12:03,503 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:12:03,627 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:12:03,628 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:12:03,650 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:12:03,651 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:12:34,967 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:12:34,969 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:12:35,021 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:12:35,021 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:12:35,059 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:12:35,092 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:12:35,819 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:12:35,819 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:12:38,265 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:12:38,373 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:12:38,374 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:12:38,507 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:12:38,507 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:12:38,529 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:12:38,530 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:14:09,075 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:14:09,076 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:14:09,104 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:14:09,104 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:14:09,123 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:14:09,138 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:14:09,838 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:14:09,839 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:14:12,313 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:14:12,422 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:14:12,423 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:14:12,601 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:14:12,601 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:14:12,622 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:14:12,623 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:15:26,142 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:15:26,142 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:15:26,169 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:15:26,169 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:15:26,184 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:15:26,202 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:15:26,917 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:15:26,918 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:15:29,344 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:15:29,453 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:15:29,454 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:15:29,578 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:15:29,578 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:15:29,599 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:15:29,600 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:15:49,992 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:15:49,993 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:15:50,022 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:15:50,023 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:15:50,041 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:15:50,060 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:15:50,768 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:15:50,769 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:15:53,107 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:15:53,211 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:15:53,212 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:15:53,323 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:15:53,323 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:15:53,344 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:15:53,344 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:16:26,616 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:16:26,616 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:16:26,907 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:16:26,907 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:16:26,921 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:16:26,939 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:16:27,667 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:16:27,668 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:16:30,128 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:16:30,230 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:16:30,231 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:16:30,349 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:16:30,349 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:16:30,372 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:16:30,373 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:18:19,060 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:18:19,060 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:18:19,373 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([160, 256]) from checkpoint, the shape in current model is torch.Size([288, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([288]).
2025-05-31 00:18:19,374 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 00:18:19,374 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:18:19,374 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:18:19,387 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:18:19,403 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:18:20,097 | INFO | aether2 | Tokenizer updated and saved with 288 tokens.
2025-05-31 00:18:20,098 | INFO | aether2 | Using vocab_size = 288 for model initialization.
2025-05-31 00:18:22,452 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:18:22,553 | INFO | aether2 | Tokenizer vocab loaded from file with size 288
2025-05-31 00:18:22,555 | INFO | aether2 | Token embedding updated with vocab size 288
2025-05-31 00:18:22,659 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:18:22,660 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:18:22,682 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:18:22,682 | WARNING | aether2 | start_epoch (531) is greater than total epochs (531). Skipping training.
2025-05-31 00:20:08,288 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:20:08,289 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:20:08,566 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 00:20:36,730 | INFO | aether2 | Vocab loaded with size: 288
2025-05-31 00:20:36,731 | INFO | aether2 | Token embedding initialized with vocab size: 288
2025-05-31 00:20:36,980 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 00:21:26,728 | INFO | aether2 | GOAL: I saved the document on my computer.
2025-05-31 00:21:26,793 | INFO | aether2 | Tokenized input: [1, 73, 288, 77, 289, 214, 51, 290, 264, 2]
2025-05-31 00:21:26,793 | WARNING | aether2 | Updating model and embeddings to vocab size 291 (max token index: 290)
2025-05-31 00:21:26,904 | INFO | aether2 | Model and embedding updated.
2025-05-31 00:21:57,528 | INFO | aether2 | GOAL: She slipped on the wet floor.
2025-05-31 00:21:57,528 | INFO | aether2 | Tokenized input: [1, 266, 277, 214, 77, 278, 279, 264, 2]
2025-05-31 00:22:13,504 | INFO | aether2 | GOAL: The kids jumped into the swimming pool.
2025-05-31 00:22:13,601 | INFO | aether2 | Tokenized input: [1, 77, 291, 292, 293, 77, 294, 295, 264, 2]
2025-05-31 00:22:13,601 | WARNING | aether2 | Updating model and embeddings to vocab size 296 (max token index: 295)
2025-05-31 00:22:13,699 | INFO | aether2 | Model and embedding updated.
2025-05-31 00:22:56,589 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 00:22:56,590 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 00:22:56,843 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 00:22:56,843 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 00:22:56,845 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 00:22:56,845 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 00:22:56,857 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 00:22:56,876 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 00:22:57,586 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 00:22:57,586 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 00:22:59,925 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 00:23:00,022 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 00:23:00,023 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 00:23:00,140 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 00:23:00,140 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 531
2025-05-31 00:23:00,163 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 00:23:00,164 | INFO | aether2 | Starting model training for 700 epochs...
2025-05-31 00:27:05,498 | INFO | aether2 | Epoch 532: Loss = 5.9406
2025-05-31 00:27:05,597 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:31:13,481 | INFO | aether2 | Epoch 533: Loss = 5.9399
2025-05-31 00:31:13,578 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:35:21,213 | INFO | aether2 | Epoch 534: Loss = 5.9424
2025-05-31 00:35:21,300 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:39:22,335 | INFO | aether2 | Epoch 535: Loss = 5.9410
2025-05-31 00:39:22,420 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:43:35,828 | INFO | aether2 | Epoch 536: Loss = 5.9406
2025-05-31 00:43:35,906 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:47:29,953 | INFO | aether2 | Epoch 537: Loss = 5.9433
2025-05-31 00:47:30,030 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:51:28,822 | INFO | aether2 | Epoch 538: Loss = 5.9399
2025-05-31 00:51:28,899 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:55:28,021 | INFO | aether2 | Epoch 539: Loss = 5.9418
2025-05-31 00:55:28,102 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 00:59:11,820 | INFO | aether2 | Epoch 540: Loss = 5.9408
2025-05-31 00:59:11,903 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:02:52,383 | INFO | aether2 | Epoch 541: Loss = 5.9414
2025-05-31 01:02:52,465 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:06:36,333 | INFO | aether2 | Epoch 542: Loss = 5.9396
2025-05-31 01:06:36,427 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:10:19,142 | INFO | aether2 | Epoch 543: Loss = 5.9401
2025-05-31 01:10:19,232 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:14:00,849 | INFO | aether2 | Epoch 544: Loss = 5.9406
2025-05-31 01:14:00,928 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:17:44,126 | INFO | aether2 | Epoch 545: Loss = 5.9416
2025-05-31 01:17:44,211 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:21:28,562 | INFO | aether2 | Epoch 546: Loss = 5.9403
2025-05-31 01:21:28,641 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:25:13,353 | INFO | aether2 | Epoch 547: Loss = 5.9419
2025-05-31 01:25:13,436 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:28:57,581 | INFO | aether2 | Epoch 548: Loss = 5.9391
2025-05-31 01:28:57,660 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:32:40,465 | INFO | aether2 | Epoch 549: Loss = 5.9421
2025-05-31 01:32:40,543 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:36:22,906 | INFO | aether2 | Epoch 550: Loss = 5.9387
2025-05-31 01:36:22,990 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 01:41:12,040 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:41:12,040 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:41:12,240 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	Unexpected key(s) in state_dict: "layers.8.attention.values.weight", "layers.8.attention.keys.weight", "layers.8.attention.queries.weight", "layers.8.attention.fc_out.weight", "layers.8.attention.fc_out.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm2.weight", "layers.8.norm2.bias", "layers.8.feed_forward.0.weight", "layers.8.feed_forward.0.bias", "layers.8.feed_forward.2.weight", "layers.8.feed_forward.2.bias", "layers.9.attention.values.weight", "layers.9.attention.keys.weight", "layers.9.attention.queries.weight", "layers.9.attention.fc_out.weight", "layers.9.attention.fc_out.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm2.weight", "layers.9.norm2.bias", "layers.9.feed_forward.0.weight", "layers.9.feed_forward.0.bias", "layers.9.feed_forward.2.weight", "layers.9.feed_forward.2.bias", "layers.10.attention.values.weight", "layers.10.attention.keys.weight", "layers.10.attention.queries.weight", "layers.10.attention.fc_out.weight", "layers.10.attention.fc_out.bias", "layers.10.norm1.weight", "layers.10.norm1.bias", "layers.10.norm2.weight", "layers.10.norm2.bias", "layers.10.feed_forward.0.weight", "layers.10.feed_forward.0.bias", "layers.10.feed_forward.2.weight", "layers.10.feed_forward.2.bias", "layers.11.attention.values.weight", "layers.11.attention.keys.weight", "layers.11.attention.queries.weight", "layers.11.attention.fc_out.weight", "layers.11.attention.fc_out.bias", "layers.11.norm1.weight", "layers.11.norm1.bias", "layers.11.norm2.weight", "layers.11.norm2.bias", "layers.11.feed_forward.0.weight", "layers.11.feed_forward.0.bias", "layers.11.feed_forward.2.weight", "layers.11.feed_forward.2.bias", "layers.12.attention.values.weight", "layers.12.attention.keys.weight", "layers.12.attention.queries.weight", "layers.12.attention.fc_out.weight", "layers.12.attention.fc_out.bias", "layers.12.norm1.weight", "layers.12.norm1.bias", "layers.12.norm2.weight", "layers.12.norm2.bias", "layers.12.feed_forward.0.weight", "layers.12.feed_forward.0.bias", "layers.12.feed_forward.2.weight", "layers.12.feed_forward.2.bias", "layers.13.attention.values.weight", "layers.13.attention.keys.weight", "layers.13.attention.queries.weight", "layers.13.attention.fc_out.weight", "layers.13.attention.fc_out.bias", "layers.13.norm1.weight", "layers.13.norm1.bias", "layers.13.norm2.weight", "layers.13.norm2.bias", "layers.13.feed_forward.0.weight", "layers.13.feed_forward.0.bias", "layers.13.feed_forward.2.weight", "layers.13.feed_forward.2.bias", "layers.14.attention.values.weight", "layers.14.attention.keys.weight", "layers.14.attention.queries.weight", "layers.14.attention.fc_out.weight", "layers.14.attention.fc_out.bias", "layers.14.norm1.weight", "layers.14.norm1.bias", "layers.14.norm2.weight", "layers.14.norm2.bias", "layers.14.feed_forward.0.weight", "layers.14.feed_forward.0.bias", "layers.14.feed_forward.2.weight", "layers.14.feed_forward.2.bias", "layers.15.attention.values.weight", "layers.15.attention.keys.weight", "layers.15.attention.queries.weight", "layers.15.attention.fc_out.weight", "layers.15.attention.fc_out.bias", "layers.15.norm1.weight", "layers.15.norm1.bias", "layers.15.norm2.weight", "layers.15.norm2.bias", "layers.15.feed_forward.0.weight", "layers.15.feed_forward.0.bias", "layers.15.feed_forward.2.weight", "layers.15.feed_forward.2.bias", "layers.16.attention.values.weight", "layers.16.attention.keys.weight", "layers.16.attention.queries.weight", "layers.16.attention.fc_out.weight", "layers.16.attention.fc_out.bias", "layers.16.norm1.weight", "layers.16.norm1.bias", "layers.16.norm2.weight", "layers.16.norm2.bias", "layers.16.feed_forward.0.weight", "layers.16.feed_forward.0.bias", "layers.16.feed_forward.2.weight", "layers.16.feed_forward.2.bias", "layers.17.attention.values.weight", "layers.17.attention.keys.weight", "layers.17.attention.queries.weight", "layers.17.attention.fc_out.weight", "layers.17.attention.fc_out.bias", "layers.17.norm1.weight", "layers.17.norm1.bias", "layers.17.norm2.weight", "layers.17.norm2.bias", "layers.17.feed_forward.0.weight", "layers.17.feed_forward.0.bias", "layers.17.feed_forward.2.weight", "layers.17.feed_forward.2.bias", "layers.18.attention.values.weight", "layers.18.attention.keys.weight", "layers.18.attention.queries.weight", "layers.18.attention.fc_out.weight", "layers.18.attention.fc_out.bias", "layers.18.norm1.weight", "layers.18.norm1.bias", "layers.18.norm2.weight", "layers.18.norm2.bias", "layers.18.feed_forward.0.weight", "layers.18.feed_forward.0.bias", "layers.18.feed_forward.2.weight", "layers.18.feed_forward.2.bias", "layers.19.attention.values.weight", "layers.19.attention.keys.weight", "layers.19.attention.queries.weight", "layers.19.attention.fc_out.weight", "layers.19.attention.fc_out.bias", "layers.19.norm1.weight", "layers.19.norm1.bias", "layers.19.norm2.weight", "layers.19.norm2.bias", "layers.19.feed_forward.0.weight", "layers.19.feed_forward.0.bias", "layers.19.feed_forward.2.weight", "layers.19.feed_forward.2.bias". 
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:41:12,240 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:41:12,240 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:41:12,243 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:41:12,262 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:41:12,280 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:41:12,987 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:41:12,988 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:41:15,447 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:41:15,559 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:41:15,560 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:41:15,613 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:41:15,613 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:41:15,632 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:41:15,647 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:41:16,366 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:41:16,366 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:41:16,410 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:41:16,530 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:41:16,530 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:41:28,870 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:41:28,870 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:41:29,072 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	Unexpected key(s) in state_dict: "layers.8.attention.values.weight", "layers.8.attention.keys.weight", "layers.8.attention.queries.weight", "layers.8.attention.fc_out.weight", "layers.8.attention.fc_out.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm2.weight", "layers.8.norm2.bias", "layers.8.feed_forward.0.weight", "layers.8.feed_forward.0.bias", "layers.8.feed_forward.2.weight", "layers.8.feed_forward.2.bias", "layers.9.attention.values.weight", "layers.9.attention.keys.weight", "layers.9.attention.queries.weight", "layers.9.attention.fc_out.weight", "layers.9.attention.fc_out.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm2.weight", "layers.9.norm2.bias", "layers.9.feed_forward.0.weight", "layers.9.feed_forward.0.bias", "layers.9.feed_forward.2.weight", "layers.9.feed_forward.2.bias", "layers.10.attention.values.weight", "layers.10.attention.keys.weight", "layers.10.attention.queries.weight", "layers.10.attention.fc_out.weight", "layers.10.attention.fc_out.bias", "layers.10.norm1.weight", "layers.10.norm1.bias", "layers.10.norm2.weight", "layers.10.norm2.bias", "layers.10.feed_forward.0.weight", "layers.10.feed_forward.0.bias", "layers.10.feed_forward.2.weight", "layers.10.feed_forward.2.bias", "layers.11.attention.values.weight", "layers.11.attention.keys.weight", "layers.11.attention.queries.weight", "layers.11.attention.fc_out.weight", "layers.11.attention.fc_out.bias", "layers.11.norm1.weight", "layers.11.norm1.bias", "layers.11.norm2.weight", "layers.11.norm2.bias", "layers.11.feed_forward.0.weight", "layers.11.feed_forward.0.bias", "layers.11.feed_forward.2.weight", "layers.11.feed_forward.2.bias", "layers.12.attention.values.weight", "layers.12.attention.keys.weight", "layers.12.attention.queries.weight", "layers.12.attention.fc_out.weight", "layers.12.attention.fc_out.bias", "layers.12.norm1.weight", "layers.12.norm1.bias", "layers.12.norm2.weight", "layers.12.norm2.bias", "layers.12.feed_forward.0.weight", "layers.12.feed_forward.0.bias", "layers.12.feed_forward.2.weight", "layers.12.feed_forward.2.bias", "layers.13.attention.values.weight", "layers.13.attention.keys.weight", "layers.13.attention.queries.weight", "layers.13.attention.fc_out.weight", "layers.13.attention.fc_out.bias", "layers.13.norm1.weight", "layers.13.norm1.bias", "layers.13.norm2.weight", "layers.13.norm2.bias", "layers.13.feed_forward.0.weight", "layers.13.feed_forward.0.bias", "layers.13.feed_forward.2.weight", "layers.13.feed_forward.2.bias", "layers.14.attention.values.weight", "layers.14.attention.keys.weight", "layers.14.attention.queries.weight", "layers.14.attention.fc_out.weight", "layers.14.attention.fc_out.bias", "layers.14.norm1.weight", "layers.14.norm1.bias", "layers.14.norm2.weight", "layers.14.norm2.bias", "layers.14.feed_forward.0.weight", "layers.14.feed_forward.0.bias", "layers.14.feed_forward.2.weight", "layers.14.feed_forward.2.bias", "layers.15.attention.values.weight", "layers.15.attention.keys.weight", "layers.15.attention.queries.weight", "layers.15.attention.fc_out.weight", "layers.15.attention.fc_out.bias", "layers.15.norm1.weight", "layers.15.norm1.bias", "layers.15.norm2.weight", "layers.15.norm2.bias", "layers.15.feed_forward.0.weight", "layers.15.feed_forward.0.bias", "layers.15.feed_forward.2.weight", "layers.15.feed_forward.2.bias", "layers.16.attention.values.weight", "layers.16.attention.keys.weight", "layers.16.attention.queries.weight", "layers.16.attention.fc_out.weight", "layers.16.attention.fc_out.bias", "layers.16.norm1.weight", "layers.16.norm1.bias", "layers.16.norm2.weight", "layers.16.norm2.bias", "layers.16.feed_forward.0.weight", "layers.16.feed_forward.0.bias", "layers.16.feed_forward.2.weight", "layers.16.feed_forward.2.bias", "layers.17.attention.values.weight", "layers.17.attention.keys.weight", "layers.17.attention.queries.weight", "layers.17.attention.fc_out.weight", "layers.17.attention.fc_out.bias", "layers.17.norm1.weight", "layers.17.norm1.bias", "layers.17.norm2.weight", "layers.17.norm2.bias", "layers.17.feed_forward.0.weight", "layers.17.feed_forward.0.bias", "layers.17.feed_forward.2.weight", "layers.17.feed_forward.2.bias", "layers.18.attention.values.weight", "layers.18.attention.keys.weight", "layers.18.attention.queries.weight", "layers.18.attention.fc_out.weight", "layers.18.attention.fc_out.bias", "layers.18.norm1.weight", "layers.18.norm1.bias", "layers.18.norm2.weight", "layers.18.norm2.bias", "layers.18.feed_forward.0.weight", "layers.18.feed_forward.0.bias", "layers.18.feed_forward.2.weight", "layers.18.feed_forward.2.bias", "layers.19.attention.values.weight", "layers.19.attention.keys.weight", "layers.19.attention.queries.weight", "layers.19.attention.fc_out.weight", "layers.19.attention.fc_out.bias", "layers.19.norm1.weight", "layers.19.norm1.bias", "layers.19.norm2.weight", "layers.19.norm2.bias", "layers.19.feed_forward.0.weight", "layers.19.feed_forward.0.bias", "layers.19.feed_forward.2.weight", "layers.19.feed_forward.2.bias". 
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:41:29,072 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:41:29,073 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:41:29,073 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:41:29,087 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:41:29,104 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:41:29,809 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:41:29,809 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:41:32,172 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:41:32,269 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:41:32,270 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:41:32,325 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:41:32,325 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:41:32,342 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:41:32,361 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:41:33,070 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:41:33,070 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:41:33,120 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:41:33,240 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:41:33,241 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:42:24,249 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:42:24,250 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:42:24,500 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:42:24,500 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:42:24,501 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:42:24,501 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:42:24,520 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:42:24,535 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:42:25,257 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:42:25,258 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:42:27,752 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:42:27,875 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:42:27,876 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:42:28,007 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 01:42:28,007 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 01:42:28,029 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 01:42:28,029 | INFO | aether2 | Starting model training for 600 epochs...
2025-05-31 01:43:15,783 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:43:15,783 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:43:15,992 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	Unexpected key(s) in state_dict: "layers.8.attention.values.weight", "layers.8.attention.keys.weight", "layers.8.attention.queries.weight", "layers.8.attention.fc_out.weight", "layers.8.attention.fc_out.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm2.weight", "layers.8.norm2.bias", "layers.8.feed_forward.0.weight", "layers.8.feed_forward.0.bias", "layers.8.feed_forward.2.weight", "layers.8.feed_forward.2.bias", "layers.9.attention.values.weight", "layers.9.attention.keys.weight", "layers.9.attention.queries.weight", "layers.9.attention.fc_out.weight", "layers.9.attention.fc_out.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm2.weight", "layers.9.norm2.bias", "layers.9.feed_forward.0.weight", "layers.9.feed_forward.0.bias", "layers.9.feed_forward.2.weight", "layers.9.feed_forward.2.bias", "layers.10.attention.values.weight", "layers.10.attention.keys.weight", "layers.10.attention.queries.weight", "layers.10.attention.fc_out.weight", "layers.10.attention.fc_out.bias", "layers.10.norm1.weight", "layers.10.norm1.bias", "layers.10.norm2.weight", "layers.10.norm2.bias", "layers.10.feed_forward.0.weight", "layers.10.feed_forward.0.bias", "layers.10.feed_forward.2.weight", "layers.10.feed_forward.2.bias", "layers.11.attention.values.weight", "layers.11.attention.keys.weight", "layers.11.attention.queries.weight", "layers.11.attention.fc_out.weight", "layers.11.attention.fc_out.bias", "layers.11.norm1.weight", "layers.11.norm1.bias", "layers.11.norm2.weight", "layers.11.norm2.bias", "layers.11.feed_forward.0.weight", "layers.11.feed_forward.0.bias", "layers.11.feed_forward.2.weight", "layers.11.feed_forward.2.bias", "layers.12.attention.values.weight", "layers.12.attention.keys.weight", "layers.12.attention.queries.weight", "layers.12.attention.fc_out.weight", "layers.12.attention.fc_out.bias", "layers.12.norm1.weight", "layers.12.norm1.bias", "layers.12.norm2.weight", "layers.12.norm2.bias", "layers.12.feed_forward.0.weight", "layers.12.feed_forward.0.bias", "layers.12.feed_forward.2.weight", "layers.12.feed_forward.2.bias", "layers.13.attention.values.weight", "layers.13.attention.keys.weight", "layers.13.attention.queries.weight", "layers.13.attention.fc_out.weight", "layers.13.attention.fc_out.bias", "layers.13.norm1.weight", "layers.13.norm1.bias", "layers.13.norm2.weight", "layers.13.norm2.bias", "layers.13.feed_forward.0.weight", "layers.13.feed_forward.0.bias", "layers.13.feed_forward.2.weight", "layers.13.feed_forward.2.bias", "layers.14.attention.values.weight", "layers.14.attention.keys.weight", "layers.14.attention.queries.weight", "layers.14.attention.fc_out.weight", "layers.14.attention.fc_out.bias", "layers.14.norm1.weight", "layers.14.norm1.bias", "layers.14.norm2.weight", "layers.14.norm2.bias", "layers.14.feed_forward.0.weight", "layers.14.feed_forward.0.bias", "layers.14.feed_forward.2.weight", "layers.14.feed_forward.2.bias", "layers.15.attention.values.weight", "layers.15.attention.keys.weight", "layers.15.attention.queries.weight", "layers.15.attention.fc_out.weight", "layers.15.attention.fc_out.bias", "layers.15.norm1.weight", "layers.15.norm1.bias", "layers.15.norm2.weight", "layers.15.norm2.bias", "layers.15.feed_forward.0.weight", "layers.15.feed_forward.0.bias", "layers.15.feed_forward.2.weight", "layers.15.feed_forward.2.bias", "layers.16.attention.values.weight", "layers.16.attention.keys.weight", "layers.16.attention.queries.weight", "layers.16.attention.fc_out.weight", "layers.16.attention.fc_out.bias", "layers.16.norm1.weight", "layers.16.norm1.bias", "layers.16.norm2.weight", "layers.16.norm2.bias", "layers.16.feed_forward.0.weight", "layers.16.feed_forward.0.bias", "layers.16.feed_forward.2.weight", "layers.16.feed_forward.2.bias", "layers.17.attention.values.weight", "layers.17.attention.keys.weight", "layers.17.attention.queries.weight", "layers.17.attention.fc_out.weight", "layers.17.attention.fc_out.bias", "layers.17.norm1.weight", "layers.17.norm1.bias", "layers.17.norm2.weight", "layers.17.norm2.bias", "layers.17.feed_forward.0.weight", "layers.17.feed_forward.0.bias", "layers.17.feed_forward.2.weight", "layers.17.feed_forward.2.bias", "layers.18.attention.values.weight", "layers.18.attention.keys.weight", "layers.18.attention.queries.weight", "layers.18.attention.fc_out.weight", "layers.18.attention.fc_out.bias", "layers.18.norm1.weight", "layers.18.norm1.bias", "layers.18.norm2.weight", "layers.18.norm2.bias", "layers.18.feed_forward.0.weight", "layers.18.feed_forward.0.bias", "layers.18.feed_forward.2.weight", "layers.18.feed_forward.2.bias", "layers.19.attention.values.weight", "layers.19.attention.keys.weight", "layers.19.attention.queries.weight", "layers.19.attention.fc_out.weight", "layers.19.attention.fc_out.bias", "layers.19.norm1.weight", "layers.19.norm1.bias", "layers.19.norm2.weight", "layers.19.norm2.bias", "layers.19.feed_forward.0.weight", "layers.19.feed_forward.0.bias", "layers.19.feed_forward.2.weight", "layers.19.feed_forward.2.bias". 
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:43:15,992 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:43:15,992 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:43:15,992 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:43:16,006 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:43:16,021 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:43:16,712 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:43:16,712 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:43:19,147 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:43:19,250 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:43:19,250 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:43:19,305 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:43:19,305 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:43:19,321 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:43:19,339 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:43:20,037 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:43:20,037 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:43:20,080 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:43:20,213 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:43:20,213 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:45:50,562 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:45:50,567 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:45:50,803 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:45:50,803 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:45:50,804 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:45:50,804 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:45:50,814 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:45:50,834 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:45:51,578 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:45:51,580 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:45:54,101 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:45:54,212 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:45:54,212 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:45:54,328 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 01:45:54,328 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 01:45:54,349 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 01:45:54,350 | INFO | aether2 | Starting model training for 600 epochs...
2025-05-31 01:53:56,086 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:53:56,086 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:53:56,339 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:53:56,339 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:53:56,339 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:53:56,339 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:53:56,353 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:53:56,370 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:53:57,076 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:53:57,077 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:53:59,513 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:53:59,629 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:53:59,631 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:53:59,831 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 01:53:59,831 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 01:53:59,862 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 01:53:59,863 | INFO | aether2 | Starting model training for 600 epochs...
2025-05-31 01:55:30,013 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:55:30,015 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:55:30,559 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	Unexpected key(s) in state_dict: "layers.8.attention.values.weight", "layers.8.attention.keys.weight", "layers.8.attention.queries.weight", "layers.8.attention.fc_out.weight", "layers.8.attention.fc_out.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm2.weight", "layers.8.norm2.bias", "layers.8.feed_forward.0.weight", "layers.8.feed_forward.0.bias", "layers.8.feed_forward.2.weight", "layers.8.feed_forward.2.bias", "layers.9.attention.values.weight", "layers.9.attention.keys.weight", "layers.9.attention.queries.weight", "layers.9.attention.fc_out.weight", "layers.9.attention.fc_out.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm2.weight", "layers.9.norm2.bias", "layers.9.feed_forward.0.weight", "layers.9.feed_forward.0.bias", "layers.9.feed_forward.2.weight", "layers.9.feed_forward.2.bias", "layers.10.attention.values.weight", "layers.10.attention.keys.weight", "layers.10.attention.queries.weight", "layers.10.attention.fc_out.weight", "layers.10.attention.fc_out.bias", "layers.10.norm1.weight", "layers.10.norm1.bias", "layers.10.norm2.weight", "layers.10.norm2.bias", "layers.10.feed_forward.0.weight", "layers.10.feed_forward.0.bias", "layers.10.feed_forward.2.weight", "layers.10.feed_forward.2.bias", "layers.11.attention.values.weight", "layers.11.attention.keys.weight", "layers.11.attention.queries.weight", "layers.11.attention.fc_out.weight", "layers.11.attention.fc_out.bias", "layers.11.norm1.weight", "layers.11.norm1.bias", "layers.11.norm2.weight", "layers.11.norm2.bias", "layers.11.feed_forward.0.weight", "layers.11.feed_forward.0.bias", "layers.11.feed_forward.2.weight", "layers.11.feed_forward.2.bias", "layers.12.attention.values.weight", "layers.12.attention.keys.weight", "layers.12.attention.queries.weight", "layers.12.attention.fc_out.weight", "layers.12.attention.fc_out.bias", "layers.12.norm1.weight", "layers.12.norm1.bias", "layers.12.norm2.weight", "layers.12.norm2.bias", "layers.12.feed_forward.0.weight", "layers.12.feed_forward.0.bias", "layers.12.feed_forward.2.weight", "layers.12.feed_forward.2.bias", "layers.13.attention.values.weight", "layers.13.attention.keys.weight", "layers.13.attention.queries.weight", "layers.13.attention.fc_out.weight", "layers.13.attention.fc_out.bias", "layers.13.norm1.weight", "layers.13.norm1.bias", "layers.13.norm2.weight", "layers.13.norm2.bias", "layers.13.feed_forward.0.weight", "layers.13.feed_forward.0.bias", "layers.13.feed_forward.2.weight", "layers.13.feed_forward.2.bias", "layers.14.attention.values.weight", "layers.14.attention.keys.weight", "layers.14.attention.queries.weight", "layers.14.attention.fc_out.weight", "layers.14.attention.fc_out.bias", "layers.14.norm1.weight", "layers.14.norm1.bias", "layers.14.norm2.weight", "layers.14.norm2.bias", "layers.14.feed_forward.0.weight", "layers.14.feed_forward.0.bias", "layers.14.feed_forward.2.weight", "layers.14.feed_forward.2.bias", "layers.15.attention.values.weight", "layers.15.attention.keys.weight", "layers.15.attention.queries.weight", "layers.15.attention.fc_out.weight", "layers.15.attention.fc_out.bias", "layers.15.norm1.weight", "layers.15.norm1.bias", "layers.15.norm2.weight", "layers.15.norm2.bias", "layers.15.feed_forward.0.weight", "layers.15.feed_forward.0.bias", "layers.15.feed_forward.2.weight", "layers.15.feed_forward.2.bias", "layers.16.attention.values.weight", "layers.16.attention.keys.weight", "layers.16.attention.queries.weight", "layers.16.attention.fc_out.weight", "layers.16.attention.fc_out.bias", "layers.16.norm1.weight", "layers.16.norm1.bias", "layers.16.norm2.weight", "layers.16.norm2.bias", "layers.16.feed_forward.0.weight", "layers.16.feed_forward.0.bias", "layers.16.feed_forward.2.weight", "layers.16.feed_forward.2.bias", "layers.17.attention.values.weight", "layers.17.attention.keys.weight", "layers.17.attention.queries.weight", "layers.17.attention.fc_out.weight", "layers.17.attention.fc_out.bias", "layers.17.norm1.weight", "layers.17.norm1.bias", "layers.17.norm2.weight", "layers.17.norm2.bias", "layers.17.feed_forward.0.weight", "layers.17.feed_forward.0.bias", "layers.17.feed_forward.2.weight", "layers.17.feed_forward.2.bias", "layers.18.attention.values.weight", "layers.18.attention.keys.weight", "layers.18.attention.queries.weight", "layers.18.attention.fc_out.weight", "layers.18.attention.fc_out.bias", "layers.18.norm1.weight", "layers.18.norm1.bias", "layers.18.norm2.weight", "layers.18.norm2.bias", "layers.18.feed_forward.0.weight", "layers.18.feed_forward.0.bias", "layers.18.feed_forward.2.weight", "layers.18.feed_forward.2.bias", "layers.19.attention.values.weight", "layers.19.attention.keys.weight", "layers.19.attention.queries.weight", "layers.19.attention.fc_out.weight", "layers.19.attention.fc_out.bias", "layers.19.norm1.weight", "layers.19.norm1.bias", "layers.19.norm2.weight", "layers.19.norm2.bias", "layers.19.feed_forward.0.weight", "layers.19.feed_forward.0.bias", "layers.19.feed_forward.2.weight", "layers.19.feed_forward.2.bias". 
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:55:30,561 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:55:30,562 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:55:30,563 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:55:30,594 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:55:30,638 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:55:32,155 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:55:32,156 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:55:35,915 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:55:36,051 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:55:36,053 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:55:36,129 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:55:36,129 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:55:36,151 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:55:36,170 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:55:37,064 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:55:37,065 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:55:37,127 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:55:37,274 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:55:37,275 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:56:32,493 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:56:32,493 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:56:32,763 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	size mismatch for embedding.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([288, 256]) from checkpoint, the shape in current model is torch.Size([296, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([296]).
2025-05-31 01:56:32,763 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 01:56:32,767 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:56:32,767 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:56:32,773 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:56:32,797 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:56:33,493 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:56:33,493 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:56:35,812 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:56:35,922 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:56:35,922 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 01:56:36,033 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 01:56:36,033 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 01:56:36,058 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 01:56:36,060 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 01:58:54,010 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 01:58:54,016 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 01:58:54,349 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 01:58:54,349 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 01:58:54,361 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 01:58:54,375 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 01:58:55,106 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 01:58:55,106 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 01:58:57,406 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 01:58:57,500 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 01:58:57,500 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 02:09:58,093 | INFO | aether2 | Vocab loaded with size: 296
2025-05-31 02:09:58,093 | INFO | aether2 | Token embedding initialized with vocab size: 296
2025-05-31 02:09:58,343 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:09:58,343 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:09:58,365 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:09:58,384 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:09:59,079 | INFO | aether2 | Tokenizer updated and saved with 296 tokens.
2025-05-31 02:09:59,080 | INFO | aether2 | Using vocab_size = 296 for model initialization.
2025-05-31 02:10:01,413 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:10:01,503 | INFO | aether2 | Tokenizer vocab loaded from file with size 296
2025-05-31 02:10:01,513 | INFO | aether2 | Token embedding updated with vocab size 296
2025-05-31 02:10:01,613 | INFO | aether2 | Optimizer state NOT loaded because number of layers changed.
2025-05-31 02:10:01,613 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 02:10:01,613 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 02:13:35,589 | INFO | aether2 | GOAL: She planted a tree in the backyard.
2025-05-31 02:13:35,636 | INFO | aether2 | Tokenized input: [1, 266, 228, 80, 296, 173, 77, 297, 264, 2]
2025-05-31 02:13:35,637 | WARNING | aether2 | Updating model and embeddings to vocab size 298 (max token index: 297)
2025-05-31 02:13:35,749 | INFO | aether2 | Model and embedding updated.
2025-05-31 02:13:51,655 | INFO | aether2 | GOAL: The stars twinkled brightly in the night sky.
2025-05-31 02:13:51,697 | INFO | aether2 | Tokenized input: [1, 77, 298, 299, 201, 173, 77, 211, 263, 264, 2]
2025-05-31 02:13:51,698 | WARNING | aether2 | Updating model and embeddings to vocab size 300 (max token index: 299)
2025-05-31 02:13:51,792 | INFO | aether2 | Model and embedding updated.
2025-05-31 02:14:08,587 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:14:08,590 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:14:08,840 | WARNING | aether2 | Size mismatch error when loading model: Error(s) in loading state_dict for StackedTransformer:
	Unexpected key(s) in state_dict: "layers.19.attention.values.weight", "layers.19.attention.keys.weight", "layers.19.attention.queries.weight", "layers.19.attention.fc_out.weight", "layers.19.attention.fc_out.bias", "layers.19.norm1.weight", "layers.19.norm1.bias", "layers.19.norm2.weight", "layers.19.norm2.bias", "layers.19.feed_forward.0.weight", "layers.19.feed_forward.0.bias", "layers.19.feed_forward.2.weight", "layers.19.feed_forward.2.bias". 
	size mismatch for embedding.weight: copying a param with shape torch.Size([296, 256]) from checkpoint, the shape in current model is torch.Size([300, 256]).
	size mismatch for fc_out.weight: copying a param with shape torch.Size([296, 256]) from checkpoint, the shape in current model is torch.Size([300, 256]).
	size mismatch for fc_out.bias: copying a param with shape torch.Size([296]) from checkpoint, the shape in current model is torch.Size([300]).
2025-05-31 02:14:08,840 | WARNING | aether2 | Vocabulary or architecture changed  retraining model...
2025-05-31 02:14:08,846 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:14:08,846 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:14:08,858 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:14:08,870 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:14:09,596 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:14:09,597 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:14:12,050 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:14:12,170 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:14:12,170 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:14:12,300 | INFO | aether2 | Optimizer state NOT loaded because number of layers changed.
2025-05-31 02:14:12,300 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 02:14:12,302 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 02:15:38,549 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:15:38,549 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:15:38,814 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 02:16:02,309 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:16:02,311 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:16:02,582 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 02:16:21,173 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:16:21,173 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:16:21,381 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:16:21,381 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:16:21,393 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:16:21,413 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:16:22,139 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:16:22,139 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:16:24,533 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:16:24,633 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:16:24,633 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:16:24,700 | INFO | aether2 | Optimizer state NOT loaded because number of layers changed.
2025-05-31 02:16:24,700 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 02:16:24,700 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 02:21:47,662 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:21:47,668 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:21:47,886 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:21:47,886 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:21:47,906 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:21:47,927 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:21:48,615 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:21:48,616 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:21:51,072 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:21:51,235 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:21:51,236 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:22:02,862 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:22:02,862 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:22:03,130 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:22:03,130 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:22:03,149 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:22:03,163 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:22:03,880 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:22:03,881 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:22:06,254 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:22:06,371 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:22:06,372 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:22:06,492 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 02:22:06,494 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 02:22:06,514 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 02:22:06,514 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 02:22:44,913 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:22:44,913 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:22:45,157 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:22:45,157 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:22:45,175 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:22:45,192 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:22:45,890 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:22:45,890 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:22:48,364 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:22:48,468 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:22:48,469 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:22:48,572 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 02:22:48,573 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 02:22:48,595 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 02:22:48,596 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 02:23:04,455 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:23:04,455 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:23:04,693 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:23:04,693 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:23:04,704 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:23:04,728 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:23:05,447 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:23:05,447 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:23:08,008 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:23:08,114 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:23:08,114 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:23:08,235 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 02:23:08,235 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 551
2025-05-31 02:23:08,254 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 02:23:08,256 | WARNING | aether2 | start_epoch (551) is greater than total epochs (500). Skipping training.
2025-05-31 02:23:45,645 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:23:45,645 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:23:45,906 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:23:45,906 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:23:45,925 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:23:45,941 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:23:46,652 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:23:46,653 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:23:48,925 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 02:23:48,925 | INFO | aether2 | Starting model training for 20 epochs...
2025-05-31 02:25:10,567 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:25:10,567 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:25:10,820 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:25:10,820 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:25:10,838 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:25:10,857 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:25:11,547 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:25:11,548 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:25:14,036 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 02:25:14,036 | INFO | aether2 | Starting model training for 20 epochs...
2025-05-31 02:27:24,635 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:27:24,636 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:27:24,922 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:27:24,922 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:27:24,938 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:27:24,954 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:27:25,680 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:27:25,681 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:27:28,370 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 02:27:28,370 | INFO | aether2 | Starting model training for 10 epochs...
2025-05-31 02:29:25,924 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:29:25,924 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:29:26,156 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:29:26,156 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:29:26,174 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:29:26,192 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:29:26,948 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:29:26,950 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:29:29,464 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 02:29:29,464 | INFO | aether2 | Starting model training for 10 epochs...
2025-05-31 02:31:45,583 | INFO | aether2 | Epoch 1: Loss = 2.5949
2025-05-31 02:31:45,740 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:34:03,130 | INFO | aether2 | Epoch 2: Loss = 1.8001
2025-05-31 02:34:03,288 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:36:15,513 | INFO | aether2 | Epoch 3: Loss = 1.5978
2025-05-31 02:36:15,661 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:38:29,520 | INFO | aether2 | Epoch 4: Loss = 1.4966
2025-05-31 02:38:29,661 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:40:46,918 | INFO | aether2 | Epoch 5: Loss = 1.4380
2025-05-31 02:40:47,064 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:43:01,824 | INFO | aether2 | Epoch 6: Loss = 1.3793
2025-05-31 02:43:01,955 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:45:13,181 | INFO | aether2 | Epoch 7: Loss = 1.3219
2025-05-31 02:45:13,306 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:47:25,810 | INFO | aether2 | Epoch 8: Loss = 1.2841
2025-05-31 02:47:26,032 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:49:43,337 | INFO | aether2 | Epoch 9: Loss = 1.2768
2025-05-31 02:49:43,493 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:52:01,494 | INFO | aether2 | Epoch 10: Loss = 1.2509
2025-05-31 02:52:01,631 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 02:52:01,633 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-31 02:52:01,633 | INFO | aether2 | Reloading model and tokenizer after training...
2025-05-31 02:52:01,633 | INFO | aether2 | Tokenizer vocab reloaded with 300 tokens.
2025-05-31 02:52:01,840 | INFO | aether2 | Model reloaded and ready for inference.
2025-05-31 02:53:02,854 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:53:02,854 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:53:03,070 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:53:03,070 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:53:03,086 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:53:03,104 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:53:03,846 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:53:03,846 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:53:06,327 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 02:53:06,494 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 02:53:06,494 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 02:53:06,557 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 02:53:06,558 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 11
2025-05-31 02:53:06,578 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 02:53:06,578 | INFO | aether2 | Starting model training for 20 epochs...
2025-05-31 02:58:59,432 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 02:58:59,432 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 02:58:59,518 | WARNING | aether2 | Model file 'aether_model.pth' not found. Training new model...
2025-05-31 02:58:59,518 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 02:58:59,518 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 02:58:59,537 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 02:58:59,556 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 02:59:00,325 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 02:59:00,326 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 02:59:02,822 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 02:59:02,822 | INFO | aether2 | Starting model training for 20 epochs...
2025-05-31 03:01:16,130 | INFO | aether2 | Epoch 1: Loss = 2.5753
2025-05-31 03:01:16,132 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 03:01:16,132 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 03:01:16,151 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 03:01:16,171 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 03:01:16,887 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 03:01:16,887 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 03:01:16,936 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 03:01:16,936 | INFO | aether2 | Starting model training for 20 epochs...
2025-05-31 03:03:34,094 | INFO | aether2 | Epoch 1: Loss = 2.5836
2025-05-31 03:04:02,388 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 03:04:02,388 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 03:04:02,474 | WARNING | aether2 | Model file 'aether_model.pth' not found. Training new model...
2025-05-31 03:04:02,475 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 03:04:02,475 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 03:04:02,494 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 03:04:02,508 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 03:04:03,230 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 03:04:03,230 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 03:04:05,648 | WARNING | aether2 | No checkpoint found  starting training from scratch...
2025-05-31 03:04:05,648 | INFO | aether2 | Starting model training for 20 epochs...
2025-05-31 03:06:17,553 | INFO | aether2 | Epoch 1: Loss = 2.6018
2025-05-31 03:06:17,671 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:06:17,671 | INFO | aether2 | Epoch 1: Loss improved to 2.6018, checkpoint saved.
2025-05-31 03:06:17,796 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:08:30,836 | INFO | aether2 | Epoch 2: Loss = 1.7686
2025-05-31 03:08:30,994 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:08:30,994 | INFO | aether2 | Epoch 2: Loss improved to 1.7686, checkpoint saved.
2025-05-31 03:08:31,129 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:10:47,509 | INFO | aether2 | Epoch 3: Loss = 1.5708
2025-05-31 03:10:47,670 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:10:47,670 | INFO | aether2 | Epoch 3: Loss improved to 1.5708, checkpoint saved.
2025-05-31 03:10:47,800 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:13:04,329 | INFO | aether2 | Epoch 4: Loss = 1.4966
2025-05-31 03:13:04,466 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:13:04,467 | INFO | aether2 | Epoch 4: Loss improved to 1.4966, checkpoint saved.
2025-05-31 03:13:04,580 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:15:14,777 | INFO | aether2 | Epoch 5: Loss = 1.4388
2025-05-31 03:15:14,916 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:15:14,916 | INFO | aether2 | Epoch 5: Loss improved to 1.4388, checkpoint saved.
2025-05-31 03:15:15,033 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:17:16,176 | INFO | aether2 | Epoch 6: Loss = 1.3761
2025-05-31 03:17:16,304 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:17:16,306 | INFO | aether2 | Epoch 6: Loss improved to 1.3761, checkpoint saved.
2025-05-31 03:17:16,416 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:19:16,938 | INFO | aether2 | Epoch 7: Loss = 1.3226
2025-05-31 03:19:17,067 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:19:17,067 | INFO | aether2 | Epoch 7: Loss improved to 1.3226, checkpoint saved.
2025-05-31 03:19:17,189 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:21:20,544 | INFO | aether2 | Epoch 8: Loss = 1.3039
2025-05-31 03:21:20,674 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:21:20,674 | INFO | aether2 | Epoch 8: Loss improved to 1.3039, checkpoint saved.
2025-05-31 03:21:20,791 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:23:24,276 | INFO | aether2 | Epoch 9: Loss = 1.2815
2025-05-31 03:23:24,396 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:23:24,396 | INFO | aether2 | Epoch 9: Loss improved to 1.2815, checkpoint saved.
2025-05-31 03:23:24,520 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:25:26,407 | INFO | aether2 | Epoch 10: Loss = 1.2636
2025-05-31 03:25:26,537 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:25:26,537 | INFO | aether2 | Epoch 10: Loss improved to 1.2636, checkpoint saved.
2025-05-31 03:25:26,664 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:27:27,992 | INFO | aether2 | Epoch 11: Loss = 1.2419
2025-05-31 03:27:28,109 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:27:28,109 | INFO | aether2 | Epoch 11: Loss improved to 1.2419, checkpoint saved.
2025-05-31 03:27:28,233 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:29:28,180 | INFO | aether2 | Epoch 12: Loss = 1.2424
2025-05-31 03:29:28,180 | INFO | aether2 | Epoch 12: No improvement. Patience counter = 1/3
2025-05-31 03:29:28,300 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:31:29,345 | INFO | aether2 | Epoch 13: Loss = 1.2315
2025-05-31 03:31:29,470 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:31:29,471 | INFO | aether2 | Epoch 13: Loss improved to 1.2315, checkpoint saved.
2025-05-31 03:31:29,601 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:33:32,478 | INFO | aether2 | Epoch 14: Loss = 1.2277
2025-05-31 03:33:32,602 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:33:32,602 | INFO | aether2 | Epoch 14: Loss improved to 1.2277, checkpoint saved.
2025-05-31 03:33:32,728 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:35:36,681 | INFO | aether2 | Epoch 15: Loss = 1.2120
2025-05-31 03:35:36,816 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:35:36,816 | INFO | aether2 | Epoch 15: Loss improved to 1.2120, checkpoint saved.
2025-05-31 03:35:36,934 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:37:40,233 | INFO | aether2 | Epoch 16: Loss = 1.2091
2025-05-31 03:37:40,356 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:37:40,356 | INFO | aether2 | Epoch 16: Loss improved to 1.2091, checkpoint saved.
2025-05-31 03:37:40,473 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:39:44,657 | INFO | aether2 | Epoch 17: Loss = 1.2170
2025-05-31 03:39:44,658 | INFO | aether2 | Epoch 17: No improvement. Patience counter = 1/3
2025-05-31 03:39:44,796 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:41:51,855 | INFO | aether2 | Epoch 18: Loss = 1.2114
2025-05-31 03:41:51,856 | INFO | aether2 | Epoch 18: No improvement. Patience counter = 2/3
2025-05-31 03:41:51,980 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 03:43:55,582 | INFO | aether2 | Epoch 19: Loss = 1.2521
2025-05-31 03:43:55,582 | INFO | aether2 | Epoch 19: No improvement. Patience counter = 3/3
2025-05-31 03:43:55,582 | INFO | aether2 | No improvement for 3 epochs. Early stopping at epoch 19.
2025-05-31 03:43:55,582 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-31 03:43:55,582 | INFO | aether2 | Reloading model and tokenizer after training...
2025-05-31 03:43:55,597 | INFO | aether2 | Tokenizer vocab reloaded with 300 tokens.
2025-05-31 03:43:55,822 | INFO | aether2 | Model reloaded and ready for inference.
2025-05-31 04:01:03,656 | INFO | aether2 | GOAL: whats your name ?
2025-05-31 04:01:03,656 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-31 04:01:25,664 | INFO | aether2 | GOAL: your name is Aether
2025-05-31 04:01:25,664 | INFO | aether2 | Tokenized input: [1, 134, 154, 50, 163, 2]
2025-05-31 04:01:53,137 | INFO | aether2 | GOAL: whats your name ?
2025-05-31 04:01:53,138 | INFO | aether2 | Tokenized input: [1, 153, 134, 154, 2]
2025-05-31 13:05:13,130 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 13:05:13,131 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 13:05:13,361 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 13:05:59,812 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 13:05:59,812 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 13:05:59,955 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 13:06:33,069 | INFO | aether2 | Vocab loaded with size: 300
2025-05-31 13:06:33,070 | INFO | aether2 | Token embedding initialized with vocab size: 300
2025-05-31 13:06:33,220 | INFO | aether2 | Model loaded successfully from aether_model.pth.
2025-05-31 13:06:55,607 | INFO | aether2 | GOAL: train model
2025-05-31 13:06:55,608 | INFO | aether2 | Starting model training process...
2025-05-31 13:06:55,609 | INFO | aether2 | Training files listed in config: ['ai_Model/chat_training_data.json', 'ai_Model/chat_training_data_en.json']
2025-05-31 13:06:55,609 | INFO | aether2 | Current working directory: C:\Users\olive\Documents\java\backend\backend
2025-05-31 13:06:55,637 | INFO | aether2 | Loaded 10000 examples from ai_Model/chat_training_data.json
2025-05-31 13:06:55,660 | INFO | aether2 | Loaded 10002 examples from ai_Model/chat_training_data_en.json
2025-05-31 13:06:56,411 | INFO | aether2 | Tokenizer updated and saved with 300 tokens.
2025-05-31 13:06:56,412 | INFO | aether2 | Using vocab_size = 300 for model initialization.
2025-05-31 13:06:59,375 | INFO | aether2 | Found checkpoint: Loading from checkpoint_latest.pth...
2025-05-31 13:06:59,560 | INFO | aether2 | Tokenizer vocab loaded from file with size 300
2025-05-31 13:06:59,561 | INFO | aether2 | Token embedding updated with vocab size 300
2025-05-31 13:06:59,612 | INFO | aether2 | Optimizer state loaded from checkpoint.
2025-05-31 13:06:59,612 | INFO | aether2 | Checkpoint loaded from checkpoint_latest.pth, starting from epoch 19
2025-05-31 13:06:59,632 | INFO | aether2 | Vocabulary successfully saved to tokenizer_vocab.json.
2025-05-31 13:06:59,632 | INFO | aether2 | Starting model training for 80 epochs...
2025-05-31 13:08:53,594 | INFO | aether2 | Epoch 20: Loss = 1.2295
2025-05-31 13:08:53,755 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:08:53,755 | INFO | aether2 | Epoch 20: Loss improved to 1.2295, checkpoint saved.
2025-05-31 13:08:53,882 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:10:45,589 | INFO | aether2 | Epoch 21: Loss = 1.2217
2025-05-31 13:10:46,169 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:10:46,169 | INFO | aether2 | Epoch 21: Loss improved to 1.2217, checkpoint saved.
2025-05-31 13:10:46,277 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:12:38,961 | INFO | aether2 | Epoch 22: Loss = 1.2138
2025-05-31 13:12:39,154 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:12:39,154 | INFO | aether2 | Epoch 22: Loss improved to 1.2138, checkpoint saved.
2025-05-31 13:12:39,268 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:14:35,729 | INFO | aether2 | Epoch 23: Loss = 1.2180
2025-05-31 13:14:35,729 | INFO | aether2 | Epoch 23: No improvement. Patience counter = 1/3
2025-05-31 13:14:35,866 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:16:31,903 | INFO | aether2 | Epoch 24: Loss = 1.2301
2025-05-31 13:16:31,903 | INFO | aether2 | Epoch 24: No improvement. Patience counter = 2/3
2025-05-31 13:16:32,039 | INFO | aether2 | Checkpoint saved: checkpoint_latest.pth
2025-05-31 13:18:26,220 | INFO | aether2 | Epoch 25: Loss = 1.2290
2025-05-31 13:18:26,220 | INFO | aether2 | Epoch 25: No improvement. Patience counter = 3/3
2025-05-31 13:18:26,220 | INFO | aether2 | No improvement for 3 epochs. Early stopping at epoch 25.
2025-05-31 13:18:26,221 | INFO | aether2 | Training complete! Model saved successfully.
2025-05-31 13:18:26,221 | INFO | aether2 | Reloading model and tokenizer after training...
2025-05-31 13:18:26,222 | INFO | aether2 | Tokenizer vocab reloaded with 300 tokens.
2025-05-31 13:18:26,456 | INFO | aether2 | Model reloaded and ready for inference.
2025-05-31 13:18:26,502 | INFO | aether2 | Training finished.
